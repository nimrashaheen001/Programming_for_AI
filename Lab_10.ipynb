{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimrashaheen001/Programming_for_AI/blob/main/Lab_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Traditional Machine Learning Implementation**"
      ],
      "metadata": {
        "id": "FbORs_SNsdZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "titanic_data = pd.read_csv('titanic.csv')\n",
        "\n",
        "#print(titanic_data.isnull().sum())\n",
        "# Handle missing values=\n",
        "titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace=True)\n",
        "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "#sex = pd.get_dummies(train['Sex'],dtype=int)\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Sex', 'Embarked'], dtype=int)\n",
        "\n",
        "# Select features and target variable\n",
        "X = titanic_data[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]\n",
        "y = titanic_data[['Survived']]\n",
        "\n",
        "\n",
        "# Step 4: Split the Data into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train the KNN Model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)  # You can tune n_neighbors\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make Predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "#print(\"Predicted Values:\")\n",
        "#print(y_pred )\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h_b1vtSbXw-",
        "outputId": "667aba3a-8c31-405b-a17c-68a9ff506365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Accuracy: 0.7095\n",
            "Confusion Matrix:\n",
            "[[87 18]\n",
            " [34 40]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-35ad67740960>:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace=True)\n",
            "<ipython-input-2-35ad67740960>:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sklearn Pipeline Implementation**"
      ],
      "metadata": {
        "id": "0v6QLYCxh2mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "#print(\"Original DataFrame:\")\n",
        "#print(data)\n",
        "#print(data.shape)\n",
        "\n",
        "\n",
        "def impute_embarked(X):\n",
        "    X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])  # Fill missing values\n",
        "    return X\n",
        "\n",
        "# Custom function to create the FamilySize feature\n",
        "def create_family_size(X):\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1  # Adding 1 for the individual themselves\n",
        "    return X\n",
        "\n",
        "# Custom function to drop specified columns\n",
        "def drop_columns(X):\n",
        "    return X.drop(['SibSp', 'Parch'], axis=1)\n",
        "\n",
        "# Function to create FamilySize and drop SibSp and Parch columns\n",
        "def family_size(X):\n",
        "    X = create_family_size(X)\n",
        "    X = drop_columns(X)\n",
        "    return X\n",
        "\n",
        "# Create pipelines for Age\n",
        "age_pipeline = Pipeline(steps=[\n",
        "    ('age_imputer', SimpleImputer(strategy='mean')),  # Impute Age\n",
        "    ('age_scaler', MinMaxScaler())  # Scale Age\n",
        "])\n",
        "\n",
        "fare_pipeline = Pipeline(steps=[\n",
        "    #('fare_imputer', SimpleImputer(strategy='mean')),  # Impute Fare\n",
        "    ('fare_scaler', MinMaxScaler())  # Scale Fare\n",
        "])\n",
        "\n",
        "family_size_pipeline = Pipeline(steps=[\n",
        "    ('family_size_creator', FunctionTransformer(family_size)),\n",
        "    ('family_size_scaler', MinMaxScaler()),  # Scale Family_Size\n",
        "])\n",
        "\n",
        "embarked_pipeline = Pipeline(steps=[\n",
        "    ('embarked_imputer', FunctionTransformer(impute_embarked)),  # Impute Embarked\n",
        "    ('embarked_onehot', OneHotEncoder())  # One-hot encode Embarked\n",
        "])\n",
        "\n",
        "# Create a ColumnTransformer to preprocess the data\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('drop', 'drop', ['PassengerId', 'Name', 'Ticket', 'Cabin']),\n",
        "    ('age_encoder', age_pipeline, ['Age']),\n",
        "    ('fare_encoder', fare_pipeline, ['Fare']),\n",
        "    ('family_size', family_size_pipeline, ['SibSp', 'Parch']),  # Process FamilySize\n",
        "    ('embarked_encoder', embarked_pipeline, ['Embarked']),\n",
        "    ('sex_encoder', OneHotEncoder(), ['Sex']),\n",
        "    ('scaler', MinMaxScaler(), ['Pclass']),  # Scale Pclass\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Create a complete pipeline that includes preprocessing and the classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing steps\n",
        "    ('classifier', KNeighborsClassifier(n_neighbors=5))  # KNN Classifier\n",
        "])\n",
        "\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2FbXPYah0pj",
        "outputId": "9a2f4489-4e8a-4341-e78a-33a93c95733f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Accuracy: 0.80\n",
            "Confusion Matrix:\n",
            "[[90 15]\n",
            " [21 53]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random UnderSampling**"
      ],
      "metadata": {
        "id": "zx-w82snrfSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter # Used to count the occurrences of each class in the dataset, providing a quick summary of class distribution.\n",
        "\n",
        "from sklearn.datasets import make_classification#A utility function to generate synthetic datasets for classification tasks.\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler #A technique from the imbalanced-learn library to balance datasets by undersampling the majority class.\n",
        "\n",
        "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
        "\n",
        "#n_classes=2: The dataset will have 2 classes (binary classification).\n",
        "#class_sep=2: Controls the separation between the two classes. A larger value makes them more separable.\n",
        "#weights=[0.1, 0.9]: Specifies the class distribution:\n",
        "#10% of samples will belong to class 0.\n",
        "#90% of samples will belong to class 1 (imbalanced dataset).\n",
        "#n_informative=3: Number of informative features (important for classification).\n",
        "#_redundant=1: Number of redundant features (linear combinations of informative features).\n",
        "#flip_y=0: No noise is added to the labels (i.e., no label flipping).\n",
        "#n_features=20: Total number of features in the dataset.\n",
        "#n_clusters_per_class=1: Each class will have a single cluster of points.\n",
        "#n_samples=1000: The dataset will contain 1000 samples in total.\n",
        "#random_state=10: Ensures reproducibility of the generated dataset.\n",
        "\n",
        "print('Original dataset shape %s' % Counter(y))\n",
        "#Counter(y): Counts the occurrences of each class in y.\n",
        "#Prints the original distribution of classes. Since weights=[0.1, 0.9], we expect approximately:\n",
        "#100 samples of class 0.\n",
        "#900 samples of class 1.\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42) #Creates an instance of RandomUnderSampler\n",
        "X_res, y_res = rus.fit_resample(X, y)\n",
        "print('Resampled dataset shape %s' % Counter(y_res))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8FbZtYgiugt",
        "outputId": "7ac63d9b-f7ef-4f26-8640-3173262193aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape Counter({1: 900, 0: 100})\n",
            "Resampled dataset shape Counter({0: 100, 1: 100})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random OverSampling**"
      ],
      "metadata": {
        "id": "dR0Slieoruhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=1, weights=[0.01, 0.05, 0.94], class_sep=0.8, random_state=0)\n",
        "print('Original dataset shape %s' % Counter(y))\n",
        "#print(X)\n",
        "#print(y)\n",
        "\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "\n",
        "print('Resampled dataset shape %s' % Counter(y_resampled))\n",
        "#print(X_resampled)\n",
        "#print(y_resampled)\n",
        "#print(sorted(Counter(y_resampled).items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfeNMbw8qes_",
        "outputId": "a064cba9-3e9c-4ea8-8e9e-ce5d26289473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape Counter({2: 94, 1: 5, 0: 1})\n",
            "Resampled dataset shape Counter({2: 94, 1: 94, 0: 94})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SMOTE OverSampling**"
      ],
      "metadata": {
        "id": "dZO91UFzrzAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "X, y = make_classification(n_samples=5000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=1, weights=[0.01, 0.05, 0.94], class_sep=0.8, random_state=0)\n",
        "print('Original dataset shape %s' % Counter(y))\n",
        "\n",
        "\n",
        "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
        "print('SMOTE Resampled dataset shape %s' % Counter(y_resampled))\n",
        "\n",
        "\n",
        "#print(sorted(Counter(y_resampled).items()))\n",
        "\n",
        "#X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_resample(X, y)\n",
        "#print('ADASYN Resampled dataset shape %s' % Counter(y_resampled_adasyn))\n",
        "#print(sorted(Counter(y_resampled).items()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gawD2Zsi2RWY",
        "outputId": "daa3dd41-457b-4afe-b816-ae39edf48123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape Counter({2: 4674, 1: 262, 0: 64})\n",
            "SMOTE Resampled dataset shape Counter({2: 4674, 1: 4674, 0: 4674})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Tasks\n",
        "\n",
        "Perform following operations on the Titanic Dataset:\n",
        "\n",
        "*   Apply Random Undersampling to balance the dataset. (https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html)\n",
        "*   Apply Random Oversampling and SMOTE (Synthetic Minority Oversampling Technique) to balance the dataset. (https://imbalanced-learn.org/stable/over_sampling.html)\n",
        "*   Remove Outliers from the dataset.\n",
        "\n",
        "*   Apply k-fold cross validation on the dataset and display fold wise results.\n",
        "\n",
        "Perform the steps mentioned above on the following dataset\n",
        "\n",
        "https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease"
      ],
      "metadata": {
        "id": "Vu7nZz_DrFFY"
      }
    }
  ]
}