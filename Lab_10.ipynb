{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimrashaheen001/Programming_for_AI/blob/main/Lab_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Traditional Machine Learning Implementation**"
      ],
      "metadata": {
        "id": "FbORs_SNsdZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "titanic_data = pd.read_csv('titanic.csv')\n",
        "\n",
        "#print(titanic_data.isnull().sum())\n",
        "# Handle missing values=\n",
        "titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace=True)\n",
        "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "#sex = pd.get_dummies(train['Sex'],dtype=int)\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Sex', 'Embarked'], dtype=int)\n",
        "\n",
        "# Select features and target variable\n",
        "X = titanic_data[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]\n",
        "y = titanic_data[['Survived']]\n",
        "\n",
        "\n",
        "# Step 4: Split the Data into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train the KNN Model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)  # You can tune n_neighbors\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make Predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "#print(\"Predicted Values:\")\n",
        "#print(y_pred )\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h_b1vtSbXw-",
        "outputId": "667aba3a-8c31-405b-a17c-68a9ff506365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Accuracy: 0.7095\n",
            "Confusion Matrix:\n",
            "[[87 18]\n",
            " [34 40]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-35ad67740960>:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace=True)\n",
            "<ipython-input-2-35ad67740960>:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sklearn Pipeline Implementation**"
      ],
      "metadata": {
        "id": "0v6QLYCxh2mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "#print(\"Original DataFrame:\")\n",
        "#print(data)\n",
        "#print(data.shape)\n",
        "\n",
        "\n",
        "def impute_embarked(X):\n",
        "    X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])  # Fill missing values\n",
        "    return X\n",
        "\n",
        "# Custom function to create the FamilySize feature\n",
        "def create_family_size(X):\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1  # Adding 1 for the individual themselves\n",
        "    return X\n",
        "\n",
        "# Custom function to drop specified columns\n",
        "def drop_columns(X):\n",
        "    return X.drop(['SibSp', 'Parch'], axis=1)\n",
        "\n",
        "# Function to create FamilySize and drop SibSp and Parch columns\n",
        "def family_size(X):\n",
        "    X = create_family_size(X)\n",
        "    X = drop_columns(X)\n",
        "    return X\n",
        "\n",
        "# Create pipelines for Age\n",
        "age_pipeline = Pipeline(steps=[\n",
        "    ('age_imputer', SimpleImputer(strategy='mean')),  # Impute Age\n",
        "    ('age_scaler', MinMaxScaler())  # Scale Age\n",
        "])\n",
        "\n",
        "fare_pipeline = Pipeline(steps=[\n",
        "    #('fare_imputer', SimpleImputer(strategy='mean')),  # Impute Fare\n",
        "    ('fare_scaler', MinMaxScaler())  # Scale Fare\n",
        "])\n",
        "\n",
        "family_size_pipeline = Pipeline(steps=[\n",
        "    ('family_size_creator', FunctionTransformer(family_size)),\n",
        "    ('family_size_scaler', MinMaxScaler()),  # Scale Family_Size\n",
        "])\n",
        "\n",
        "embarked_pipeline = Pipeline(steps=[\n",
        "    ('embarked_imputer', FunctionTransformer(impute_embarked)),  # Impute Embarked\n",
        "    ('embarked_onehot', OneHotEncoder())  # One-hot encode Embarked\n",
        "])\n",
        "\n",
        "# Create a ColumnTransformer to preprocess the data\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('drop', 'drop', ['PassengerId', 'Name', 'Ticket', 'Cabin']),\n",
        "    ('age_encoder', age_pipeline, ['Age']),\n",
        "    ('fare_encoder', fare_pipeline, ['Fare']),\n",
        "    ('family_size', family_size_pipeline, ['SibSp', 'Parch']),  # Process FamilySize\n",
        "    ('embarked_encoder', embarked_pipeline, ['Embarked']),\n",
        "    ('sex_encoder', OneHotEncoder(), ['Sex']),\n",
        "    ('scaler', MinMaxScaler(), ['Pclass']),  # Scale Pclass\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Create a complete pipeline that includes preprocessing and the classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing steps\n",
        "    ('classifier', KNeighborsClassifier(n_neighbors=5))  # KNN Classifier\n",
        "])\n",
        "\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2FbXPYah0pj",
        "outputId": "9a2f4489-4e8a-4341-e78a-33a93c95733f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Accuracy: 0.80\n",
            "Confusion Matrix:\n",
            "[[90 15]\n",
            " [21 53]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random UnderSampling**"
      ],
      "metadata": {
        "id": "zx-w82snrfSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Use Random Undersampling?\n",
        "\n",
        "Imbalanced datasets can cause machine learning models to favor the majority class.\n",
        "Undersampling helps balance the dataset by reducing the number of samples in the majority class.\n",
        "Trade-off: Risk of losing important data (since some majority-class samples are removed)."
      ],
      "metadata": {
        "id": "yrSku2EeROEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import Counter # Used to count the occurrences of each class in the dataset, providing a quick summary of class distribution.\n",
        "\n",
        "from sklearn.datasets import make_classification#A utility function to generate synthetic datasets for classification tasks.\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler #A technique from the imbalanced-learn library to balance datasets by undersampling the majority class.\n",
        "\n",
        "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
        "\n",
        "#n_classes=2: The dataset will have 2 classes (binary classification).\n",
        "#class_sep=2: Controls the separation between the two classes. A larger value makes them more separable.\n",
        "#weights=[0.1, 0.9]: Specifies the class distribution:\n",
        "#10% of samples will belong to class 0.\n",
        "#90% of samples will belong to class 1 (imbalanced dataset).\n",
        "#n_informative=3: Number of informative features (important for classification).\n",
        "#_redundant=1: Number of redundant features (linear combinations of informative features).\n",
        "#flip_y=0: No noise is added to the labels (i.e., no label flipping).\n",
        "#n_features=20: Total number of features in the dataset.\n",
        "#n_clusters_per_class=1: Each class will have a single cluster of points.\n",
        "#n_samples=1000: The dataset will contain 1000 samples in total.\n",
        "#random_state=10: Ensures reproducibility of the generated dataset.\n",
        "\n",
        "print('Original dataset shape %s' % Counter(y))\n",
        "\n",
        "#Counter(y): Counts the occurrences of each class in y.\n",
        "#Prints the original distribution of classes. Since weights=[0.1, 0.9], we expect approximately:\n",
        "#100 samples of class 0.\n",
        "#900 samples of class 1.\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42) #Creates an instance of RandomUnderSampler\n",
        "X_res, y_res = rus.fit_resample(X, y)\n",
        "print('Resampled dataset shape %s' % Counter(y_res))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8FbZtYgiugt",
        "outputId": "7ac63d9b-f7ef-4f26-8640-3173262193aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape Counter({1: 900, 0: 100})\n",
            "Resampled dataset shape Counter({0: 100, 1: 100})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random OverSampling**"
      ],
      "metadata": {
        "id": "dR0Slieoruhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is RandomOverSampler?\n",
        "\n",
        "RandomOverSampler is a method from the imbalanced-learn library.\n",
        "It balances imbalanced datasets by increasing the number of samples in the minority class.\n",
        "How it works:\n",
        "It randomly duplicates samples from the minority class until both classes have the same number of samples.\n",
        "\n",
        "Advantages of Oversampling\n",
        "Balances the Dataset:\n",
        "\n",
        "Helps prevent models from being biased toward the majority class.\n",
        "Retains Minority Class Information:\n",
        "Unlike undersampling, no minority-class samples are removed.\n",
        "\n",
        "Trade-Offs\n",
        "Risk of Overfitting:\n",
        "\n",
        "Since oversampling duplicates existing samples, the model might overfit to the minority class.\n",
        "Increased Dataset Size:\n",
        "The number of samples grows, potentially increasing memory and computation requirements."
      ],
      "metadata": {
        "id": "vc_C1QwdRyjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=1, weights=[0.01, 0.05, 0.94], class_sep=0.8, random_state=0)\n",
        "print('Original dataset shape %s' % Counter(y))\n",
        "#print(X)\n",
        "#print(y)\n",
        "\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "\n",
        "print('Resampled dataset shape %s' % Counter(y_resampled))\n",
        "#print(X_resampled)\n",
        "#print(y_resampled)\n",
        "#print(sorted(Counter(y_resampled).items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfeNMbw8qes_",
        "outputId": "a064cba9-3e9c-4ea8-8e9e-ce5d26289473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape Counter({2: 94, 1: 5, 0: 1})\n",
            "Resampled dataset shape Counter({2: 94, 1: 94, 0: 94})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SMOTE OverSampling**"
      ],
      "metadata": {
        "id": "dZO91UFzrzAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What is SMOTE?\n",
        "\n",
        "SMOTE (Synthetic Minority Oversampling Technique):\n",
        "Generates synthetic samples for minority classes by:\n",
        "\n",
        "Selecting a sample from the minority class.\n",
        "\n",
        "Finding its k-nearest neighbors.\n",
        "\n",
        "Creating a synthetic sample along the line joining the original sample and one of its neighbors."
      ],
      "metadata": {
        "id": "Iz0u4m5WShW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "X, y = make_classification(n_samples=5000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=1, weights=[0.01, 0.05, 0.94], class_sep=0.8, random_state=0)\n",
        "print('Original dataset shape %s' % Counter(y))\n",
        "\n",
        "\n",
        "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
        "print('SMOTE Resampled dataset shape %s' % Counter(y_resampled))\n",
        "\n",
        "\n",
        "#print(sorted(Counter(y_resampled).items()))\n",
        "\n",
        "#X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_resample(X, y)\n",
        "#print('ADASYN Resampled dataset shape %s' % Counter(y_resampled_adasyn))\n",
        "#print(sorted(Counter(y_resampled).items()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gawD2Zsi2RWY",
        "outputId": "daa3dd41-457b-4afe-b816-ae39edf48123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape Counter({2: 4674, 1: 262, 0: 64})\n",
            "SMOTE Resampled dataset shape Counter({2: 4674, 1: 4674, 0: 4674})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Tasks\n",
        "\n",
        "Perform following operations on the Titanic Dataset:\n",
        "\n",
        "*   Apply Random Undersampling to balance the dataset. (https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html)\n",
        "*   Apply Random Oversampling and SMOTE (Synthetic Minority Oversampling Technique) to balance the dataset. (https://imbalanced-learn.org/stable/over_sampling.html)\n",
        "*   Remove Outliers from the dataset.\n",
        "\n",
        "*   Apply k-fold cross validation on the dataset and display fold wise results.\n",
        "\n",
        "Perform the steps mentioned above on the following dataset\n",
        "\n",
        "https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease"
      ],
      "metadata": {
        "id": "Vu7nZz_DrFFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy.stats import zscore\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_redundant=2,\n",
        "    n_classes=3,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.2, 0.5, 0.3],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Convert to a DataFrame for easier manipulation\n",
        "data = pd.DataFrame(X, columns=[f'Feature_{i}' for i in range(X.shape[1])])\n",
        "data['Target'] = y\n",
        "\n",
        "# Step 2: Remove outliers using Z-score method\n",
        "def remove_outliers(df, threshold=3):\n",
        "    \"\"\"Removes rows with z-scores above the threshold.\"\"\"\n",
        "    z_scores = np.abs(zscore(df.iloc[:, :-1]))  # Compute z-scores for feature columns\n",
        "    mask = (z_scores < threshold).all(axis=1)  # Mask rows without extreme z-scores\n",
        "    return df[mask]\n",
        "\n",
        "# Remove outliers\n",
        "data_cleaned = remove_outliers(data)\n",
        "X_cleaned = data_cleaned.iloc[:, :-1].values\n",
        "y_cleaned = data_cleaned['Target'].values\n",
        "\n",
        "print(f\"Original dataset shape: {Counter(y)}\")\n",
        "print(f\"Cleaned dataset shape: {Counter(y_cleaned)}\")\n",
        "\n",
        "# Step 3: Apply k-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold CV\n",
        "\n",
        "# Create a model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "fold_results = []\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X_cleaned), 1):\n",
        "    X_train, X_test = X_cleaned[train_index], X_cleaned[test_index]\n",
        "    y_train, y_test = y_cleaned[train_index], y_cleaned[test_index]\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    model.fit(X_train, y_train)\n",
        "    accuracy = model.score(X_test, y_test)\n",
        "    fold_results.append(accuracy)\n",
        "    print(f\"Fold {fold} Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 4: Display overall results\n",
        "print(f\"\\nFold-wise accuracies: {fold_results}\")\n",
        "print(f\"Mean accuracy: {np.mean(fold_results):.2f}\")\n",
        "print(f\"Standard deviation: {np.std(fold_results):.2f}\")\n"
      ],
      "metadata": {
        "id": "eGOXA35hTWbE",
        "outputId": "193bfaf8-ca81-4a75-9b87-84fd5ea60329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape: Counter({1: 495, 2: 303, 0: 202})\n",
            "Cleaned dataset shape: Counter({1: 484, 2: 292, 0: 187})\n",
            "Fold 1 Accuracy: 0.96\n",
            "Fold 2 Accuracy: 0.97\n",
            "Fold 3 Accuracy: 0.96\n",
            "Fold 4 Accuracy: 0.96\n",
            "Fold 5 Accuracy: 0.95\n",
            "\n",
            "Fold-wise accuracies: [0.9637305699481865, 0.9740932642487047, 0.9637305699481865, 0.9635416666666666, 0.953125]\n",
            "Mean accuracy: 0.96\n",
            "Standard deviation: 0.01\n"
          ]
        }
      ]
    }
  ]
}