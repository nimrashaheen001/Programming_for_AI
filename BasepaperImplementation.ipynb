{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNeWdyA44mqTKlZ1q2SUcMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimrashaheen001/Programming_for_AI/blob/main/BasepaperImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhOF-ZCumbr-",
        "outputId": "6515931b-49ad-4b17-ae64-2e705a76caf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.12.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6400 images in total.\n",
            "Calling __len__ function!\n",
            "Epoch 0/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [04:45<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 10.8677 Acc: 0.4854 MMSE Loss: 19.0536 CDR Loss: 0.4276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [01:06<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.3758 Acc: 0.5438 MMSE Loss: 14.5230 CDR Loss: 0.3276\n",
            "Epoch 1/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 9.8764 Acc: 0.4955 MMSE Loss: 17.1688 CDR Loss: 0.4098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 17.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 9.1959 Acc: 0.5195 MMSE Loss: 15.4099 CDR Loss: 0.3628\n",
            "Epoch 2/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 9.5573 Acc: 0.5115 MMSE Loss: 16.5912 CDR Loss: 0.4070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.1571 Acc: 0.5539 MMSE Loss: 14.1137 CDR Loss: 0.3780\n",
            "Epoch 3/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 9.3156 Acc: 0.5199 MMSE Loss: 16.1708 CDR Loss: 0.3822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.2580 Acc: 0.5547 MMSE Loss: 14.3374 CDR Loss: 0.3220\n",
            "Epoch 4/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 9.1210 Acc: 0.5180 MMSE Loss: 15.8462 CDR Loss: 0.3906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.3710 Acc: 0.5141 MMSE Loss: 14.4666 CDR Loss: 0.3613\n",
            "Epoch 5/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 9.1940 Acc: 0.5170 MMSE Loss: 15.9758 CDR Loss: 0.3956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 9.5478 Acc: 0.5734 MMSE Loss: 16.8320 CDR Loss: 0.3123\n",
            "Epoch 6/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 8.8529 Acc: 0.5320 MMSE Loss: 15.3409 CDR Loss: 0.3883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 11.0605 Acc: 0.5734 MMSE Loss: 19.9656 CDR Loss: 0.3798\n",
            "Epoch 7/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 8.0100 Acc: 0.5555 MMSE Loss: 13.8578 CDR Loss: 0.3244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.0343 Acc: 0.5922 MMSE Loss: 13.9963 CDR Loss: 0.3287\n",
            "Epoch 8/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.7802 Acc: 0.5641 MMSE Loss: 13.4033 CDR Loss: 0.3364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 10.2897 Acc: 0.5672 MMSE Loss: 18.4394 CDR Loss: 0.3537\n",
            "Epoch 9/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.5550 Acc: 0.5670 MMSE Loss: 12.9485 CDR Loss: 0.3513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 10.2087 Acc: 0.5953 MMSE Loss: 18.3982 CDR Loss: 0.3160\n",
            "Epoch 10/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.4164 Acc: 0.5736 MMSE Loss: 12.7149 CDR Loss: 0.3274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 7.3784 Acc: 0.5914 MMSE Loss: 12.7335 CDR Loss: 0.3136\n",
            "Epoch 11/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.1266 Acc: 0.5748 MMSE Loss: 12.1545 CDR Loss: 0.3274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 16.7154 Acc: 0.5859 MMSE Loss: 31.3014 CDR Loss: 0.3158\n",
            "Epoch 12/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.0439 Acc: 0.5938 MMSE Loss: 12.0173 CDR Loss: 0.3209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7891 Acc: 0.5898 MMSE Loss: 11.5489 CDR Loss: 0.3105\n",
            "Epoch 13/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.9188 Acc: 0.5979 MMSE Loss: 11.7742 CDR Loss: 0.3271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.9563 Acc: 0.6109 MMSE Loss: 11.9811 CDR Loss: 0.3041\n",
            "Epoch 14/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5515 Acc: 0.6059 MMSE Loss: 11.1080 CDR Loss: 0.3081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.6122 Acc: 0.6141 MMSE Loss: 11.3066 CDR Loss: 0.2938\n",
            "Epoch 15/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.4516 Acc: 0.6146 MMSE Loss: 10.9140 CDR Loss: 0.3117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7326 Acc: 0.6125 MMSE Loss: 11.5481 CDR Loss: 0.2996\n",
            "Epoch 16/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5369 Acc: 0.6098 MMSE Loss: 11.0872 CDR Loss: 0.3057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.4218 Acc: 0.6148 MMSE Loss: 10.9425 CDR Loss: 0.2929\n",
            "Epoch 17/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.3379 Acc: 0.6158 MMSE Loss: 10.7248 CDR Loss: 0.2936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 7.1625 Acc: 0.6195 MMSE Loss: 12.4386 CDR Loss: 0.2834\n",
            "Epoch 18/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.3577 Acc: 0.6215 MMSE Loss: 10.7445 CDR Loss: 0.3103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.5805 Acc: 0.6141 MMSE Loss: 11.2610 CDR Loss: 0.3020\n",
            "Epoch 19/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.3144 Acc: 0.6213 MMSE Loss: 10.6665 CDR Loss: 0.3042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.5938 Acc: 0.6078 MMSE Loss: 11.2772 CDR Loss: 0.2830\n",
            "Epoch 20/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.2600 Acc: 0.6244 MMSE Loss: 10.5832 CDR Loss: 0.2980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7695 Acc: 0.6234 MMSE Loss: 11.6220 CDR Loss: 0.2972\n",
            "Epoch 21/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.3265 Acc: 0.6215 MMSE Loss: 10.7187 CDR Loss: 0.3011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.4332 Acc: 0.6227 MMSE Loss: 10.9774 CDR Loss: 0.3001\n",
            "Epoch 22/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.1509 Acc: 0.6162 MMSE Loss: 10.3386 CDR Loss: 0.3072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.4792 Acc: 0.6203 MMSE Loss: 11.0903 CDR Loss: 0.2797\n",
            "Epoch 23/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:29<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.2713 Acc: 0.6258 MMSE Loss: 10.6065 CDR Loss: 0.2952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.3210 Acc: 0.6281 MMSE Loss: 10.7665 CDR Loss: 0.2865\n",
            "Epoch 24/24\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.3565 Acc: 0.6256 MMSE Loss: 10.7592 CDR Loss: 0.3074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.6636 Acc: 0.6203 MMSE Loss: 11.4279 CDR Loss: 0.3124\n",
            "Best val Acc: 0.628125\n",
            "Training complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install nibabel\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import nibabel as nib  # For potential metadata extraction (if needed)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from PIL import Image  # For image loading\n",
        "from torchvision import transforms # For image transformations\n",
        "#Mount Google Drive (uncomment this in Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class BasicBlock2D(nn.Module):  # Changed class name to BasicBlock2D\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock2D, self).__init__()\n",
        "        # Changed to 2D convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        # Changed to 2D batch normalization\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet2D(nn.Module):  # Changed class name to ResNet2D\n",
        "    def __init__(self, block, num_blocks, in_channels=3, num_classes=4):  # Updated in_channels and num_classes\n",
        "        super(ResNet2D, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # Initial convolution layer (changed to 2D)\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)  # Changed to 2D batch normalization\n",
        "\n",
        "        # ResNet layers (using BasicBlock2D)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "        # Classification head (changed to 2D)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Changed to 2D adaptive average pooling\n",
        "        self.fc = nn.Linear(512 * block.expansion, 512)\n",
        "\n",
        "        # Multiple output heads (adjusted for 4 classes)\n",
        "        self.classification_head = nn.Linear(512, num_classes)\n",
        "        self.regression_mmse = nn.Linear(512, 1)  # MMSE score regression\n",
        "        self.regression_cdr = nn.Linear(512, 1)   # Clinical Dementia Rating regression\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input processing\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # ResNet blocks\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        # Global pooling\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # Shared features\n",
        "        features = F.relu(self.fc(out))\n",
        "\n",
        "        # Multi-task outputs\n",
        "        classification = self.classification_head(features)\n",
        "        mmse_score = self.regression_mmse(features)\n",
        "        cdr_score = self.regression_cdr(features)\n",
        "\n",
        "        return {\n",
        "            'classification': classification,\n",
        "            'mmse_score': mmse_score,\n",
        "            'cdr_score': cdr_score,\n",
        "            'features': features\n",
        "        }\n",
        "\n",
        "def ResNet18_2D(in_channels=3, num_classes=4):  # Changed function name and defaults\n",
        "    return ResNet2D(BasicBlock2D, [2, 2, 2, 2], in_channels, num_classes)  # Us\n",
        "\n",
        "class BrainMRIDataset(Dataset):\n",
        "    def __init__(self, data_dir, classes=['VeryMildDemented', 'MildDemented', 'NonDemented', 'ModerateDemented'], transform=None):\n",
        "        \"\"\"\n",
        "        Dataset for multimodal brain MRI data\n",
        "\n",
        "        Args:\n",
        "            data_dir (str): Directory containing the data\n",
        "            classes (list): List of class names\n",
        "            transform (callable, optional): Optional transform to be applied on a sample\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.classes = classes\n",
        "        self.transform = transform\n",
        "\n",
        "        # Find all images and labels\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Assuming directory structure: data_dir/class_label/image.png (or other image format)\n",
        "        for class_idx, class_name in enumerate(classes):\n",
        "            class_path = os.path.join(data_dir, class_name)\n",
        "            image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
        "\n",
        "            for image_file in image_files:\n",
        "                image_path = os.path.join(class_path, image_file)\n",
        "                self.images.append(image_path)\n",
        "                self.labels.append(class_idx) # Assign class index as label\n",
        "\n",
        "        print(f\"Found {len(self.images)} images in total.\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert('RGB')  # Convert to RGB if needed\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Simulate or load mmse_score and cdr_score (Replace with your actual logic)\n",
        "        # Here I'm simulating them based on the label for demonstration purposes\n",
        "        mmse_score = torch.tensor([28.0 if label == 2 else 20.0 + np.random.normal(0, 2)], dtype=torch.float32) # Assuming label 2 is 'NonDemented'\n",
        "        cdr_score = torch.tensor([0.0 if label == 2 else 1.0 + np.random.normal(0, 0.5)], dtype=torch.float32)   # Assuming label 2 is 'NonDemented'\n",
        "\n",
        "\n",
        "        return {'image': image, 'label': torch.tensor(label, dtype=torch.long), 'mmse_score': mmse_score, 'cdr_score': cdr_score}\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        print(\"Calling __len__ function!\")  # Debug print statement\n",
        "        return len(self.images)\n",
        "\n",
        "# ... (Rest of the code) ...\n",
        "\n",
        "def train_model(model, dataloaders, criterion_dict, optimizer, scheduler, num_epochs=25, device='cuda'):\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            running_mmse_loss = 0.0\n",
        "            running_cdr_loss = 0.0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs in tqdm(dataloaders[phase]):\n",
        "                images = inputs['image'].to(device)\n",
        "                labels = inputs['label'].to(device)\n",
        "                mmse_scores = inputs['mmse_score'].to(device)\n",
        "                cdr_scores = inputs['cdr_score'].to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(images)\n",
        "                    _, preds = torch.max(outputs['classification'], 1)\n",
        "\n",
        "                    # Compute losses\n",
        "                    classification_loss = criterion_dict['classification'](outputs['classification'], labels)\n",
        "                    mmse_loss = criterion_dict['regression'](outputs['mmse_score'], mmse_scores)\n",
        "                    cdr_loss = criterion_dict['regression'](outputs['cdr_score'], cdr_scores)\n",
        "\n",
        "                    # Combined loss\n",
        "                    loss = classification_loss + 0.5 * mmse_loss + 0.5 * cdr_loss\n",
        "\n",
        "                    # Backward + optimize only in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                running_mmse_loss += mmse_loss.item() * images.size(0)\n",
        "                running_cdr_loss += cdr_loss.item() * images.size(0)\n",
        "\n",
        "            if phase == 'train' and scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "            epoch_mmse_loss = running_mmse_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_cdr_loss = running_cdr_loss / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} MMSE Loss: {epoch_mmse_loss:.4f} CDR Loss: {epoch_cdr_loss:.4f}')\n",
        "\n",
        "            # Deep copy the model if it's the best validation accuracy so far\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = model.state_dict().copy()\n",
        "\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        " # Set paths\n",
        "    data_dir = \"/content/drive/MyDrive/Alzheimer_MRI_4_classes_dataset\"  # Update with your Google Drive path\n",
        "\n",
        "    # Define transformations (resize, normalize, etc.)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to a common size\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    # Create dataset\n",
        "    dataset = BrainMRIDataset(data_dir=data_dir, classes=['VeryMildDemented', 'MildDemented', 'NonDemented', 'ModerateDemented'], transform=transform)\n",
        "    # Split into train and validation sets\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(dataset)),\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=dataset.labels\n",
        "    )\n",
        "\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
        "\n",
        "    dataloaders = {\n",
        "        'train': train_loader,\n",
        "        'val': val_loader\n",
        "    }\n",
        "\n",
        "   # Create model\n",
        "    model = ResNet18_2D(in_channels=3, num_classes=4)  # Update in_channels and num_classes for 2D images and 4 classes\n",
        "    # ... (Loss, optimizer, training, saving - Similar a\n",
        "\n",
        "    # Define loss functions\n",
        "    criterion_dict = {\n",
        "        'classification': nn.CrossEntropyLoss(),\n",
        "        'regression': nn.MSELoss()\n",
        "    }\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(\n",
        "        model=model,\n",
        "        dataloaders=dataloaders,\n",
        "        criterion_dict=criterion_dict,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=25,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), 'brain_mri_model.pth')\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def predict(model, dataloader, device='cuda'):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    results = {\n",
        "        'subject_ids': [],\n",
        "        'true_labels': [],\n",
        "        'predictions': [],\n",
        "        'mmse_scores': [],\n",
        "        'cdr_scores': []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs in tqdm(dataloader):\n",
        "            images = inputs['image'].to(device)\n",
        "            labels = inputs['label'].cpu().numpy()\n",
        "            subject_ids = inputs['subject_id']\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs['classification'], 1)\n",
        "\n",
        "            # Store results\n",
        "            results['subject_ids'].extend(subject_ids)\n",
        "            results['true_labels'].extend(labels)\n",
        "            results['predictions'].extend(preds.cpu().numpy())\n",
        "            results['mmse_scores'].extend(outputs['mmse_score'].cpu().numpy().flatten())\n",
        "            results['cdr_scores'].extend(outputs['cdr_score'].cpu().numpy().flatten())\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nibabel\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import nibabel as nib  # For potential metadata extraction (if needed)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from PIL import Image  # For image loading\n",
        "from torchvision import transforms # For image transformations\n",
        "#Mount Google Drive (uncomment this in Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class BasicBlock2D(nn.Module):  # Changed class name to BasicBlock2D\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock2D, self).__init__()\n",
        "        # Changed to 2D convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        # Changed to 2D batch normalization\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet2D(nn.Module):  # Changed class name to ResNet2D\n",
        "    def __init__(self, block, num_blocks, in_channels=3, num_classes=4):  # Updated in_channels and num_classes\n",
        "        super(ResNet2D, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # Initial convolution layer (changed to 2D)\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)  # Changed to 2D batch normalization\n",
        "\n",
        "        # ResNet layers (using BasicBlock2D)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "        # Classification head (changed to 2D)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Changed to 2D adaptive average pooling\n",
        "        self.fc = nn.Linear(512 * block.expansion, 512)\n",
        "\n",
        "        # Multiple output heads (adjusted for 4 classes)\n",
        "        self.classification_head = nn.Linear(512, num_classes)\n",
        "        self.regression_mmse = nn.Linear(512, 1)  # MMSE score regression\n",
        "        self.regression_cdr = nn.Linear(512, 1)   # Clinical Dementia Rating regression\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input processing\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # ResNet blocks\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        # Global pooling\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # Shared features\n",
        "        features = F.relu(self.fc(out))\n",
        "\n",
        "        # Multi-task outputs\n",
        "        classification = self.classification_head(features)\n",
        "        mmse_score = self.regression_mmse(features)\n",
        "        cdr_score = self.regression_cdr(features)\n",
        "\n",
        "        return {\n",
        "            'classification': classification,\n",
        "            'mmse_score': mmse_score,\n",
        "            'cdr_score': cdr_score,\n",
        "            'features': features\n",
        "        }\n",
        "\n",
        "def ResNet18_2D(in_channels=3, num_classes=4):  # Changed function name and defaults\n",
        "    return ResNet2D(BasicBlock2D, [2, 2, 2, 2], in_channels, num_classes)  # Us\n",
        "\n",
        "class BrainMRIDataset(Dataset):\n",
        "    def __init__(self, data_dir, classes=['VeryMildDemented', 'MildDemented', 'NonDemented', 'ModerateDemented'], transform=None):\n",
        "        \"\"\"\n",
        "        Dataset for multimodal brain MRI data\n",
        "\n",
        "        Args:\n",
        "            data_dir (str): Directory containing the data\n",
        "            classes (list): List of class names\n",
        "            transform (callable, optional): Optional transform to be applied on a sample\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.classes = classes\n",
        "        self.transform = transform\n",
        "\n",
        "        # Find all images and labels\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Assuming directory structure: data_dir/class_label/image.png (or other image format)\n",
        "        for class_idx, class_name in enumerate(classes):\n",
        "            class_path = os.path.join(data_dir, class_name)\n",
        "            image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
        "\n",
        "            for image_file in image_files:\n",
        "                image_path = os.path.join(class_path, image_file)\n",
        "                self.images.append(image_path)\n",
        "                self.labels.append(class_idx) # Assign class index as label\n",
        "\n",
        "        print(f\"Found {len(self.images)} images in total.\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert('RGB')  # Convert to RGB if needed\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Simulate or load mmse_score and cdr_score (Replace with your actual logic)\n",
        "        # Here I'm simulating them based on the label for demonstration purposes\n",
        "        mmse_score = torch.tensor([28.0 if label == 2 else 20.0 + np.random.normal(0, 2)], dtype=torch.float32) # Assuming label 2 is 'NonDemented'\n",
        "        cdr_score = torch.tensor([0.0 if label == 2 else 1.0 + np.random.normal(0, 0.5)], dtype=torch.float32)   # Assuming label 2 is 'NonDemented'\n",
        "\n",
        "\n",
        "        return {'image': image, 'label': torch.tensor(label, dtype=torch.long), 'mmse_score': mmse_score, 'cdr_score': cdr_score}\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        print(\"Calling __len__ function!\")  # Debug print statement\n",
        "        return len(self.images)\n",
        "\n",
        "# ... (Rest of the code) ...\n",
        "\n",
        "# Define training function with accuracy tracking\n",
        "def train_model(model, dataloaders, criterion_dict, optimizer, scheduler, num_epochs=100, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    best_acc = 0.0\n",
        "\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            total_samples = 0\n",
        "\n",
        "            for inputs in tqdm(dataloaders[phase]):\n",
        "                images = inputs['image'].to(device)\n",
        "                labels = inputs['label'].to(device)\n",
        "                mmse_scores = inputs['mmse_score'].to(device)\n",
        "                cdr_scores = inputs['cdr_score'].to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(images)\n",
        "                    _, preds = torch.max(outputs['classification'], 1)\n",
        "\n",
        "                    classification_loss = criterion_dict['classification'](outputs['classification'], labels)\n",
        "                    mmse_loss = criterion_dict['regression'](outputs['mmse_score'], mmse_scores)\n",
        "                    cdr_loss = criterion_dict['regression'](outputs['cdr_score'], cdr_scores)\n",
        "\n",
        "                    loss = classification_loss + 0.5 * mmse_loss + 0.5 * cdr_loss\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                total_samples += labels.size(0)\n",
        "\n",
        "            if phase == 'train' and scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / total_samples\n",
        "            epoch_acc = running_corrects.double() / total_samples\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_acc_history.append(epoch_acc.item())\n",
        "            else:\n",
        "                val_acc_history.append(epoch_acc.item())\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = model.state_dict().copy()\n",
        "\n",
        "    print(f'Best validation accuracy: {best_acc:.4f}')\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_acc_history, val_acc_history\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        " # Set paths\n",
        "    data_dir = \"/content/drive/MyDrive/Alzheimer_MRI_4_classes_dataset\"  # Update with your Google Drive path\n",
        "\n",
        "    # Define transformations (resize, normalize, etc.)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to a common size\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    # Create dataset\n",
        "    dataset = BrainMRIDataset(data_dir=data_dir, classes=['VeryMildDemented', 'MildDemented', 'NonDemented', 'ModerateDemented'], transform=transform)\n",
        "    # Split into train and validation sets\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(dataset)),\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=dataset.labels\n",
        "    )\n",
        "\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
        "\n",
        "    dataloaders = {\n",
        "        'train': train_loader,\n",
        "        'val': val_loader\n",
        "    }\n",
        "\n",
        "   # Create model\n",
        "    model = ResNet18_2D(in_channels=3, num_classes=4)  # Update in_channels and num_classes for 2D images and 4 classes\n",
        "    # ... (Loss, optimizer, training, saving - Similar a\n",
        "\n",
        "    # Define loss functions\n",
        "    criterion_dict = {\n",
        "        'classification': nn.CrossEntropyLoss(),\n",
        "        'regression': nn.MSELoss()\n",
        "    }\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(\n",
        "        model=model,\n",
        "        dataloaders=dataloaders,\n",
        "        criterion_dict=criterion_dict,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=25,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), 'brain_mri_model.pth')\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def predict(model, dataloader, device='cuda'):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    results = {\n",
        "        'subject_ids': [],\n",
        "        'true_labels': [],\n",
        "        'predictions': [],\n",
        "        'mmse_scores': [],\n",
        "        'cdr_scores': []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs in tqdm(dataloader):\n",
        "            images = inputs['image'].to(device)\n",
        "            labels = inputs['label'].cpu().numpy()\n",
        "            subject_ids = inputs['subject_id']\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs['classification'], 1)\n",
        "\n",
        "            # Store results\n",
        "            results['subject_ids'].extend(subject_ids)\n",
        "            results['true_labels'].extend(labels)\n",
        "            results['predictions'].extend(preds.cpu().numpy())\n",
        "            results['mmse_scores'].extend(outputs['mmse_score'].cpu().numpy().flatten())\n",
        "            results['cdr_scores'].extend(outputs['cdr_score'].cpu().numpy().flatten())\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    data_dir = \"/content/drive/MyDrive/Alzheimer_MRI_4_classes_dataset\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    dataset = BrainMRIDataset(data_dir=data_dir, classes=['VeryMildDemented', 'MildDemented', 'NonDemented', 'ModerateDemented'], transform=transform)\n",
        "    train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42, stratify=dataset.labels)\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
        "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "\n",
        "    model = ResNet18_2D(in_channels=3, num_classes=4)\n",
        "    criterion_dict = {'classification': nn.CrossEntropyLoss(), 'regression': nn.MSELoss()}\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    model, train_acc, val_acc = train_model(model, dataloaders, criterion_dict, optimizer, scheduler, num_epochs=100, device=device)\n",
        "    torch.save(model.state_dict(), 'brain_mri_model.pth')\n",
        "\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "JEWw2pqp4bHA",
        "outputId": "9d65e7f1-1bee-4534-8fcd-b7797143aeb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.12.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Found 6400 images in total.\n",
            "Calling __len__ function!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [05:08<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 11.1434 Acc: 0.4684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:59<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.6331 Acc: 0.5523\n",
            "Epoch 2/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 9.7905 Acc: 0.5135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.6142 Acc: 0.5750\n",
            "Epoch 3/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:26<00:00,  6.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 9.3597 Acc: 0.5102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.8621 Acc: 0.5508\n",
            "Epoch 4/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 9.1881 Acc: 0.5320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 7.9488 Acc: 0.5664\n",
            "Epoch 5/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 9.0723 Acc: 0.5172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:18<00:00, 17.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.1068 Acc: 0.5586\n",
            "Epoch 6/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 9.1736 Acc: 0.5336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:18<00:00, 16.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.3636 Acc: 0.5578\n",
            "Epoch 7/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 8.7519 Acc: 0.5291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.3783 Acc: 0.5766\n",
            "Epoch 8/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.8471 Acc: 0.5678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 7.7580 Acc: 0.5898\n",
            "Epoch 9/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.8448 Acc: 0.5740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 7.5954 Acc: 0.6047\n",
            "Epoch 10/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.6211 Acc: 0.5869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 17.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 7.2306 Acc: 0.6063\n",
            "Epoch 11/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.3375 Acc: 0.5945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 7.2868 Acc: 0.6117\n",
            "Epoch 12/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.2290 Acc: 0.6006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 17.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 7.4251 Acc: 0.6125\n",
            "Epoch 13/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 7.0835 Acc: 0.6176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 7.3650 Acc: 0.6148\n",
            "Epoch 14/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.9401 Acc: 0.6117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 8.2565 Acc: 0.6000\n",
            "Epoch 15/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.8374 Acc: 0.6229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.5302 Acc: 0.6188\n",
            "Epoch 16/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5838 Acc: 0.6355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7761 Acc: 0.6242\n",
            "Epoch 17/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.8009 Acc: 0.6303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 7.0065 Acc: 0.6273\n",
            "Epoch 18/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6811 Acc: 0.6406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 17.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7293 Acc: 0.6281\n",
            "Epoch 19/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6862 Acc: 0.6377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.8244 Acc: 0.6273\n",
            "Epoch 20/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5947 Acc: 0.6379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.8471 Acc: 0.6297\n",
            "Epoch 21/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6739 Acc: 0.6410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7127 Acc: 0.6328\n",
            "Epoch 22/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6519 Acc: 0.6398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.8315 Acc: 0.6227\n",
            "Epoch 23/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5976 Acc: 0.6420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7003 Acc: 0.6289\n",
            "Epoch 24/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.4518 Acc: 0.6408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.8581 Acc: 0.6320\n",
            "Epoch 25/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.4163 Acc: 0.6521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.5984 Acc: 0.6281\n",
            "Epoch 26/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5647 Acc: 0.6387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.8425 Acc: 0.6320\n",
            "Epoch 27/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.4770 Acc: 0.6486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.8438 Acc: 0.6281\n",
            "Epoch 28/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5369 Acc: 0.6459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.6726 Acc: 0.6305\n",
            "Epoch 29/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5788 Acc: 0.6441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.4016 Acc: 0.6289\n",
            "Epoch 30/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5389 Acc: 0.6484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.6205 Acc: 0.6313\n",
            "Epoch 31/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5696 Acc: 0.6354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.5542 Acc: 0.6328\n",
            "Epoch 32/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5763 Acc: 0.6457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.6996 Acc: 0.6281\n",
            "Epoch 33/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6979 Acc: 0.6467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.5686 Acc: 0.6289\n",
            "Epoch 34/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6256 Acc: 0.6438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7116 Acc: 0.6297\n",
            "Epoch 35/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6357 Acc: 0.6391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7847 Acc: 0.6289\n",
            "Epoch 36/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:27<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6759 Acc: 0.6451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.6935 Acc: 0.6258\n",
            "Epoch 37/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6379 Acc: 0.6404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.5484 Acc: 0.6297\n",
            "Epoch 38/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.4745 Acc: 0.6514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.3060 Acc: 0.6320\n",
            "Epoch 39/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.4711 Acc: 0.6428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.5717 Acc: 0.6297\n",
            "Epoch 40/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5789 Acc: 0.6393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7533 Acc: 0.6297\n",
            "Epoch 41/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.4449 Acc: 0.6438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.5984 Acc: 0.6313\n",
            "Epoch 42/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5363 Acc: 0.6404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.6519 Acc: 0.6289\n",
            "Epoch 43/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6021 Acc: 0.6436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 17.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.4335 Acc: 0.6242\n",
            "Epoch 44/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6017 Acc: 0.6396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.5389 Acc: 0.6313\n",
            "Epoch 45/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.6303 Acc: 0.6430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7138 Acc: 0.6297\n",
            "Epoch 46/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.4598 Acc: 0.6463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7223 Acc: 0.6289\n",
            "Epoch 47/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1280/1280 [03:28<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 6.5371 Acc: 0.6488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 6.7087 Acc: 0.6328\n",
            "Epoch 48/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▍       | 316/1280 [00:51<02:35,  6.19it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import logging\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    \"\"\"Enhanced ResNet block with optional squeeze-and-excitation\"\"\"\n",
        "    def __init__(self, in_planes, planes, stride=1, use_se=True):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "        # Squeeze-and-Excitation\n",
        "        self.use_se = use_se\n",
        "        if use_se:\n",
        "            self.se = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Conv2d(planes, planes // 16, kernel_size=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(planes // 16, planes, kernel_size=1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.use_se:\n",
        "            se_weights = self.se(out)\n",
        "            out = out * se_weights\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class BrainMRIModel(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=4, block_type=ResNetBlock):\n",
        "        super(BrainMRIModel, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # Initial layers\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # ResNet layers with residual blocks\n",
        "        self.layer1 = self._make_layer(block_type, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(block_type, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(block_type, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(block_type, 512, 2, stride=2)\n",
        "\n",
        "        # Global pooling and feature extraction\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        # Multi-task heads\n",
        "        self.classification_head = nn.Linear(256, num_classes)\n",
        "        self.regression_heads = nn.ModuleDict({\n",
        "            'mmse': nn.Linear(256, 1),\n",
        "            'cdr': nn.Linear(256, 1)\n",
        "        })\n",
        "\n",
        "    def _make_layer(self, block_type, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for curr_stride in strides:\n",
        "            layers.append(block_type(self.in_planes, planes, curr_stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.maxpool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        features = self.feature_extractor(x)\n",
        "\n",
        "        return {\n",
        "            'classification': self.classification_head(features),\n",
        "            'mmse_score': self.regression_heads['mmse'](features),\n",
        "            'cdr_score': self.regression_heads['cdr'](features),\n",
        "            'features': features\n",
        "        }\n",
        "\n",
        "class BrainMRIDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, classes: List[str] = None, transform=None):\n",
        "        \"\"\"Enhanced dataset with more robust loading and metadata handling\"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.classes = classes or ['VeryMildDemented', 'MildDemented', 'NonDemented', 'ModerateDemented']\n",
        "        self.transform = transform or self._get_default_transforms()\n",
        "\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.metadata = []  # Optional: store additional metadata\n",
        "\n",
        "        self._load_dataset()\n",
        "\n",
        "    def _get_default_transforms(self):\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def _load_dataset(self):\n",
        "        for class_idx, class_name in enumerate(self.classes):\n",
        "            class_path = os.path.join(self.data_dir, class_name)\n",
        "            for image_file in os.listdir(class_path):\n",
        "                full_path = os.path.join(class_path, image_file)\n",
        "                if os.path.isfile(full_path):\n",
        "                    self.images.append(full_path)\n",
        "                    self.labels.append(class_idx)\n",
        "                    # Optional: Add metadata logic here\n",
        "\n",
        "        logger.info(f\"Loaded {len(self.images)} images across {len(self.classes)} classes\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Simulated scores with more sophisticated generation\n",
        "        noise = np.random.normal(0, 0.5)\n",
        "        mmse_score = torch.tensor([max(0, min(30, 24 + noise * (label + 1)))], dtype=torch.float32)\n",
        "        cdr_score = torch.tensor([max(0, min(3, 0.5 * label + noise))], dtype=torch.float32)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'label': torch.tensor(label, dtype=torch.long),\n",
        "            'mmse_score': mmse_score,\n",
        "            'cdr_score': cdr_score\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    dataloaders: Dict[str, DataLoader],\n",
        "    criterion_dict: Dict[str, nn.Module],\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
        "    num_epochs: int = 50,\n",
        "    device: str = 'cuda'\n",
        ") -> Tuple[nn.Module, List[float], List[float]]:\n",
        "    \"\"\"Enhanced training function with early stopping and advanced logging\"\"\"\n",
        "    model = model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'val']:\n",
        "            model.train() if phase == 'train' else model.eval()\n",
        "            running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                for batch in tqdm(dataloaders[phase], desc=f\"{phase.capitalize()} Epoch {epoch+1}\"):\n",
        "                    images = batch['image'].to(device)\n",
        "                    labels = batch['label'].to(device)\n",
        "                    mmse_scores = batch['mmse_score'].to(device)\n",
        "                    cdr_scores = batch['cdr_score'].to(device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(images)\n",
        "\n",
        "                    class_loss = criterion_dict['classification'](outputs['classification'], labels)\n",
        "                    mmse_loss = criterion_dict['regression'](outputs['mmse_score'], mmse_scores)\n",
        "                    cdr_loss = criterion_dict['regression'](outputs['cdr_score'], cdr_scores)\n",
        "\n",
        "                    # Weighted loss\n",
        "                    loss = class_loss + 0.3 * mmse_loss + 0.3 * cdr_loss\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs['classification'], 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Compute epoch metrics\n",
        "            epoch_loss = running_loss / len(dataloaders[phase])\n",
        "            epoch_acc = correct / total\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_losses.append(epoch_loss)\n",
        "                train_accuracies.append(epoch_acc)\n",
        "                scheduler.step()\n",
        "            else:\n",
        "                val_losses.append(epoch_loss)\n",
        "                val_accuracies.append(epoch_acc)\n",
        "\n",
        "                # Early stopping\n",
        "                if epoch_loss < best_val_loss:\n",
        "                    best_val_loss = epoch_loss\n",
        "                    early_stopping_counter = 0\n",
        "                    torch.save(model.state_dict(), 'best_model.pth')\n",
        "                else:\n",
        "                    early_stopping_counter += 1\n",
        "\n",
        "                if early_stopping_counter >= patience:\n",
        "                    logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "                    break\n",
        "\n",
        "        logger.info(f\"Epoch {epoch+1}: Train Loss {train_losses[-1]:.4f}, Val Loss {val_losses[-1]:.4f}\")\n",
        "        logger.info(f\"Train Accuracy: {train_accuracies[-1]:.4f}, Val Accuracy: {val_accuracies[-1]:.4f}\")\n",
        "\n",
        "    return model, train_accuracies, val_accuracies\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Dataset and training configuration\n",
        "    data_dir = \"/path/to/Alzheimer_MRI_4_classes_dataset\"\n",
        "\n",
        "    # K-Fold Cross Validation\n",
        "    dataset = BrainMRIDataset(data_dir)\n",
        "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_indices, val_indices) in enumerate(kfold.split(dataset.images, dataset.labels), 1):\n",
        "        logger.info(f\"Training Fold {fold}\")\n",
        "\n",
        "        train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "        val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
        "\n",
        "        dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "\n",
        "        model = BrainMRIModel(in_channels=3, num_classes=4)\n",
        "\n",
        "        criterion_dict = {\n",
        "            'classification': nn.CrossEntropyLoss(),\n",
        "            'regression': nn.MSELoss()\n",
        "        }\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=50)\n",
        "\n",
        "        model, train_acc, val_acc = train_model(\n",
        "            model=model,\n",
        "            dataloaders=dataloaders,\n",
        "            criterion_dict=criterion_dict,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            num_epochs=50,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "PotGFCl9kM-W",
        "outputId": "6559f2ce-b765-463d-b2f4-496fdd2db3ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/path/to/Alzheimer_MRI_4_classes_dataset/VeryMildDemented'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6b2ed52f597a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-6b2ed52f597a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# K-Fold Cross Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBrainMRIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-6b2ed52f597a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, classes, transform)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Optional: store additional metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_default_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-6b2ed52f597a>\u001b[0m in \u001b[0;36m_load_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mclass_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                 \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/Alzheimer_MRI_4_classes_dataset/VeryMildDemented'"
          ]
        }
      ]
    }
  ]
}