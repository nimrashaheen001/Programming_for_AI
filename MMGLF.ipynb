{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUNf+U8nHp1MSzKTb4Smmt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimrashaheen001/Programming_for_AI/blob/main/MMGLF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK\n",
        "import SimpleITK as sitk"
      ],
      "metadata": {
        "id": "hsb6gZelFSF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JrIClaXDFSCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06y1HMZQ_EDb",
        "outputId": "0835ef03-82db-4c5a-bd8b-5c5a6da192fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install SimpleITK\n",
        "import SimpleITK as sitk\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import transform\n",
        "import os\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import csv\n",
        "\n",
        "\n",
        "def resize_image(itkimage, newSize, resamplemethod):\n",
        "\n",
        "    resampler = sitk.ResampleImageFilter()\n",
        "    originSize = itkimage.GetSize()\n",
        "    originSpacing = itkimage.GetSpacing()\n",
        "\n",
        "    newSize = np.array(newSize, float)\n",
        "    factor = originSize / newSize\n",
        "    newSpacing = originSpacing * factor\n",
        "    newSize = newSize.astype(np.int)\n",
        "\n",
        "    if resamplemethod == sitk.sitkNearestNeighbor:\n",
        "        resampler.SetOutputPixelType(sitk.sitkUInt8)\n",
        "    else:\n",
        "        resampler.SetOutputPixelType(sitk.sitkFloat32)\n",
        "\n",
        "    resampler.SetReferenceImage(itkimage)\n",
        "    resampler.SetSize(newSize.tolist())\n",
        "    resampler.SetOutputSpacing(newSpacing.tolist())\n",
        "    resampler.SetTransform(sitk.Transform(3, sitk.sitkIdentity))\n",
        "    resampler.SetInterpolator(resamplemethod)\n",
        "    itk_img_res = resampler.Execute(itkimage)\n",
        "\n",
        "    return itk_img_res\n",
        "\n",
        "def norm_image(image):\n",
        "    max = torch.max(image)\n",
        "    min = torch.min(image)\n",
        "    image = (image - min) / (max - min)\n",
        "    return image\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def specificity(Y_test, Y_pred, n):\n",
        "\n",
        "    spe = []\n",
        "    con_mat = confusion_matrix(Y_test,Y_pred)\n",
        "    for i in range(n):\n",
        "        number = np.sum(con_mat[:,:])\n",
        "        tp = con_mat[i][i]\n",
        "        fn = np.sum(con_mat[i,:]) - tp\n",
        "        fp = np.sum(con_mat[:,i]) - tp\n",
        "        tn = number - tp - fn - fp\n",
        "        spe1 = tn/ (tn + fp)\n",
        "        spe.append(spe1)\n",
        "    return spe\n",
        "\n",
        "\n",
        "def ACC(Y_test, Y_pred, n):\n",
        "    acc = []\n",
        "    con_mat = confusion_matrix(Y_test, Y_pred)\n",
        "    for i in range(n):\n",
        "        number = np.sum(con_mat[:, :])\n",
        "        tp = con_mat[i][i]\n",
        "        fn = np.sum(con_mat[i, :]) - tp\n",
        "        fp = np.sum(con_mat[:, i]) - tp\n",
        "        tn = number - tp - fn - fp\n",
        "        acc1 = (tp + tn) / number\n",
        "        acc.append(acc1)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ADMdataset(Dataset):\n",
        "    def __init__(self, data_txt):\n",
        "        self.data_txt=data_txt\n",
        "        self.datasets=[ ]\n",
        "\n",
        "        for file in open(self.data_txt,'r'):\n",
        "            image_file=file.strip('\\n').split(' ')[0]\n",
        "            image_label=file.strip('\\n').split(' ')[1]\n",
        "            self.datasets.append([image_file, image_label])\n",
        "            # print(self.datasets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.datasets[idx][0]\n",
        "        dir_name = os.path.dirname(os.path.dirname(os.path.dirname(self.datasets[idx][0])))\n",
        "        txt_file_path = os.path.join(dir_name, 'tabular.csv')\n",
        "        series_reader = sitk.ImageSeriesReader()\n",
        "        fileNames = series_reader.GetGDCMSeriesFileNames(self.datasets[idx][0])\n",
        "        series_reader.SetFileNames(fileNames)\n",
        "        images = series_reader.Execute()\n",
        "        images = resize_image(images, (64, 64, 64), resamplemethod=sitk.sitkLinear)\n",
        "        img_array = sitk.GetArrayFromImage(images)\n",
        "        img_vol = torch.from_numpy(img_array)\n",
        "        img_vol = norm_image(img_vol)\n",
        "        img_vol = img_vol.unsqueeze(0).float()\n",
        "        image_label = self.datasets[idx][1]\n",
        "        image_label = float(image_label)\n",
        "        c = os.path.basename(image)\n",
        "        list = []\n",
        "        with open(txt_file_path, 'r') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                if row['PTID'] == c:\n",
        "                    del row['PTID']\n",
        "                    for v in row.values():\n",
        "                        v = float(v)\n",
        "                        list.append(v)\n",
        "                    data = torch.tensor(list)\n",
        "        image_label = torch.tensor(image_label)\n",
        "        return img_vol, image_label, data\n",
        "    def __len__(self):\n",
        "        return len(self.datasets)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from _py_abc import ABCMeta\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections.abc import Sequence # Import Sequence from collections.abc instead of collections\n",
        "\n",
        "\n",
        "\n",
        "def conv3d(in_channels, out_channels, kernel_size=3, stride=1):\n",
        "    if kernel_size != 1:\n",
        "        padding = 1\n",
        "    else:\n",
        "        padding = 0\n",
        "    return nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
        "\n",
        "\n",
        "class ConvBnReLU(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, bn_momentum=0.05, kernel_size=3, stride=1, padding=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm3d(out_channels, momentum=bn_momentum)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bn_momentum=0.05, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3d(in_channels, out_channels, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels, momentum=bn_momentum)\n",
        "        self.conv2 = conv3d(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels, momentum=bn_momentum)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                conv3d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm3d(out_channels, momentum=bn_momentum),\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self, dim, heads=8, qkv_bias=False, qk_scale=None, dropout_rate=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_heads = heads\n",
        "        head_dim = dim // heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(dropout_rate)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = (\n",
        "            self.qkv(x)\n",
        "            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
        "            .permute(2, 0, 3, 1, 4)\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[0],\n",
        "            qkv[1],\n",
        "            qkv[2],\n",
        "        )  # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "b4mi2AXMCwxr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from _py_abc import ABCMeta\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections.abc import Sequence\n",
        "\n",
        "from collections import OrderedDict\n",
        "from typing import Any, Dict, Optional, Sequence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from networks.blocks import ConvBnReLU, ResBlock, conv3d, Attention\n",
        "\n",
        "\n",
        "class ConcatHNN1FC(nn.Module):\n",
        "    def __init__(self, in_channels=1, n_outputs=3, bn_momentum=0.1, n_basefilters=4, ndim_non_img=10) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = ConvBnReLU(in_channels, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.pool1 = nn.MaxPool3d(2, stride=2)  # 32\n",
        "        self.block1 = ResBlock(n_basefilters, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.block2 = ResBlock(n_basefilters, 2 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 16\n",
        "        self.block3 = ResBlock(2 * n_basefilters, 4 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 8\n",
        "        self.block4 = ResBlock(4 * n_basefilters, 8 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 4\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc = nn.Linear(8 * n_basefilters + ndim_non_img, 32)\n",
        "        self.fc1 = nn.Linear(32, n_outputs)\n",
        "\n",
        "\n",
        "    def forward(self, image, tabular):\n",
        "        out = self.conv1(image)\n",
        "        out = self.pool1(out)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.block4(out)\n",
        "        out = self.global_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = torch.cat((out, tabular), dim=1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConcatHNN2FC(nn.Module):\n",
        "    def __init__(self, in_channels=1, n_outputs=3, bn_momentum=0.1, n_basefilters=4, ndim_non_img=10, bottleneck_dim=15) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = ConvBnReLU(in_channels, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.pool1 = nn.MaxPool3d(2, stride=2)  # 32\n",
        "        self.block1 = ResBlock(n_basefilters, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.block2 = ResBlock(n_basefilters, 2 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 16\n",
        "        self.block3 = ResBlock(2 * n_basefilters, 4 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 8\n",
        "        self.block4 = ResBlock(4 * n_basefilters, 8 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 4\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc = nn.Linear(8 * n_basefilters + ndim_non_img, bottleneck_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(bottleneck_dim, n_outputs)\n",
        "\n",
        "    def forward(self, image, tabular):\n",
        "        out = self.conv1(image)\n",
        "        out = self.pool1(out)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.block4(out)\n",
        "        out = self.global_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = torch.cat((out, tabular), dim=1)\n",
        "        out = self.fc(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class HeterogeneousResNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, n_outputs=3, bn_momentum=0.1, n_basefilters=4) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = ConvBnReLU(in_channels, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.pool1 = nn.MaxPool3d(2, stride=2)  # 32\n",
        "        self.block1 = ResBlock(n_basefilters, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.block2 = ResBlock(n_basefilters, 2 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 16\n",
        "        self.block3 = ResBlock(2 * n_basefilters, 4 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 8\n",
        "        self.block4 = ResBlock(4 * n_basefilters, 8 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 4\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc = nn.Linear(8 * n_basefilters, n_outputs)\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        out = self.conv1(image)\n",
        "        out = self.pool1(out)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.block4(out)\n",
        "        out = self.global_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class InteractiveHNN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=1, n_outputs=3, bn_momentum=0.1, n_basefilters=4, ndim_non_img=10) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # ResNet\n",
        "        self.conv1 = ConvBnReLU(in_channels, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.pool1 = nn.MaxPool3d(2, stride=2)  # 32\n",
        "        self.block1 = ResBlock(n_basefilters, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.block2 = ResBlock(n_basefilters, 2 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 16\n",
        "        self.block3 = ResBlock(2 * n_basefilters, 4 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 8\n",
        "        self.block4 = ResBlock(4 * n_basefilters, 8 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 4\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc = nn.Linear(8 * n_basefilters, n_outputs)\n",
        "\n",
        "        layers = [\n",
        "            (\"aux_base\", nn.Linear(ndim_non_img, 15, bias=False)),\n",
        "            (\"aux_relu\", nn.ReLU()),\n",
        "            # (\"aux_dropout\", nn.Dropout(p=0.2, inplace=True)),\n",
        "            (\"aux_1\", nn.Linear(15, n_basefilters, bias=False)),\n",
        "        ]\n",
        "        self.aux = nn.Sequential(OrderedDict(layers))\n",
        "\n",
        "        self.aux_2 = nn.Linear(n_basefilters, n_basefilters, bias=False)\n",
        "        self.aux_3 = nn.Linear(n_basefilters, 2 * n_basefilters, bias=False)\n",
        "        self.aux_4 = nn.Linear(2 * n_basefilters, 4 * n_basefilters, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, image, tabular):\n",
        "        out = self.conv1(image)\n",
        "        out = self.pool1(out)\n",
        "        tabular = tabular.to(torch.float32)\n",
        "\n",
        "        attention = self.aux(tabular)\n",
        "        batch_size, n_channels = out.size()[:2]\n",
        "        out = torch.mul(out, attention.view(batch_size, n_channels, 1, 1, 1))\n",
        "        out = self.block1(out)\n",
        "\n",
        "        attention = self.aux_2(attention)\n",
        "        batch_size, n_channels = out.size()[:2]\n",
        "        out = torch.mul(out, attention.view(batch_size, n_channels, 1, 1, 1))\n",
        "        out = self.block2(out)\n",
        "\n",
        "        attention = self.aux_3(attention)\n",
        "        batch_size, n_channels = out.size()[:2]\n",
        "        out = torch.mul(out, attention.view(batch_size, n_channels, 1, 1, 1))\n",
        "        out = self.block3(out)\n",
        "\n",
        "        attention = self.aux_4(attention)\n",
        "        batch_size, n_channels = out.size()[:2]\n",
        "        out = torch.mul(out, attention.view(batch_size, n_channels, 1, 1, 1))\n",
        "        out = self.block4(out)\n",
        "\n",
        "        out = self.global_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out1 = F.softmax(out, dim=1)\n",
        "\n",
        "        return out1\n",
        "\n",
        "class DAFT2021(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int=1,\n",
        "            n_outputs: int=3,\n",
        "            bn_momentum: float = 0.1,\n",
        "            n_basefilters: int = 4,\n",
        "            filmblock_args: Optional[Dict[Any, Any]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if filmblock_args is None:\n",
        "            filmblock_args = {}\n",
        "\n",
        "        if filmblock_args is None:\n",
        "            filmblock_args = {}\n",
        "\n",
        "        self.split_size = 4 * n_basefilters\n",
        "        self.conv1 = ConvBnReLU(in_channels, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.pool1 = nn.MaxPool3d(2, stride=2)  # 32\n",
        "        self.block1 = ResBlock(n_basefilters, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.block2 = ResBlock(n_basefilters, 2 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 16\n",
        "        self.block3 = ResBlock(2 * n_basefilters, 4 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 8\n",
        "        self.blockX = DAFTBlock(4 * n_basefilters, 8 * n_basefilters, bn_momentum=bn_momentum, **filmblock_args)  # 4\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc = nn.Linear(8 * n_basefilters, n_outputs)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, image, tabular):\n",
        "        out = self.conv1(image)\n",
        "        out = self.pool1(out)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.blockX(out, tabular)\n",
        "        out = self.global_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FilmBase(nn.Module, metaclass=ABCMeta):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        bn_momentum: float,\n",
        "        stride: int,\n",
        "        ndim_non_img: int,\n",
        "        location: int,\n",
        "        activation: str,\n",
        "        scale: bool,\n",
        "        shift: bool,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # sanity checks\n",
        "        if location not in set(range(5)):\n",
        "            raise ValueError(f\"Invalid location specified: {location}\")\n",
        "        if activation not in {\"tanh\", \"sigmoid\", \"linear\"}:\n",
        "            raise ValueError(f\"Invalid location specified: {location}\")\n",
        "        if (not isinstance(scale, bool) or not isinstance(shift, bool)) or (not scale and not shift):\n",
        "            raise ValueError(\n",
        "                f\"scale and shift must be of type bool:\\n    -> scale value: {scale}, \"\n",
        "                \"scale type {type(scale)}\\n    -> shift value: {shift}, shift type: {type(shift)}\"\n",
        "            )\n",
        "        # ResBlock\n",
        "        self.conv1 = conv3d(in_channels, out_channels, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels, momentum=bn_momentum, affine=(location != 3))\n",
        "        self.conv2 = conv3d(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels, momentum=bn_momentum)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                conv3d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm3d(out_channels, momentum=bn_momentum),\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "        # Film-specific variables\n",
        "        self.location = location\n",
        "        if self.location == 2 and self.downsample is None:\n",
        "            raise ValueError(\"This is equivalent to location=1 and no downsampling!\")\n",
        "        # location decoding\n",
        "        self.film_dims = 0\n",
        "        if location in {0, 1, 2}:\n",
        "            self.film_dims = in_channels\n",
        "        elif location in {3, 4}:\n",
        "            self.film_dims = out_channels\n",
        "        if activation == \"sigmoid\":\n",
        "            self.scale_activation = nn.Sigmoid()\n",
        "        elif activation == \"tanh\":\n",
        "            self.scale_activation = nn.Tanh()\n",
        "        elif activation == \"linear\":\n",
        "            self.scale_activation = None\n",
        "\n",
        "\n",
        "    def rescale_features(self, feature_map, x_aux):\n",
        "        \"\"\"method to recalibrate feature map x\"\"\"\n",
        "\n",
        "    def forward(self, feature_map, x_aux):\n",
        "\n",
        "        if self.location == 0:\n",
        "            feature_map = self.rescale_features(feature_map, x_aux)\n",
        "        residual = feature_map\n",
        "\n",
        "        if self.location == 1:\n",
        "            residual = self.rescale_features(residual, x_aux)\n",
        "\n",
        "        if self.location == 2:\n",
        "            feature_map = self.rescale_features(feature_map, x_aux)\n",
        "        out = self.conv1(feature_map)\n",
        "        out = self.bn1(out)\n",
        "\n",
        "        if self.location == 3:\n",
        "            out = self.rescale_features(out, x_aux)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        if self.location == 4:\n",
        "            out = self.rescale_features(out, x_aux)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(residual)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DAFTBlock(FilmBase):\n",
        "    # Block for ZeCatNet\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int=1,\n",
        "        out_channels: int=3,\n",
        "        bn_momentum: float = 0.1,\n",
        "        stride: int = 2,\n",
        "        ndim_non_img: int = 10,\n",
        "        location: int = 3,\n",
        "        activation: str = \"linear\",\n",
        "        scale: bool = True,\n",
        "        shift: bool = True,\n",
        "        bottleneck_dim: int = 15,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            bn_momentum=bn_momentum,\n",
        "            stride=stride,\n",
        "            ndim_non_img=ndim_non_img,\n",
        "            location=location,\n",
        "            activation=activation,\n",
        "            scale=scale,\n",
        "            shift=shift,\n",
        "        )\n",
        "\n",
        "        self.bottleneck_dim = bottleneck_dim\n",
        "        aux_input_dims = self.film_dims\n",
        "        # shift and scale decoding\n",
        "        self.split_size = 0\n",
        "        if scale and shift:\n",
        "            self.split_size = self.film_dims\n",
        "            self.scale = None\n",
        "            self.shift = None\n",
        "            self.film_dims = 2 * self.film_dims\n",
        "        elif not scale:\n",
        "            self.scale = 1\n",
        "            self.shift = None\n",
        "        elif not shift:\n",
        "            self.shift = 0\n",
        "            self.scale = None\n",
        "\n",
        "        # create aux net\n",
        "        layers = [\n",
        "            (\"aux_base\", nn.Linear(ndim_non_img + aux_input_dims, self.bottleneck_dim, bias=False)),\n",
        "            (\"aux_relu\", nn.ReLU()),\n",
        "            (\"aux_out\", nn.Linear(self.bottleneck_dim, self.film_dims, bias=False)),\n",
        "        ]\n",
        "        self.aux = nn.Sequential(OrderedDict(layers))\n",
        "\n",
        "    def rescale_features(self, feature_map, x_aux):\n",
        "\n",
        "        squeeze = self.global_pool(feature_map)\n",
        "        squeeze = squeeze.view(squeeze.size(0), -1)\n",
        "        squeeze = torch.cat((squeeze, x_aux), dim=1)\n",
        "\n",
        "        attention = self.aux(squeeze)\n",
        "        if self.scale == self.shift:\n",
        "            v_scale, v_shift = torch.split(attention, self.split_size, dim=1)\n",
        "            v_scale = v_scale.view(*v_scale.size(), 1, 1, 1).expand_as(feature_map)\n",
        "            v_shift = v_shift.view(*v_shift.size(), 1, 1, 1).expand_as(feature_map)\n",
        "            if self.scale_activation is not None:\n",
        "                v_scale = self.scale_activation(v_scale)\n",
        "        elif self.scale is None:\n",
        "            v_scale = attention\n",
        "            v_scale = v_scale.view(*v_scale.size(), 1, 1, 1).expand_as(feature_map)\n",
        "            v_shift = self.shift\n",
        "            if self.scale_activation is not None:\n",
        "                v_scale = self.scale_activation(v_scale)\n",
        "        elif self.shift is None:\n",
        "            v_scale = self.scale\n",
        "            v_shift = attention\n",
        "            v_shift = v_shift.view(*v_shift.size(), 1, 1, 1).expand_as(feature_map)\n",
        "        else:\n",
        "            raise AssertionError(\n",
        "                f\"Sanity checking on scale and shift failed. Must be of type bool or None: {self.scale}, {self.shift}\"\n",
        "            )\n",
        "\n",
        "        return (v_scale * feature_map) + v_shift\n",
        "\n",
        "\n",
        "class Fusion2022(nn.Module):\n",
        "    def __init__(self, in_channels=1, n_outputs=3, bn_momentum=0.1, n_basefilters=4, ndim_non_img=10) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.conv1 = ConvBnReLU(in_channels, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.pool1 = nn.MaxPool3d(2, stride=2)  # 32\n",
        "        self.block1 = ResBlock(n_basefilters, n_basefilters, bn_momentum=bn_momentum)\n",
        "        self.block2 = ResBlock(n_basefilters, 2 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 16\n",
        "        self.block3 = ResBlock(2 * n_basefilters, 4 * n_basefilters, bn_momentum=bn_momentum, stride=2)  # 8\n",
        "        self.blockX = FBlock(4 * n_basefilters, 8 * n_basefilters, bn_momentum=bn_momentum)  # 4\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc = nn.Linear(32, 16)\n",
        "        self.fc1 = nn.Linear(16, n_outputs)\n",
        "\n",
        "        layers = [\n",
        "            (\"aux_base\", nn.Linear(ndim_non_img, 7, bias=False)),\n",
        "            (\"aux_relu\", nn.ReLU()),\n",
        "            # (\"aux_dropout\", nn.Dropout(p=0.2, inplace=True)),\n",
        "            (\"aux_1\", nn.Linear(7, 4, bias=False)),\n",
        "        ]\n",
        "\n",
        "        layers2 = [\n",
        "            (\"aux_base\", nn.Linear(ndim_non_img, 15, bias=False)),\n",
        "            (\"aux_relu\", nn.ReLU()),\n",
        "            # (\"aux_dropout\", nn.Dropout(p=0.2, inplace=True)),\n",
        "            (\"aux_1\", nn.Linear(15, 32, bias=False)),\n",
        "        ]\n",
        "        self.aux = nn.Sequential(OrderedDict(layers))\n",
        "        self.aux1 = nn.Sequential(OrderedDict(layers2))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, image, tabular):\n",
        "        out = self.conv1(image)\n",
        "        out = self.pool1(out)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.blockX(out, tabular)\n",
        "        out = self.global_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "#\n",
        "class FBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bn_momentum=0.05, stride=1, n_basefilters=4, ndim_non_img=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3d(in_channels, out_channels, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels, momentum=bn_momentum)\n",
        "        self.conv2 = conv3d(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels, momentum=bn_momentum)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.global_pool1 = nn.AvgPool3d(5)\n",
        "        self.fc5 = nn.Linear(32, 4)\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                conv3d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm3d(out_channels, momentum=bn_momentum),\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "        layers = [\n",
        "            (\"aux_base\", nn.Linear(ndim_non_img, 7, bias=False)),\n",
        "            (\"aux_relu\", nn.ReLU()),\n",
        "            # (\"aux_dropout\", nn.Dropout(p=0.2, inplace=True)),\n",
        "            (\"aux_1\", nn.Linear(7, n_basefilters, bias=False)),\n",
        "        ]\n",
        "        self.aux = nn.Sequential(OrderedDict(layers))\n",
        "\n",
        "    def forward(self, x, tabular):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out2 = self.bn1(out)\n",
        "        batch_size, n_channels = out2.size()[:2]\n",
        "        attention = self.aux(tabular)\n",
        "        cat2 = self.fc5(out2)\n",
        "        out = torch.cat(attention, cat2)\n",
        "        out = Attention(out)\n",
        "\n",
        "        out2 = self.relu(out)\n",
        "        out = self.conv2(out2)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wF95jg77DKVg",
        "outputId": "f3db501d-7159-4bde-dbf8-994efc9c7789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'networks'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-96588c9af263>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvBnReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'networks'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def ACC(Y_test, Y_pred, n):\n",
        "    acc = []\n",
        "    con_mat = confusion_matrix(Y_test, Y_pred)\n",
        "    for i in range(n):\n",
        "        number = np.sum(con_mat[:, :])\n",
        "        tp = con_mat[i][i]\n",
        "        fn = np.sum(con_mat[i, :]) - tp\n",
        "        fp = np.sum(con_mat[:, i]) - tp\n",
        "        tn = number - tp - fn - fp\n",
        "        acc1 = (tp + tn) / number\n",
        "        acc.append(acc1)\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "Tn1IttSDGOnB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def creat_filelist(input_path, classes):\n",
        "\n",
        "    dir_image1 = []\n",
        "    file_list = []\n",
        "    for index, name in enumerate(classes):\n",
        "        print('index', index)\n",
        "        index_str = str(index)\n",
        "        dir_image1_temp = input_path + '/' + name + '/'\n",
        "        for dir2 in os.listdir(dir_image1_temp):\n",
        "            dir_image2_temp = dir_image1_temp + '/' + dir2 + ' ' + index_str\n",
        "            # dir_image2_temp1 = dir_image2_temp.join(' ')\n",
        "            # dir_image2_temp2 = dir_image2_temp.join(index)\n",
        "            file_list.append(dir_image2_temp)\n",
        "\n",
        "    return dir_image1, file_list\n",
        "\n",
        "\n",
        "def creat_txtfile(output_path, file_list):\n",
        "    with open(output_path, 'w') as f:\n",
        "        for list in file_list:\n",
        "            print(list)\n",
        "            f.write(str(list) + '\\n')\n",
        "\n",
        "\n",
        "def main():\n",
        "    dir_image0 = '/path/to/your/dataset/train'\n",
        "    dir_image1 = os.listdir(dir_image0)\n",
        "    classes = dir_image1\n",
        "    print(classes)\n",
        "    dir_list, file_list = creat_filelist(dir_image0, classes)\n",
        "\n",
        "    # print(file_list[0:3])\n",
        "    output_path = 'train.txt'\n",
        "    creat_txtfile(output_path, file_list)\n",
        "    # print(output_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "rC_JPNW3GWW2",
        "outputId": "8a924b77-0354-4279-e4b7-77ef9e7872d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dataset/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c4ec47b9eb78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-c4ec47b9eb78>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdir_image0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dataset/train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdir_image1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_image0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_image1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-6SkIixGWK6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}