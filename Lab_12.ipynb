{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimrashaheen001/Programming_for_AI/blob/main/Lab_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentiment Analysis (Text Classification)**\n",
        "*   **Downloading Datset from Kaggle to Google Colab**\n",
        "*   **Text Cleaning**\n",
        "*   **Text Preprocessing**\n",
        "*   **Feature Engineering**\n",
        "*   **ML Model**"
      ],
      "metadata": {
        "id": "QKxILf5ndoUD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmKnCfBodcue",
        "outputId": "28cd7138-855f-437a-a77b-28741b0b730d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
            "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
            "License(s): other\n",
            "imdb-dataset-of-50k-movie-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  imdb-dataset-of-50k-movie-reviews.zip\n",
            "replace IMDB Dataset.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: IMDB Dataset.csv        \n"
          ]
        }
      ],
      "source": [
        "#!/bin/bash\n",
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Set up Kaggle API credentials\n",
        "#os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
        "#/content/kaggle.json\n",
        "# Make the Kaggle API key available to the environment\n",
        "with open('/content/kaggle.json') as f:\n",
        "    kaggle_json = json.load(f)\n",
        "    os.environ['KAGGLE_USERNAME'] = kaggle_json['username']\n",
        "    os.environ['KAGGLE_KEY'] = kaggle_json['key']\n",
        "\n",
        "#!/bin/bash\n",
        "!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "!unzip imdb-dataset-of-50k-movie-reviews.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Preprocessing Libraries**"
      ],
      "metadata": {
        "id": "Opp1GMraebjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "stopwords.words('english')\n",
        "exclude = string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5-bcmcNee4l",
        "outputId": "45c8911e-136a-45a7-932a-741645f56c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading Data**"
      ],
      "metadata": {
        "id": "A4QQYCBbeonO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = pd.read_csv('/content/IMDB Dataset.csv')\n",
        "df = temp_df.iloc[:30000]"
      ],
      "metadata": {
        "id": "IT9OMIfMe0jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Cleaning & Preprocessing**"
      ],
      "metadata": {
        "id": "KkZ1tUgelQgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html_tags(text):\n",
        "    pattern = re.compile('<.*?>')\n",
        "    return pattern.sub(r'', text)\n",
        "\n",
        "def remove_url(text):\n",
        "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return pattern.sub(r'', text)\n",
        "\n",
        "#exclude = \"!.,?\"\n",
        "def remove_punc(text):\n",
        "    return text.translate(str.maketrans('', '', exclude))"
      ],
      "metadata": {
        "id": "1pKTCdM-e9ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review'] = df['review'].str.lower()\n",
        "\n",
        "df['review'] = df['review'].apply(remove_html_tags)\n",
        "\n",
        "df['review'] = df['review'].apply(remove_url)\n",
        "\n",
        "df['review'] = df['review'].apply(remove_punc)\n",
        "\n",
        "#df['review'] = df['review'].apply(word_tokenize)\n",
        "\n",
        "#df['review'] = df['review'].apply(remove_stopwords)\n",
        "\n",
        "#df['review'] = df['review'].apply(lemmatize_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMJGFEx7kbKF",
        "outputId": "07152532-d502-48b1-fc9f-e77928219077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-6f636dc7a57c>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['review'] = df['review'].str.lower()\n",
            "<ipython-input-25-6f636dc7a57c>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['review'] = df['review'].apply(remove_html_tags)\n",
            "<ipython-input-25-6f636dc7a57c>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['review'] = df['review'].apply(remove_url)\n",
            "<ipython-input-25-6f636dc7a57c>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['review'] = df['review'].apply(remove_punc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**"
      ],
      "metadata": {
        "id": "zV6bPViYl3_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target Column Encoding**"
      ],
      "metadata": {
        "id": "JJozhru2MDY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#X = df.drop('sentiment', axis=1)\n",
        "X = df['review']\n",
        "Y = df['sentiment']\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "\n",
        "#This code is performing label encoding on the target variable ('sentiment') in the dataset, converting categorical labels into numeric values.\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "#This creates an instance of the LabelEncoder class and assigns it to the variable encoder.\n",
        "#The LabelEncoder will be used to convert categorical labels (sentiment values) into numerical labels (integers).\n",
        "\n",
        "Y = encoder.fit_transform(Y)\n",
        "#This line uses the fit_transform() method of the LabelEncoder object.\n",
        "#fit_transform(Y) does two things:\n",
        "#fit(): It learns the unique classes in the target variable Y (the sentiment labels) and assigns them integer values.\n",
        "#transform(): It then transforms the categorical labels in Y into their corresponding numeric values.\n",
        "\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuTETTE2Retv",
        "outputId": "5c9bba9a-5d16-46e4-871d-7e40d43b8a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        one of the other reviewers has mentioned that ...\n",
            "1        a wonderful little production the filming tech...\n",
            "2        i thought this was a wonderful way to spend ti...\n",
            "3        basically theres a family where a little boy j...\n",
            "4        petter matteis love in the time of money is a ...\n",
            "                               ...                        \n",
            "29995    new york i love you finally makes it to our sh...\n",
            "29996    this movie makes you wish imdb would let you v...\n",
            "29997    space camp which had the unfortunate luck to b...\n",
            "29998    octavio paz mexican poet writer and diplomat w...\n",
            "29999    having watched 10 minutes of this movie i was ...\n",
            "Name: review, Length: 30000, dtype: object\n",
            "0        positive\n",
            "1        positive\n",
            "2        positive\n",
            "3        negative\n",
            "4        positive\n",
            "           ...   \n",
            "29995    positive\n",
            "29996    negative\n",
            "29997    negative\n",
            "29998    positive\n",
            "29999    negative\n",
            "Name: sentiment, Length: 30000, dtype: object\n",
            "[1 1 1 ... 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKWnAdeeFWQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**"
      ],
      "metadata": {
        "id": "S7enK_qOl9Mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs text classification using the Bag of Words (BoW) model, which converts text data into numeric format, and applies the Random Forest Classifier to predict sentiment labels. It also evaluates the model's performance using accuracy and confusion matrix"
      ],
      "metadata": {
        "id": "AvK0kJzbFVWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "#print(X_train.head)\n",
        "\n",
        "#print(X_train)\n",
        "#print(X_test)\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the training data and transform it\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the same vectorizer\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Output the shapes of the resulting Bag of Words matrices\n",
        "print(f\"Shape of X_train_bow: {X_train_bow.shape}\")\n",
        "print(f\"Shape of X_test_bow: {X_test_bow.shape}\")\n",
        "\n",
        "# Applying Random Forest Classifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "rf.fit(X_train_bow,y_train)\n",
        "y_pred = rf.predict(X_test_bow)\n",
        "#accuracy_score(y_test,y_pred)\n",
        "\n",
        "print (accuracy_score(y_test,y_pred))\n",
        "print (confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu52GS7sSEbM",
        "outputId": "ca388082-7675-4791-8a4b-24be98bb31cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24000,)\n",
            "Shape of X_train_bow: (24000, 139736)\n",
            "Shape of X_test_bow: (6000, 139736)\n",
            "0.8485\n",
            "[[2562  467]\n",
            " [ 442 2529]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**n-gram (2-gram)**"
      ],
      "metadata": {
        "id": "J1sjksiJMUIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates an instance of CountVectorizer, but with a modification: the ngram_range parameter is set to (2, 2).\n",
        "ngram_range=(2, 2) specifies that the vectorizer will extract bigrams (pairs of consecutive words) from the text. This means that instead of counting single words (unigrams), the model will count sequences of two consecutive words (bigrams) in the text.\n",
        "\n",
        "For example, for the sentence \"I love Python,\" the bigrams would be (\"I love\", \"love Python\")."
      ],
      "metadata": {
        "id": "Yl4hXejLGqMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(ngram_range=(2,2))\n",
        "\n",
        "X_train_n_gram = cv.fit_transform(X_train)\n",
        "X_test_n_gram = cv.transform(X_test)\n",
        "\n",
        "# Output the shapes of the resulting Bag of Words matrices\n",
        "print(f\"Shape of X_train_bow: {X_train_n_gram.shape}\")\n",
        "print(f\"Shape of X_test_bow: {X_test_n_gram.shape}\")\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "rf.fit(X_train_n_gram,y_train)\n",
        "y_pred = rf.predict(X_test_n_gram)\n",
        "\n",
        "print (accuracy_score(y_test,y_pred))\n",
        "print (confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m0KGuBSNatZ",
        "outputId": "415324c8-0f76-403c-da0e-33d9d182da45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_bow: (24000, 1500023)\n",
            "Shape of X_test_bow: (6000, 1500023)\n",
            "0.845\n",
            "[[2547  482]\n",
            " [ 448 2523]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF/IDF**"
      ],
      "metadata": {
        "id": "4QsY7r2INm7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code applies TF-IDF (Term Frequency-Inverse Document Frequency) vectorization to the text data and uses Random Forest Classifier to perform sentiment classification. TF-IDF is a popular method for converting text data into a numerical format, where each word is weighted based on its importance in the document and across the entire dataset."
      ],
      "metadata": {
        "id": "WBapvjSOHQEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# Output the shapes of the resulting Bag of Words matrices\n",
        "print(f\"Shape of X_train_bow: {X_train_tfidf.shape}\")\n",
        "print(f\"Shape of X_test_bow: {X_test_tfidf.shape}\")\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "rf.fit(X_train_tfidf,y_train)\n",
        "y_pred = rf.predict(X_test_tfidf)\n",
        "\n",
        "print (accuracy_score(y_test,y_pred))\n",
        "print (confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQNLOLLyNr2v",
        "outputId": "8b09de06-8253-48e9-b6c8-82dd92643956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_bow: (24000, 139736)\n",
            "Shape of X_test_bow: (6000, 139736)\n",
            "0.8335\n",
            "[[2529  500]\n",
            " [ 499 2472]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task:**\n",
        "*   **Add a Python Function for Word-based Tokenization for each of the IMDB reviews data.**\n",
        "*   **After tokenization, add a Python Function to remove Stop Words from the IMDB reviews data.**\n",
        "*   **After Stopword Removal, add a Python Function to perform Lemmitization over IMDB Reviews data.**\n",
        "\n",
        "**After applying the above mentioned data preprocessing steps, again run this code and analyse the performance of the ML models for text classification of IMDB Reviews.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xbqQjfUg4PG3"
      }
    }
  ]
}