{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOffvx7E0ZHu/sX/jFgqzQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimrashaheen001/Programming_for_AI/blob/main/thesisfusion1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/Data-1.zip'\n",
        "extract_path = '/content/dataset'\n",
        "\n",
        "if zipfile.is_zipfile(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(\"❌ The file is not a valid zip file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iofcVcpiGCAg",
        "outputId": "05b74d26-4d5e-4134-e094-c7373cf43bf3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob, os\n",
        "\n",
        "# 1) Load tabular data\n",
        "tab_lookup = pd.read_csv(\"/content/tabular.csv\")\n",
        "\n",
        "# 2) Normalize ImageID column\n",
        "tab_lookup['ImageID'] = tab_lookup['ImageID'].astype(str)\n",
        "\n",
        "# If your CSV has no extension, but your dataset has .png\n",
        "tab_lookup['ImageID'] = tab_lookup['ImageID'].apply(lambda x: x if x.endswith(\".png\") else x + \".png\")\n",
        "\n",
        "# If CSV has .jpg but dataset has .png\n",
        "# tab_lookup['ImageID'] = tab_lookup['ImageID'].str.replace(\".jpg\", \".png\")\n",
        "\n",
        "# 3) Check matching\n",
        "img_files = [os.path.basename(f) for f in glob.glob(\"/content/images/*/*\")]  # adjust to your path\n",
        "img_set = set(img_files)\n",
        "tab_set = set(tab_lookup['ImageID'])\n",
        "\n",
        "print(\"CSV sample:\", list(tab_set)[:5])\n",
        "print(\"Image sample:\", list(img_set)[:5])\n",
        "print(\"Overlap:\", len(img_set & tab_set))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fsU6lfhH8Gy",
        "outputId": "a743d3f9-2f31-4941-c649-727969da89dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV sample: ['OAS1_0031_MR1_mpr-1_147.jpg.png', 'OAS1_0052_MR1_mpr-1_106.jpg.png', 'OAS1_0021_MR1_mpr-3_157.jpg.png', 'OAS1_0001_MR1_mpr-3_112.jpg.png', 'OAS1_0028_MR1_mpr-2_123.jpg.png']\n",
            "Image sample: []\n",
            "Overlap: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Recursively search for images under /content\n",
        "all_imgs = glob.glob(\"/content/**/*.png\", recursive=True) + glob.glob(\"/content/**/*.jpg\", recursive=True)\n",
        "\n",
        "print(\"Total images found:\", len(all_imgs))\n",
        "print(\"Sample paths:\", all_imgs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQHDdqiMIk5b",
        "outputId": "b919186a-387d-4f75-f291-4894f13423df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found: 2982\n",
            "Sample paths: ['/content/dataset/Data/Data/AD/OAS1_0003_MR1_mpr-1_128.jpg', '/content/dataset/Data/Data/AD/OAS1_0021_MR1_mpr-3_117.jpg', '/content/dataset/Data/Data/AD/OAS1_0003_MR1_mpr-1_156.jpg', '/content/dataset/Data/Data/AD/OAS1_0022_MR1_mpr-1_156.jpg', '/content/dataset/Data/Data/AD/OAS1_0015_MR1_mpr-1_112.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# === 1. Load your CSV ===\n",
        "csv_path = \"/content/tabular.csv\"   # change if different\n",
        "tab_lookup = pd.read_csv(csv_path)\n",
        "\n",
        "print(\"[Before] Shape:\", tab_lookup.shape)\n",
        "print(\"[Before] Sample IDs:\", tab_lookup['ImageID'].head().tolist())\n",
        "\n",
        "# === 2. Clean ImageID column ===\n",
        "# remove double extensions like \".jpg.png\"\n",
        "tab_lookup['ImageID'] = tab_lookup['ImageID'].str.replace(\".jpg.png\", \".jpg\", regex=False)\n",
        "\n",
        "# normalize to lowercase (in case your files are lowercase)\n",
        "tab_lookup['ImageID'] = tab_lookup['ImageID'].str.lower()\n",
        "\n",
        "print(\"[After] Sample IDs:\", tab_lookup['ImageID'].head().tolist())\n",
        "\n",
        "# === 3. Collect real image filenames ===\n",
        "all_imgs = glob.glob(\"/content/dataset/Data/Data/**/*.jpg\", recursive=True)\n",
        "img_files = [os.path.basename(f).lower() for f in all_imgs]\n",
        "\n",
        "print(\"Total images found:\", len(img_files))\n",
        "print(\"Sample real files:\", img_files[:5])\n",
        "\n",
        "# === 4. Verify overlap ===\n",
        "tab_set = set(tab_lookup['ImageID'])\n",
        "img_set = set(img_files)\n",
        "overlap = tab_set & img_set\n",
        "\n",
        "print(f\"[Check] CSV IDs: {len(tab_set)}, Image files: {len(img_set)}, Overlap: {len(overlap)}\")\n",
        "\n",
        "if len(overlap) == 0:\n",
        "    print(\"❌ Still no overlap! Check extensions or path issues.\")\n",
        "else:\n",
        "    print(\"✅ Overlap found:\", len(overlap))\n",
        "\n",
        "# === 5. Save fixed CSV ===\n",
        "fixed_path = \"/content/tabular_fixed.csv\"\n",
        "tab_lookup.to_csv(fixed_path, index=False)\n",
        "print(f\"✅ Fixed CSV saved to {fixed_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsS7H-2AkxIh",
        "outputId": "e9c1394b-40c8-4f0b-9694-856ab2947aac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Before] Shape: (2982, 9)\n",
            "[Before] Sample IDs: ['OAS1_0003_MR1_mpr-1_100.jpg', 'OAS1_0003_MR1_mpr-1_101.jpg', 'OAS1_0003_MR1_mpr-1_102.jpg', 'OAS1_0003_MR1_mpr-1_103.jpg', 'OAS1_0003_MR1_mpr-1_104.jpg']\n",
            "[After] Sample IDs: ['oas1_0003_mr1_mpr-1_100.jpg', 'oas1_0003_mr1_mpr-1_101.jpg', 'oas1_0003_mr1_mpr-1_102.jpg', 'oas1_0003_mr1_mpr-1_103.jpg', 'oas1_0003_mr1_mpr-1_104.jpg']\n",
            "Total images found: 2982\n",
            "Sample real files: ['oas1_0003_mr1_mpr-1_128.jpg', 'oas1_0021_mr1_mpr-3_117.jpg', 'oas1_0003_mr1_mpr-1_156.jpg', 'oas1_0022_mr1_mpr-1_156.jpg', 'oas1_0015_mr1_mpr-1_112.jpg']\n",
            "[Check] CSV IDs: 2982, Image files: 2982, Overlap: 2982\n",
            "✅ Overlap found: 2982\n",
            "✅ Fixed CSV saved to /content/tabular_fixed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tab_lookup = pd.read_csv(\"/content/tabular_fixed.csv\")\n"
      ],
      "metadata": {
        "id": "OG4TyGpZlKjy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Multimodal TransUNet + TabNet Top-K Fusion (Robust ID/Class Fix)\n",
        "# ================================\n",
        "# - Auto-fixes CSV ↔ image filename mismatches (case, extensions, double extensions)\n",
        "# - Auto-maps class label aliases (e.g., MCI <-> MCI-1)\n",
        "# - Concatenates TransUNet CLS embeddings + top-k tabular features\n",
        "# - Early stopping + test metrics\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ----------------\n",
        "# Config\n",
        "# ----------------\n",
        "class Cfg:\n",
        "    # Paths\n",
        "    data_root = \"/content/dataset/Data/Data\"\n",
        "    tabular_csv = \"/content/tabular_fixed.csv\"           # full tabular file\n",
        "    topk_csv    = \"/content/tabular_top10_features.csv\"  # optional (from TabNet)\n",
        "    id_col = \"ImageID\"\n",
        "    target_col = \"Class\"   # your CSV uses \"Class\"\n",
        "    # Image\n",
        "    img_size = 224\n",
        "    in_channels = 3\n",
        "    # Train\n",
        "    batch_size = 16\n",
        "    num_workers = 2\n",
        "    seed = 42\n",
        "    epochs = 100\n",
        "    patience = 6\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # Optim\n",
        "    lr_head = 1e-4\n",
        "    lr_backbone = 1e-5\n",
        "    weight_decay = 1e-4\n",
        "    # TransUNet-ish\n",
        "    patch_size = 16\n",
        "    embed_dim = 768\n",
        "    num_heads = 8\n",
        "    num_layers = 4\n",
        "    freeze_backbone = True\n",
        "    unfreeze_epoch = 5\n",
        "\n",
        "cfg = Cfg()\n",
        "\n",
        "# Repro\n",
        "def set_seed(s):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "# ----------------\n",
        "# Filename & class normalization helpers\n",
        "# ----------------\n",
        "def normalize_ext_to_jpg(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize a filename to .jpg and lowercase.\n",
        "    Also fixes double extensions like `.jpg.png` -> `.jpg`, `.png.jpg` -> `.jpg`.\n",
        "    \"\"\"\n",
        "    n = name.strip().lower()\n",
        "    # remove any path parts, keep basename only\n",
        "    n = os.path.basename(n)\n",
        "    # strip double extensions\n",
        "    n = n.replace(\".jpg.png\", \".jpg\").replace(\".png.jpg\", \".jpg\")\n",
        "    # if it has an extension other than jpg/png/jpeg, keep it; otherwise standardize to .jpg\n",
        "    root, ext = os.path.splitext(n)\n",
        "    if ext in [\".jpeg\", \".png\", \".jpg\"]:\n",
        "        n = root + \".jpg\"\n",
        "    return n\n",
        "\n",
        "def stem_without_ext(name: str) -> str:\n",
        "    \"\"\"Lowercased basename stem (no extension).\"\"\"\n",
        "    return os.path.splitext(os.path.basename(name.strip().lower()))[0]\n",
        "\n",
        "def class_alias(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize class labels across CSV and folders.\n",
        "    Adjust here if your CSV/folders use slightly different labels.\n",
        "    \"\"\"\n",
        "    if name is None:\n",
        "        return \"\"\n",
        "    s = str(name).strip()\n",
        "    # Common aliases:\n",
        "    # Some datasets use \"MCI\" in CSV and \"MCI-1\" in folders, or vice versa.\n",
        "    if s.upper() == \"MCI\":\n",
        "        return \"MCI-1\"\n",
        "    if s.upper() == \"MCI-1\":\n",
        "        return \"MCI-1\"\n",
        "    if s.upper() == \"AD\":\n",
        "        return \"AD\"\n",
        "    if s.upper() == \"CN\":\n",
        "        return \"CN\"\n",
        "    # Fallback: return as-is\n",
        "    return s\n",
        "\n",
        "# ----------------\n",
        "# Load tabular (prefer top-k)\n",
        "# ----------------\n",
        "if os.path.exists(cfg.topk_csv):\n",
        "    df_tab = pd.read_csv(cfg.topk_csv)\n",
        "    print(f\"[Info] Loaded top-k features from {cfg.topk_csv} -> shape {df_tab.shape}\")\n",
        "else:\n",
        "    df_tab = pd.read_csv(cfg.tabular_csv)\n",
        "    print(f\"[Warn] {cfg.topk_csv} not found. Falling back to {cfg.tabular_csv} -> shape {df_tab.shape}\")\n",
        "\n",
        "# Basic checks\n",
        "if cfg.id_col not in df_tab.columns or cfg.target_col not in df_tab.columns:\n",
        "    raise ValueError(f\"CSV must contain '{cfg.id_col}' and '{cfg.target_col}'.\")\n",
        "\n",
        "# Drop columns that shouldn't be features if present\n",
        "drop_cols = {cfg.id_col, cfg.target_col}\n",
        "# Drop Sex only if it exists\n",
        "if \"Sex\" in df_tab.columns:\n",
        "    drop_cols.add(\"Sex\")\n",
        "\n",
        "feature_cols = [c for c in df_tab.columns if c not in drop_cols]\n",
        "print(f\"[Info] Using {len(feature_cols)} tabular features: {feature_cols[:8]}{' ...' if len(feature_cols)>8 else ''}\")\n",
        "\n",
        "# Normalize CSV IDs & classes; build lookup dict with *multiple keys* per row\n",
        "tab_dict: Dict[str, Tuple[np.ndarray, str]] = {}\n",
        "no_ext_rows = 0\n",
        "for _, row in df_tab.iterrows():\n",
        "    raw_id = str(row[cfg.id_col])\n",
        "    raw_cls = class_alias(row[cfg.target_col])\n",
        "    feats = row[feature_cols].to_numpy(dtype=np.float32)\n",
        "\n",
        "    # Prepare multiple keys for robust matching\n",
        "    keys = set()\n",
        "\n",
        "    # 1) as normalized .jpg\n",
        "    keys.add(normalize_ext_to_jpg(raw_id))\n",
        "\n",
        "    # 2) if CSV gave a stem (no extension), add both .jpg and .png variants\n",
        "    root, ext = os.path.splitext(raw_id)\n",
        "    if ext == \"\":\n",
        "        no_ext_rows += 1\n",
        "        keys.add((root.strip().lower() + \".jpg\"))\n",
        "        keys.add((root.strip().lower() + \".png\"))\n",
        "\n",
        "    # 3) also allow the pure stem (for safety, we'll only compare against stems if needed)\n",
        "    keys.add(stem_without_ext(raw_id))\n",
        "\n",
        "    for k in keys:\n",
        "        tab_dict[k] = (feats, raw_cls)\n",
        "\n",
        "if no_ext_rows:\n",
        "    print(f\"[Info] CSV rows without extension: {no_ext_rows}\")\n",
        "\n",
        "# ----------------\n",
        "# Image transforms\n",
        "# ----------------\n",
        "def to_three_channels(img_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"If tensor is (1,H,W) replicate to (3,H,W).\"\"\"\n",
        "    if img_tensor.ndim == 3 and img_tensor.size(0) == 1:\n",
        "        return img_tensor.repeat(3, 1, 1)\n",
        "    return img_tensor\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(25),\n",
        "    transforms.RandomResizedCrop(cfg.img_size, scale=(0.85, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(to_three_channels),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(to_three_channels),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "# ----------------\n",
        "# Dataset: robust join by filename (with and without ext) + class aliasing\n",
        "# ----------------\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, image_folder: ImageFolder, tab_dict: Dict[str, Tuple[np.ndarray, str]],\n",
        "                 feature_cols: List[str], is_train: bool, debug_limit: int = 10):\n",
        "        super().__init__()\n",
        "        self.ifolder = image_folder\n",
        "        self.tab_dict = tab_dict\n",
        "        self.feature_cols = feature_cols\n",
        "\n",
        "        kept, miss_tab, cls_mismatch = 0, 0, 0\n",
        "        missing_examples, mismatch_examples = [], []\n",
        "\n",
        "        filtered_samples: List[Tuple[str, int]] = []\n",
        "        for path, y in self.ifolder.samples:\n",
        "            base = os.path.basename(path)\n",
        "            base_norm = normalize_ext_to_jpg(base)          # e.g., abc.jpg (lower)\n",
        "            base_stem = stem_without_ext(base_norm)         # e.g., abc\n",
        "            folder_cls = class_alias(self.ifolder.classes[y])\n",
        "\n",
        "            row = None\n",
        "            # Try matching by normalized filename with extension\n",
        "            if base_norm in self.tab_dict:\n",
        "                row = self.tab_dict[base_norm]\n",
        "            # Else try by original lower filename (in case extension normalized differently in CSV)\n",
        "            elif base.lower() in self.tab_dict:\n",
        "                row = self.tab_dict[base.lower()]\n",
        "            # Else try by stem (CSV may have stem key)\n",
        "            elif base_stem in self.tab_dict:\n",
        "                row = self.tab_dict[base_stem]\n",
        "            # Else try also .png variant in case images were .png but normalized\n",
        "            else:\n",
        "                alt_png = base_stem + \".png\"\n",
        "                if alt_png in self.tab_dict:\n",
        "                    row = self.tab_dict[alt_png]\n",
        "\n",
        "            if row is None:\n",
        "                miss_tab += 1\n",
        "                if len(missing_examples) < debug_limit:\n",
        "                    missing_examples.append(base)\n",
        "                continue\n",
        "\n",
        "            feats, csv_cls = row\n",
        "            csv_cls_alias = class_alias(csv_cls)\n",
        "\n",
        "            if csv_cls_alias != folder_cls:\n",
        "                cls_mismatch += 1\n",
        "                if len(mismatch_examples) < debug_limit:\n",
        "                    mismatch_examples.append((base, csv_cls, folder_cls))\n",
        "                continue\n",
        "\n",
        "            filtered_samples.append((path, y))\n",
        "            kept += 1\n",
        "\n",
        "        self.ifolder.samples = filtered_samples\n",
        "        self.ifolder.targets = [s[1] for s in filtered_samples]\n",
        "\n",
        "        mode = \"train\" if is_train else \"eval\"\n",
        "        print(f\"[{mode}] Kept {len(self.ifolder)} images | Missing tab: {miss_tab} | Class mismatch: {cls_mismatch}\")\n",
        "        if miss_tab > 0:\n",
        "            print(f\"[{mode}] Example missing IDs (up to {debug_limit}): {missing_examples}\")\n",
        "        if cls_mismatch > 0:\n",
        "            print(f\"[{mode}] Example class mismatches (up to {debug_limit}): {mismatch_examples}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ifolder)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, y = self.ifolder.samples[idx]\n",
        "        img = self.ifolder.loader(path)\n",
        "        if not isinstance(img, Image.Image):\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "        img = self.ifolder.transform(img)\n",
        "\n",
        "        base = os.path.basename(path)\n",
        "        base_norm = normalize_ext_to_jpg(base)\n",
        "        base_stem = stem_without_ext(base_norm)\n",
        "\n",
        "        # Re-resolve the features the same way as in __init__\n",
        "        row = ( self.tab_dict.get(base_norm)\n",
        "                or self.tab_dict.get(base.lower())\n",
        "                or self.tab_dict.get(base_stem)\n",
        "                or self.tab_dict.get(base_stem + \".png\") )\n",
        "        feats_np, _ = row\n",
        "        feats = torch.from_numpy(feats_np)  # (F,)\n",
        "\n",
        "        return img, feats, y, Path(path).name\n",
        "\n",
        "# ----------------\n",
        "# ImageFolder + splits\n",
        "# ----------------\n",
        "base_folder = ImageFolder(root=cfg.data_root, transform=transform_train)\n",
        "num_classes = len(base_folder.classes)\n",
        "print(f\"[Info] Found {len(base_folder)} images across {num_classes} classes: {base_folder.classes}\")\n",
        "\n",
        "# Same files; separate instances for per-split transforms\n",
        "folder_train = ImageFolder(root=cfg.data_root, transform=transform_train)\n",
        "folder_val   = ImageFolder(root=cfg.data_root, transform=transform_eval)\n",
        "folder_test  = ImageFolder(root=cfg.data_root, transform=transform_eval)\n",
        "\n",
        "# Split (random split across all images)\n",
        "all_indices = list(range(len(base_folder)))\n",
        "random.shuffle(all_indices)\n",
        "n_total = len(all_indices)\n",
        "n_train = int(0.7 * n_total)\n",
        "n_val   = int(0.15 * n_total)\n",
        "n_test  = n_total - n_train - n_val\n",
        "\n",
        "idx_train = all_indices[:n_train]\n",
        "idx_val   = all_indices[n_train:n_train+n_val]\n",
        "idx_test  = all_indices[n_train+n_val:]\n",
        "\n",
        "def subset_folder(in_folder: ImageFolder, keep_indices: List[int]) -> ImageFolder:\n",
        "    new = ImageFolder(root=in_folder.root, transform=in_folder.transform, loader=in_folder.loader)\n",
        "    new.classes = in_folder.classes\n",
        "    new.class_to_idx = in_folder.class_to_idx\n",
        "    new.samples = [in_folder.samples[i] for i in keep_indices]\n",
        "    new.targets = [s[1] for s in new.samples]\n",
        "    return new\n",
        "\n",
        "folder_train = subset_folder(folder_train, idx_train)\n",
        "folder_val   = subset_folder(folder_val, idx_val)\n",
        "folder_test  = subset_folder(folder_test, idx_test)\n",
        "\n",
        "# Wrap with MultiModalDataset\n",
        "ds_train = MultiModalDataset(folder_train, tab_dict, feature_cols, is_train=True)\n",
        "ds_val   = MultiModalDataset(folder_val,   tab_dict, feature_cols, is_train=False)\n",
        "ds_test  = MultiModalDataset(folder_test,  tab_dict, feature_cols, is_train=False)\n",
        "\n",
        "# Safety check to avoid DataLoader on empty datasets\n",
        "def ensure_nonempty(ds, name):\n",
        "    if len(ds) == 0:\n",
        "        raise RuntimeError(\n",
        "            f\"{name} dataset is empty after matching.\\n\"\n",
        "            f\"→ Check the printed 'Missing tab' examples above and adjust normalization rules in class_alias()/normalize_ext_to_jpg().\"\n",
        "        )\n",
        "ensure_nonempty(ds_train, \"Train\")\n",
        "ensure_nonempty(ds_val, \"Val\")\n",
        "ensure_nonempty(ds_test, \"Test\")\n",
        "\n",
        "train_loader = DataLoader(ds_train, batch_size=cfg.batch_size, shuffle=True,  num_workers=cfg.num_workers)\n",
        "val_loader   = DataLoader(ds_val,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "test_loader  = DataLoader(ds_test,  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "\n",
        "# ----------------\n",
        "# TransUNet-ish backbone -> CLS embeddings\n",
        "# ----------------\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)                     # B, E, H/P, W/P\n",
        "        x = x.flatten(2).transpose(1, 2)     # B, N, E\n",
        "        cls = self.cls_token.expand(x.size(0), -1, -1)\n",
        "        x = torch.cat((cls, x), dim=1)       # B, N+1, E\n",
        "        x = x + self.pos_embed\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransUNetEncoder(nn.Module):\n",
        "    \"\"\"Transformer encoder returning normalized CLS embedding.\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_heads=8, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.patch = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
        "                                               dim_feedforward=embed_dim*4, dropout=0.1,\n",
        "                                               batch_first=False)\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch(x)         # B, N+1, E\n",
        "        x = x.permute(1,0,2)      # S, B, E\n",
        "        x = self.encoder(x)       # S, B, E\n",
        "        cls = x[0]                # B, E\n",
        "        return self.norm(cls)     # B, E\n",
        "\n",
        "# ----------------\n",
        "# Fusion head\n",
        "# ----------------\n",
        "class FusionHead(nn.Module):\n",
        "    def __init__(self, img_dim: int, tab_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(img_dim + tab_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_feat, tab_feat):\n",
        "        x = torch.cat([img_feat, tab_feat], dim=1)\n",
        "        return self.net(x)\n",
        "\n",
        "# ----------------\n",
        "# Build models\n",
        "# ----------------\n",
        "img_encoder = TransUNetEncoder(\n",
        "    img_size=cfg.img_size, patch_size=cfg.patch_size, in_channels=cfg.in_channels,\n",
        "    embed_dim=cfg.embed_dim, num_heads=cfg.num_heads, num_layers=cfg.num_layers\n",
        ").to(cfg.device)\n",
        "\n",
        "fusion_head = FusionHead(img_dim=cfg.embed_dim, tab_dim=len(feature_cols), num_classes=len(base_folder.classes)).to(cfg.device)\n",
        "\n",
        "# Freeze backbone initially (optional)\n",
        "def set_freeze(model: nn.Module, freeze: bool):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = not freeze\n",
        "\n",
        "set_freeze(img_encoder, cfg.freeze_backbone)\n",
        "\n",
        "# Optimizer with two groups\n",
        "optim_params = [\n",
        "    {\"params\": fusion_head.parameters(), \"lr\": cfg.lr_head},\n",
        "    {\"params\": [p for p in img_encoder.parameters() if p.requires_grad], \"lr\": cfg.lr_backbone},\n",
        "]\n",
        "optimizer = optim.AdamW(optim_params, weight_decay=cfg.weight_decay)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# ----------------\n",
        "# Train / Eval\n",
        "# ----------------\n",
        "def run_epoch(loader, train: bool, epoch: int, desc: str):\n",
        "    (img_encoder.train() if train else img_encoder.eval())\n",
        "    (fusion_head.train() if train else fusion_head.eval())\n",
        "\n",
        "    total_loss, preds_all, labels_all = 0.0, [], []\n",
        "    loop = tqdm(loader, desc=desc, leave=False)\n",
        "\n",
        "    for imgs, tabs, ys, _names in loop:\n",
        "        imgs = imgs.to(cfg.device)\n",
        "        tabs = tabs.to(cfg.device).float()\n",
        "        ys   = ys.to(cfg.device).long()\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            img_feats = img_encoder(imgs)\n",
        "            logits = fusion_head(img_feats, tabs)\n",
        "            loss = criterion(logits, ys)\n",
        "\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        preds_all.extend(logits.argmax(1).detach().cpu().numpy())\n",
        "        labels_all.extend(ys.detach().cpu().numpy())\n",
        "\n",
        "    epoch_loss = total_loss / len(loader.dataset)\n",
        "    acc = accuracy_score(labels_all, preds_all)\n",
        "    f1  = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "    return epoch_loss, acc, f1\n",
        "\n",
        "best_val_acc = 0.0\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    # optional unfreeze\n",
        "    if cfg.freeze_backbone and epoch == cfg.unfreeze_epoch:\n",
        "        print(f\"[Info] Unfreezing image encoder at epoch {epoch}\")\n",
        "        set_freeze(img_encoder, False)\n",
        "        optimizer = optim.AdamW([\n",
        "            {\"params\": fusion_head.parameters(), \"lr\": cfg.lr_head},\n",
        "            {\"params\": img_encoder.parameters(), \"lr\": cfg.lr_backbone},\n",
        "        ], weight_decay=cfg.weight_decay)\n",
        "\n",
        "    tr_loss, tr_acc, tr_f1 = run_epoch(train_loader, True, epoch, f\"Epoch {epoch+1}/{cfg.epochs} [Train]\")\n",
        "    va_loss, va_acc, va_f1 = run_epoch(val_loader,   False, epoch, f\"Epoch {epoch+1}/{cfg.epochs} [Val]\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | \"\n",
        "          f\"Train: loss {tr_loss:.4f} acc {tr_acc:.4f} f1 {tr_f1:.4f} || \"\n",
        "          f\"Val: loss {va_loss:.4f} acc {va_acc:.4f} f1 {va_f1:.4f}\")\n",
        "\n",
        "    if va_acc > best_val_acc:\n",
        "        best_val_acc = va_acc\n",
        "        wait = 0\n",
        "        torch.save({\n",
        "            \"img_encoder\": img_encoder.state_dict(),\n",
        "            \"fusion_head\": fusion_head.state_dict(),\n",
        "            \"classes\": base_folder.classes,\n",
        "            \"feature_cols\": feature_cols,\n",
        "            \"cfg\": cfg.__dict__,\n",
        "        }, \"best_multimodal.pth\")\n",
        "        print(f\"[+] Saved best model (val_acc={va_acc:.4f})\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= cfg.patience:\n",
        "            print(\"⏹️ Early stopping.\")\n",
        "            break\n",
        "\n",
        "# ----------------\n",
        "# Test\n",
        "# ----------------\n",
        "ckpt = torch.load(\"best_multimodal.pth\", map_location=cfg.device)\n",
        "img_encoder.load_state_dict(ckpt[\"img_encoder\"])\n",
        "fusion_head.load_state_dict(ckpt[\"fusion_head\"])\n",
        "img_encoder.eval(); fusion_head.eval()\n",
        "\n",
        "te_loss, te_acc, te_f1 = run_epoch(test_loader, False, 0, \"Test\")\n",
        "print(f\"TEST → Loss: {te_loss:.4f} | Acc: {te_acc:.4f} | F1: {te_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DKA_hbqISet",
        "outputId": "a637b6f3-3366-4a2c-860e-a38f3db6f2a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Warn] /content/tabular_top10_features.csv not found. Falling back to /content/tabular_fixed.csv -> shape (2982, 9)\n",
            "[Info] Using 6 tabular features: ['Age', 'Years_in_Education', 'APOE4', 'CSF_AB', 'CSF_P-tau181', 'CSF_T-tau']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Found 2982 images across 3 classes: ['AD', 'CN', 'MCI-1']\n",
            "[train] Kept 2087 images | Missing tab: 0 | Class mismatch: 0\n",
            "[eval] Kept 447 images | Missing tab: 0 | Class mismatch: 0\n",
            "[eval] Kept 448 images | Missing tab: 0 | Class mismatch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train: loss 1.6924 acc 0.3589 f1 0.3513 || Val: loss 1.0995 acc 0.3602 f1 0.3515\n",
            "[+] Saved best model (val_acc=0.3602)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | Train: loss 1.2486 acc 0.3536 f1 0.3487 || Val: loss 1.1195 acc 0.2953 f1 0.2635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | Train: loss 1.1795 acc 0.3220 f1 0.3147 || Val: loss 1.1057 acc 0.3065 f1 0.3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | Train: loss 1.1392 acc 0.3301 f1 0.3273 || Val: loss 1.1044 acc 0.3087 f1 0.3015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | Train: loss 1.1252 acc 0.3239 f1 0.3203 || Val: loss 1.1019 acc 0.3266 f1 0.3246\n",
            "[Info] Unfreezing image encoder at epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 | Train: loss 1.1196 acc 0.3373 f1 0.3324 || Val: loss 1.0998 acc 0.3624 f1 0.2693\n",
            "[+] Saved best model (val_acc=0.3624)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 | Train: loss 1.1108 acc 0.3512 f1 0.3359 || Val: loss 1.0982 acc 0.3535 f1 0.2927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | Train: loss 1.1076 acc 0.3498 f1 0.3273 || Val: loss 1.0952 acc 0.3915 f1 0.3103\n",
            "[+] Saved best model (val_acc=0.3915)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | Train: loss 1.0846 acc 0.4054 f1 0.3647 || Val: loss 1.0188 acc 0.4832 f1 0.4531\n",
            "[+] Saved best model (val_acc=0.4832)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train: loss 1.0216 acc 0.4854 f1 0.4676 || Val: loss 1.0345 acc 0.4720 f1 0.3804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Train: loss 0.9892 acc 0.5065 f1 0.5000 || Val: loss 0.9720 acc 0.5503 f1 0.4571\n",
            "[+] Saved best model (val_acc=0.5503)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Train: loss 0.9795 acc 0.4988 f1 0.4942 || Val: loss 0.9688 acc 0.5324 f1 0.4398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Train: loss 0.9684 acc 0.5165 f1 0.5047 || Val: loss 0.9456 acc 0.5436 f1 0.4914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Train: loss 0.9782 acc 0.5002 f1 0.4978 || Val: loss 0.9456 acc 0.5391 f1 0.4673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Train: loss 0.9629 acc 0.5228 f1 0.5130 || Val: loss 0.9454 acc 0.5011 f1 0.4691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Train: loss 0.9690 acc 0.5108 f1 0.4951 || Val: loss 0.9666 acc 0.5481 f1 0.4261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Train: loss 0.9647 acc 0.5218 f1 0.5124 || Val: loss 0.9348 acc 0.5682 f1 0.5115\n",
            "[+] Saved best model (val_acc=0.5682)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Train: loss 0.9655 acc 0.5137 f1 0.4931 || Val: loss 0.9734 acc 0.5168 f1 0.4137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Train: loss 0.9567 acc 0.5204 f1 0.5058 || Val: loss 0.9348 acc 0.5436 f1 0.4509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Train: loss 0.9543 acc 0.5175 f1 0.4917 || Val: loss 0.9265 acc 0.5459 f1 0.4463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 | Train: loss 0.9381 acc 0.5381 f1 0.5155 || Val: loss 0.9384 acc 0.5526 f1 0.4517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 | Train: loss 0.9530 acc 0.5276 f1 0.4971 || Val: loss 0.9287 acc 0.5526 f1 0.4850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 | Train: loss 0.9484 acc 0.5261 f1 0.5055 || Val: loss 0.9184 acc 0.5749 f1 0.5458\n",
            "[+] Saved best model (val_acc=0.5749)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 | Train: loss 0.9394 acc 0.5414 f1 0.5252 || Val: loss 0.9132 acc 0.5749 f1 0.5359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 | Train: loss 0.9396 acc 0.5280 f1 0.5041 || Val: loss 0.9523 acc 0.5638 f1 0.5367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 | Train: loss 0.9340 acc 0.5529 f1 0.5263 || Val: loss 0.9139 acc 0.5906 f1 0.5390\n",
            "[+] Saved best model (val_acc=0.5906)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 | Train: loss 0.9312 acc 0.5515 f1 0.5218 || Val: loss 0.9022 acc 0.5951 f1 0.5133\n",
            "[+] Saved best model (val_acc=0.5951)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 | Train: loss 0.9294 acc 0.5472 f1 0.5194 || Val: loss 0.9376 acc 0.5638 f1 0.4997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 | Train: loss 0.9238 acc 0.5592 f1 0.5319 || Val: loss 0.9673 acc 0.5213 f1 0.4271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 | Train: loss 0.9090 acc 0.5664 f1 0.5412 || Val: loss 0.8979 acc 0.6353 f1 0.6077\n",
            "[+] Saved best model (val_acc=0.6353)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 | Train: loss 0.9077 acc 0.5798 f1 0.5593 || Val: loss 0.8757 acc 0.6085 f1 0.5800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 | Train: loss 0.8931 acc 0.6138 f1 0.6079 || Val: loss 0.8656 acc 0.6756 f1 0.6605\n",
            "[+] Saved best model (val_acc=0.6756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 | Train: loss 0.9060 acc 0.5846 f1 0.5787 || Val: loss 0.9007 acc 0.6130 f1 0.6058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 | Train: loss 0.8951 acc 0.5966 f1 0.5877 || Val: loss 0.8501 acc 0.6734 f1 0.6656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 | Train: loss 0.8847 acc 0.6128 f1 0.6076 || Val: loss 0.8209 acc 0.7092 f1 0.6826\n",
            "[+] Saved best model (val_acc=0.7092)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 | Train: loss 0.8846 acc 0.6191 f1 0.6135 || Val: loss 0.8642 acc 0.6130 f1 0.5768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 | Train: loss 0.8602 acc 0.6243 f1 0.6200 || Val: loss 0.8152 acc 0.6890 f1 0.6804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 | Train: loss 0.8431 acc 0.6406 f1 0.6353 || Val: loss 0.8242 acc 0.6465 f1 0.6134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 | Train: loss 0.8332 acc 0.6608 f1 0.6593 || Val: loss 0.8049 acc 0.6667 f1 0.6354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 | Train: loss 0.8551 acc 0.6397 f1 0.6380 || Val: loss 0.8322 acc 0.6890 f1 0.6809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 | Train: loss 0.8414 acc 0.6655 f1 0.6640 || Val: loss 0.7678 acc 0.7181 f1 0.7105\n",
            "[+] Saved best model (val_acc=0.7181)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 | Train: loss 0.8200 acc 0.6747 f1 0.6735 || Val: loss 0.7618 acc 0.7181 f1 0.6759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 | Train: loss 0.8365 acc 0.6536 f1 0.6501 || Val: loss 0.7673 acc 0.7338 f1 0.7305\n",
            "[+] Saved best model (val_acc=0.7338)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 | Train: loss 0.8090 acc 0.6804 f1 0.6782 || Val: loss 0.7239 acc 0.7673 f1 0.7478\n",
            "[+] Saved best model (val_acc=0.7673)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 | Train: loss 0.8044 acc 0.6761 f1 0.6699 || Val: loss 0.8082 acc 0.6622 f1 0.6371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 | Train: loss 0.7742 acc 0.7092 f1 0.7080 || Val: loss 0.7157 acc 0.7405 f1 0.7373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 | Train: loss 0.7854 acc 0.7034 f1 0.7016 || Val: loss 0.7346 acc 0.7069 f1 0.7023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 | Train: loss 0.7823 acc 0.7120 f1 0.7107 || Val: loss 0.7378 acc 0.7204 f1 0.7057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 | Train: loss 0.7640 acc 0.7230 f1 0.7226 || Val: loss 0.7099 acc 0.7248 f1 0.7091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50 | Train: loss 0.7512 acc 0.7360 f1 0.7298 || Val: loss 0.7018 acc 0.7204 f1 0.7104\n",
            "⏹️ Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST → Loss: 0.7601 | Acc: 0.7210 | F1: 0.7107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Multimodal TransUNet + TabNet Top-K Fusion (No Early Stopping)\n",
        "# ================================\n",
        "# - Robust CSV ↔ image filename matching\n",
        "# - Normalized class aliases\n",
        "# - TransUNet encoder for images\n",
        "# - TabNet (pytorch-tabnet) for tabular features selection (optional top-k CSV)\n",
        "# - Concatenation fusion\n",
        "# - Train for fixed epochs (no early stopping)\n",
        "\n",
        "!pip install pytorch-tabnet -q\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ----------------\n",
        "# Config\n",
        "# ----------------\n",
        "class Cfg:\n",
        "    data_root = \"/content/dataset/Data/Data\"\n",
        "    tabular_csv = \"/content/tabular_fixed.csv\"\n",
        "    topk_csv    = \"/content/tabular_top10_features.csv\"\n",
        "    id_col = \"ImageID\"\n",
        "    target_col = \"Class\"\n",
        "    img_size = 224\n",
        "    in_channels = 3\n",
        "    batch_size = 16\n",
        "    num_workers = 2\n",
        "    seed = 42\n",
        "    epochs = 100     # fixed training (no early stopping)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    lr_head = 1e-4\n",
        "    lr_backbone = 1e-5\n",
        "    weight_decay = 1e-4\n",
        "    patch_size = 16\n",
        "    embed_dim = 768\n",
        "    num_heads = 8\n",
        "    num_layers = 4\n",
        "    freeze_backbone = True\n",
        "    unfreeze_epoch = 5\n",
        "\n",
        "cfg = Cfg()\n",
        "\n",
        "def set_seed(s):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "# ----------------\n",
        "# Normalize helpers\n",
        "# ----------------\n",
        "def normalize_ext_to_jpg(name: str) -> str:\n",
        "    n = os.path.basename(name.strip().lower())\n",
        "    n = n.replace(\".jpg.png\", \".jpg\").replace(\".png.jpg\", \".jpg\")\n",
        "    root, ext = os.path.splitext(n)\n",
        "    if ext in [\".jpeg\", \".png\", \".jpg\"]:\n",
        "        n = root + \".jpg\"\n",
        "    return n\n",
        "\n",
        "def stem_without_ext(name: str) -> str:\n",
        "    return os.path.splitext(os.path.basename(name.strip().lower()))[0]\n",
        "\n",
        "def class_alias(name: str) -> str:\n",
        "    if name is None: return \"\"\n",
        "    s = str(name).strip()\n",
        "    if s.upper() in [\"MCI\", \"MCI-1\"]: return \"MCI-1\"\n",
        "    if s.upper() == \"AD\": return \"AD\"\n",
        "    if s.upper() == \"CN\": return \"CN\"\n",
        "    return s\n",
        "\n",
        "# ----------------\n",
        "# Load Tabular\n",
        "# ----------------\n",
        "if os.path.exists(cfg.topk_csv):\n",
        "    df_tab = pd.read_csv(cfg.topk_csv)\n",
        "    print(f\"[Info] Loaded top-k features {df_tab.shape}\")\n",
        "else:\n",
        "    df_tab = pd.read_csv(cfg.tabular_csv)\n",
        "    print(f\"[Warn] Using full tabular {df_tab.shape}\")\n",
        "\n",
        "drop_cols = {cfg.id_col, cfg.target_col}\n",
        "if \"Sex\" in df_tab.columns: drop_cols.add(\"Sex\")\n",
        "feature_cols = [c for c in df_tab.columns if c not in drop_cols]\n",
        "\n",
        "tab_dict: Dict[str, Tuple[np.ndarray, str]] = {}\n",
        "for _, row in df_tab.iterrows():\n",
        "    raw_id = str(row[cfg.id_col])\n",
        "    feats = row[feature_cols].to_numpy(dtype=np.float32)\n",
        "    raw_cls = class_alias(row[cfg.target_col])\n",
        "    keys = {normalize_ext_to_jpg(raw_id), stem_without_ext(raw_id)}\n",
        "    for k in keys:\n",
        "        tab_dict[k] = (feats, raw_cls)\n",
        "\n",
        "print(f\"[Info] Features: {len(feature_cols)}\")\n",
        "\n",
        "# ----------------\n",
        "# Dataset\n",
        "# ----------------\n",
        "def to_three_channels(img_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    return img_tensor.repeat(3,1,1) if img_tensor.ndim==3 and img_tensor.size(0)==1 else img_tensor\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(25),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(to_three_channels),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
        "])\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(to_three_channels),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
        "])\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, image_folder: ImageFolder, tab_dict, is_train=True):\n",
        "        self.ifolder = image_folder\n",
        "        self.tab_dict = tab_dict\n",
        "        filtered_samples = []\n",
        "        for path, y in self.ifolder.samples:\n",
        "            base = os.path.basename(path)\n",
        "            key = normalize_ext_to_jpg(base)\n",
        "            key2 = stem_without_ext(base)\n",
        "            row = tab_dict.get(key) or tab_dict.get(key2)\n",
        "            if row:\n",
        "                feats, csv_cls = row\n",
        "                folder_cls = class_alias(self.ifolder.classes[y])\n",
        "                if csv_cls == folder_cls:\n",
        "                    filtered_samples.append((path, y))\n",
        "        self.ifolder.samples = filtered_samples\n",
        "        self.ifolder.targets = [s[1] for s in filtered_samples]\n",
        "\n",
        "    def __len__(self): return len(self.ifolder)\n",
        "    def __getitem__(self, idx):\n",
        "        path, y = self.ifolder.samples[idx]\n",
        "        img = self.ifolder.transform(Image.open(path).convert(\"RGB\"))\n",
        "        base = normalize_ext_to_jpg(os.path.basename(path))\n",
        "        feats, _ = self.tab_dict.get(base) or self.tab_dict[stem_without_ext(base)]\n",
        "        feats = torch.from_numpy(feats)\n",
        "        return img, feats, y, Path(path).name\n",
        "\n",
        "# ----------------\n",
        "# Splits\n",
        "# ----------------\n",
        "base_folder = ImageFolder(cfg.data_root, transform=transform_train)\n",
        "num_classes = len(base_folder.classes)\n",
        "print(f\"[Info] {len(base_folder)} images across {num_classes} classes\")\n",
        "\n",
        "# random split\n",
        "all_idx = list(range(len(base_folder)))\n",
        "random.shuffle(all_idx)\n",
        "n = len(all_idx)\n",
        "n_train, n_val = int(0.7*n), int(0.15*n)\n",
        "train_idx, val_idx, test_idx = all_idx[:n_train], all_idx[n_train:n_train+n_val], all_idx[n_train+n_val:]\n",
        "\n",
        "def subset(in_folder, idxs, transform):\n",
        "    new = ImageFolder(in_folder.root, transform=transform)\n",
        "    new.samples = [in_folder.samples[i] for i in idxs]\n",
        "    new.targets = [s[1] for s in new.samples]\n",
        "    return new\n",
        "\n",
        "ds_train = MultiModalDataset(subset(base_folder, train_idx, transform_train), tab_dict, True)\n",
        "ds_val   = MultiModalDataset(subset(base_folder, val_idx, transform_eval), tab_dict, False)\n",
        "ds_test  = MultiModalDataset(subset(base_folder, test_idx, transform_eval), tab_dict, False)\n",
        "\n",
        "train_loader = DataLoader(ds_train, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
        "val_loader   = DataLoader(ds_val,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "test_loader  = DataLoader(ds_test,  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "\n",
        "# ----------------\n",
        "# Models\n",
        "# ----------------\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, patch_size, patch_size)\n",
        "        num_patches = (img_size//patch_size)**2\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
        "        self.pos = nn.Parameter(torch.zeros(1,num_patches+1,embed_dim))\n",
        "    def forward(self,x):\n",
        "        x = self.proj(x).flatten(2).transpose(1,2)\n",
        "        cls = self.cls.expand(x.size(0),-1,-1)\n",
        "        x = torch.cat([cls,x],1) + self.pos\n",
        "        return x\n",
        "\n",
        "class TransUNetEncoder(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels, embed_dim, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.patch = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        enc_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, embed_dim*4, 0.1, batch_first=False)\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "    def forward(self,x):\n",
        "        x = self.patch(x).permute(1,0,2)\n",
        "        x = self.encoder(x)[0]\n",
        "        return self.norm(x)\n",
        "\n",
        "class FusionHead(nn.Module):\n",
        "    def __init__(self,img_dim,tab_dim,num_classes):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(img_dim+tab_dim,64),nn.ReLU(),nn.Dropout(0.3),\n",
        "            nn.Linear(64,32),nn.ReLU(),nn.Dropout(0.3),\n",
        "            nn.Linear(32,num_classes)\n",
        "        )\n",
        "    def forward(self,img_feat,tab_feat): return self.net(torch.cat([img_feat,tab_feat],1))\n",
        "\n",
        "img_encoder = TransUNetEncoder(cfg.img_size,cfg.patch_size,cfg.in_channels,cfg.embed_dim,cfg.num_heads,cfg.num_layers).to(cfg.device)\n",
        "fusion_head = FusionHead(cfg.embed_dim,len(feature_cols),num_classes).to(cfg.device)\n",
        "\n",
        "# freeze/unfreeze\n",
        "for p in img_encoder.parameters(): p.requires_grad = not cfg.freeze_backbone\n",
        "optimizer = optim.AdamW([\n",
        "    {\"params\": fusion_head.parameters(), \"lr\": cfg.lr_head},\n",
        "    {\"params\": [p for p in img_encoder.parameters() if p.requires_grad], \"lr\": cfg.lr_backbone}\n",
        "], weight_decay=cfg.weight_decay)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# ----------------\n",
        "# Train (no early stopping)\n",
        "# ----------------\n",
        "def run_epoch(loader,train):\n",
        "    img_encoder.train(train); fusion_head.train(train)\n",
        "    total_loss,preds,labels=0,[],[]\n",
        "    for imgs,tabs,ys,_ in loader:\n",
        "        imgs,tabs,ys = imgs.to(cfg.device), tabs.to(cfg.device).float(), ys.to(cfg.device)\n",
        "        with torch.set_grad_enabled(train):\n",
        "            feats = img_encoder(imgs)\n",
        "            out = fusion_head(feats,tabs)\n",
        "            loss = criterion(out,ys)\n",
        "            if train:\n",
        "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item()*imgs.size(0)\n",
        "        preds.extend(out.argmax(1).cpu().numpy()); labels.extend(ys.cpu().numpy())\n",
        "    return total_loss/len(loader.dataset), accuracy_score(labels,preds), f1_score(labels,preds,average=\"macro\")\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    if cfg.freeze_backbone and epoch==cfg.unfreeze_epoch:\n",
        "        for p in img_encoder.parameters(): p.requires_grad=True\n",
        "        optimizer = optim.AdamW(list(img_encoder.parameters())+list(fusion_head.parameters()),lr=cfg.lr_backbone,weight_decay=cfg.weight_decay)\n",
        "        print(f\"[Info] Unfrozen backbone at epoch {epoch}\")\n",
        "    tr_loss,tr_acc,tr_f1 = run_epoch(train_loader,True)\n",
        "    va_loss,va_acc,va_f1 = run_epoch(val_loader,False)\n",
        "    print(f\"Epoch {epoch+1:02d}/{cfg.epochs} | Train {tr_loss:.4f} acc {tr_acc:.4f} f1 {tr_f1:.4f} || Val {va_loss:.4f} acc {va_acc:.4f} f1 {va_f1:.4f}\")\n",
        "\n",
        "# ----------------\n",
        "# Test\n",
        "# ----------------\n",
        "te_loss,te_acc,te_f1 = run_epoch(test_loader,False)\n",
        "print(f\"TEST → Loss {te_loss:.4f} | Acc {te_acc:.4f} | F1 {te_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyGSI5emwPX0",
        "outputId": "5a01415c-93f9-43e1-c2f3-0597dea177ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[Warn] Using full tabular (2982, 9)\n",
            "[Info] Features: 6\n",
            "[Info] 2982 images across 3 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/100 | Train 1.6568 acc 0.3594 f1 0.3513 || Val 1.0994 acc 0.3445 f1 0.3025\n",
            "Epoch 02/100 | Train 1.2583 acc 0.3507 f1 0.3449 || Val 1.1043 acc 0.3311 f1 0.3198\n",
            "Epoch 03/100 | Train 1.1663 acc 0.3349 f1 0.3265 || Val 1.1103 acc 0.3132 f1 0.2831\n",
            "Epoch 04/100 | Train 1.1377 acc 0.3373 f1 0.3337 || Val 1.1004 acc 0.3445 f1 0.3446\n",
            "Epoch 05/100 | Train 1.1242 acc 0.3402 f1 0.3272 || Val 1.1029 acc 0.3423 f1 0.2896\n",
            "[Info] Unfrozen backbone at epoch 5\n",
            "Epoch 06/100 | Train 1.1184 acc 0.3445 f1 0.3220 || Val 1.1021 acc 0.3423 f1 0.2952\n",
            "Epoch 07/100 | Train 1.1074 acc 0.3637 f1 0.3447 || Val 1.0980 acc 0.3468 f1 0.3106\n",
            "Epoch 08/100 | Train 1.0986 acc 0.3805 f1 0.3573 || Val 1.0674 acc 0.4653 f1 0.3686\n",
            "Epoch 09/100 | Train 1.0636 acc 0.4327 f1 0.4047 || Val 1.0069 acc 0.4989 f1 0.4930\n",
            "Epoch 10/100 | Train 1.0312 acc 0.4705 f1 0.4554 || Val 0.9939 acc 0.5145 f1 0.4951\n",
            "Epoch 11/100 | Train 1.0207 acc 0.4777 f1 0.4678 || Val 0.9980 acc 0.5168 f1 0.4802\n",
            "Epoch 12/100 | Train 1.0135 acc 0.4921 f1 0.4831 || Val 0.9797 acc 0.5391 f1 0.4493\n",
            "Epoch 13/100 | Train 1.0049 acc 0.5050 f1 0.4866 || Val 0.9626 acc 0.5257 f1 0.4390\n",
            "Epoch 14/100 | Train 0.9963 acc 0.4931 f1 0.4815 || Val 0.9506 acc 0.5324 f1 0.4452\n",
            "Epoch 15/100 | Train 0.9823 acc 0.5137 f1 0.4953 || Val 0.9499 acc 0.5682 f1 0.5020\n",
            "Epoch 16/100 | Train 0.9883 acc 0.4945 f1 0.4748 || Val 0.9628 acc 0.5660 f1 0.4877\n",
            "Epoch 17/100 | Train 0.9751 acc 0.5256 f1 0.5071 || Val 0.9539 acc 0.5727 f1 0.4837\n",
            "Epoch 18/100 | Train 0.9763 acc 0.5127 f1 0.4955 || Val 0.9464 acc 0.5526 f1 0.4646\n",
            "Epoch 19/100 | Train 0.9698 acc 0.5170 f1 0.5006 || Val 0.9385 acc 0.5951 f1 0.5407\n",
            "Epoch 20/100 | Train 0.9658 acc 0.5242 f1 0.5014 || Val 0.9324 acc 0.5996 f1 0.5402\n",
            "Epoch 21/100 | Train 0.9561 acc 0.5414 f1 0.5197 || Val 0.9085 acc 0.5928 f1 0.5042\n",
            "Epoch 22/100 | Train 0.9642 acc 0.5319 f1 0.5135 || Val 0.9285 acc 0.6197 f1 0.5835\n",
            "Epoch 23/100 | Train 0.9564 acc 0.5232 f1 0.5117 || Val 0.9105 acc 0.6331 f1 0.5860\n",
            "Epoch 24/100 | Train 0.9445 acc 0.5510 f1 0.5372 || Val 0.9127 acc 0.6063 f1 0.5698\n",
            "Epoch 25/100 | Train 0.9420 acc 0.5582 f1 0.5362 || Val 0.8868 acc 0.6667 f1 0.6274\n",
            "Epoch 26/100 | Train 0.9358 acc 0.5664 f1 0.5537 || Val 0.9606 acc 0.5749 f1 0.5700\n",
            "Epoch 27/100 | Train 0.9269 acc 0.5606 f1 0.5406 || Val 0.8875 acc 0.6465 f1 0.5696\n",
            "Epoch 28/100 | Train 0.9410 acc 0.5755 f1 0.5567 || Val 0.9265 acc 0.5615 f1 0.5064\n",
            "Epoch 29/100 | Train 0.9097 acc 0.5788 f1 0.5673 || Val 0.8609 acc 0.6868 f1 0.6613\n",
            "Epoch 30/100 | Train 0.9079 acc 0.6124 f1 0.6048 || Val 0.8507 acc 0.6644 f1 0.6417\n",
            "Epoch 31/100 | Train 0.9028 acc 0.5980 f1 0.5865 || Val 0.8253 acc 0.6846 f1 0.6615\n",
            "Epoch 32/100 | Train 0.8784 acc 0.6282 f1 0.6173 || Val 0.8080 acc 0.7114 f1 0.6976\n",
            "Epoch 33/100 | Train 0.8861 acc 0.6210 f1 0.6130 || Val 0.7960 acc 0.6756 f1 0.6555\n",
            "Epoch 34/100 | Train 0.8570 acc 0.6545 f1 0.6500 || Val 0.7927 acc 0.7248 f1 0.7160\n",
            "Epoch 35/100 | Train 0.8502 acc 0.6526 f1 0.6477 || Val 0.7774 acc 0.7002 f1 0.6827\n",
            "Epoch 36/100 | Train 0.8553 acc 0.6560 f1 0.6498 || Val 0.7631 acc 0.7315 f1 0.7210\n",
            "Epoch 37/100 | Train 0.8400 acc 0.6660 f1 0.6610 || Val 0.7751 acc 0.7584 f1 0.7549\n",
            "Epoch 38/100 | Train 0.8412 acc 0.6675 f1 0.6675 || Val 0.7590 acc 0.7136 f1 0.7031\n",
            "Epoch 39/100 | Train 0.8117 acc 0.6962 f1 0.6926 || Val 0.7405 acc 0.7427 f1 0.7270\n",
            "Epoch 40/100 | Train 0.8252 acc 0.6799 f1 0.6782 || Val 0.7675 acc 0.7204 f1 0.7022\n",
            "Epoch 41/100 | Train 0.8016 acc 0.6981 f1 0.6969 || Val 0.7408 acc 0.7226 f1 0.7093\n",
            "Epoch 42/100 | Train 0.8184 acc 0.6862 f1 0.6845 || Val 0.7335 acc 0.7204 f1 0.7081\n",
            "Epoch 43/100 | Train 0.7873 acc 0.7254 f1 0.7254 || Val 0.6790 acc 0.7942 f1 0.7914\n",
            "Epoch 44/100 | Train 0.7624 acc 0.7355 f1 0.7329 || Val 0.6695 acc 0.8031 f1 0.7894\n",
            "Epoch 45/100 | Train 0.7824 acc 0.7183 f1 0.7161 || Val 0.6676 acc 0.7852 f1 0.7764\n",
            "Epoch 46/100 | Train 0.7596 acc 0.7465 f1 0.7463 || Val 0.6816 acc 0.7852 f1 0.7801\n",
            "Epoch 47/100 | Train 0.7534 acc 0.7437 f1 0.7427 || Val 0.6696 acc 0.7785 f1 0.7773\n",
            "Epoch 48/100 | Train 0.7395 acc 0.7566 f1 0.7564 || Val 0.6553 acc 0.7942 f1 0.7891\n",
            "Epoch 49/100 | Train 0.7247 acc 0.7662 f1 0.7647 || Val 0.6426 acc 0.7942 f1 0.7913\n",
            "Epoch 50/100 | Train 0.7291 acc 0.7609 f1 0.7594 || Val 0.6501 acc 0.8009 f1 0.7984\n",
            "Epoch 51/100 | Train 0.7152 acc 0.7657 f1 0.7640 || Val 0.6676 acc 0.7562 f1 0.7511\n",
            "Epoch 52/100 | Train 0.6882 acc 0.7897 f1 0.7865 || Val 0.6196 acc 0.8031 f1 0.7975\n",
            "Epoch 53/100 | Train 0.6806 acc 0.8040 f1 0.8018 || Val 0.6440 acc 0.7852 f1 0.7835\n",
            "Epoch 54/100 | Train 0.6899 acc 0.7935 f1 0.7934 || Val 0.6296 acc 0.7785 f1 0.7737\n",
            "Epoch 55/100 | Train 0.6751 acc 0.8059 f1 0.8051 || Val 0.6186 acc 0.8076 f1 0.7982\n",
            "Epoch 56/100 | Train 0.6606 acc 0.8222 f1 0.8208 || Val 0.6196 acc 0.8076 f1 0.8046\n",
            "Epoch 57/100 | Train 0.6778 acc 0.7959 f1 0.7946 || Val 0.6074 acc 0.8166 f1 0.8134\n",
            "Epoch 58/100 | Train 0.6543 acc 0.8203 f1 0.8207 || Val 0.5792 acc 0.8345 f1 0.8327\n",
            "Epoch 59/100 | Train 0.6396 acc 0.8304 f1 0.8288 || Val 0.5577 acc 0.8345 f1 0.8296\n",
            "Epoch 60/100 | Train 0.6310 acc 0.8366 f1 0.8353 || Val 0.5678 acc 0.8412 f1 0.8381\n",
            "Epoch 61/100 | Train 0.6497 acc 0.8270 f1 0.8260 || Val 0.5521 acc 0.8635 f1 0.8569\n",
            "Epoch 62/100 | Train 0.6097 acc 0.8486 f1 0.8475 || Val 0.5751 acc 0.8456 f1 0.8475\n",
            "Epoch 63/100 | Train 0.6184 acc 0.8443 f1 0.8431 || Val 0.5333 acc 0.8412 f1 0.8405\n",
            "Epoch 64/100 | Train 0.6103 acc 0.8500 f1 0.8487 || Val 0.5491 acc 0.8456 f1 0.8421\n",
            "Epoch 65/100 | Train 0.6164 acc 0.8519 f1 0.8505 || Val 0.5011 acc 0.9038 f1 0.9012\n",
            "Epoch 66/100 | Train 0.6010 acc 0.8505 f1 0.8500 || Val 0.5023 acc 0.8859 f1 0.8843\n",
            "Epoch 67/100 | Train 0.5840 acc 0.8553 f1 0.8544 || Val 0.4966 acc 0.8792 f1 0.8775\n",
            "Epoch 68/100 | Train 0.5608 acc 0.8759 f1 0.8748 || Val 0.5682 acc 0.8568 f1 0.8550\n",
            "Epoch 69/100 | Train 0.5823 acc 0.8668 f1 0.8660 || Val 0.5424 acc 0.8389 f1 0.8333\n",
            "Epoch 70/100 | Train 0.5599 acc 0.8730 f1 0.8713 || Val 0.4772 acc 0.9016 f1 0.8981\n",
            "Epoch 71/100 | Train 0.5471 acc 0.8884 f1 0.8874 || Val 0.4825 acc 0.8837 f1 0.8804\n",
            "Epoch 72/100 | Train 0.5463 acc 0.8864 f1 0.8844 || Val 0.4955 acc 0.8658 f1 0.8626\n",
            "Epoch 73/100 | Train 0.5601 acc 0.8735 f1 0.8722 || Val 0.4901 acc 0.9016 f1 0.8990\n",
            "Epoch 74/100 | Train 0.5381 acc 0.8912 f1 0.8904 || Val 0.4644 acc 0.9083 f1 0.9056\n",
            "Epoch 75/100 | Train 0.5404 acc 0.8884 f1 0.8880 || Val 0.4770 acc 0.8926 f1 0.8913\n",
            "Epoch 76/100 | Train 0.5566 acc 0.8812 f1 0.8802 || Val 0.5498 acc 0.8389 f1 0.8324\n",
            "Epoch 77/100 | Train 0.5577 acc 0.8807 f1 0.8793 || Val 0.4939 acc 0.8702 f1 0.8660\n",
            "Epoch 78/100 | Train 0.5425 acc 0.8970 f1 0.8964 || Val 0.4894 acc 0.8792 f1 0.8780\n",
            "Epoch 79/100 | Train 0.5402 acc 0.8931 f1 0.8916 || Val 0.4437 acc 0.9262 f1 0.9239\n",
            "Epoch 80/100 | Train 0.5304 acc 0.8970 f1 0.8958 || Val 0.4948 acc 0.8904 f1 0.8888\n",
            "Epoch 81/100 | Train 0.5024 acc 0.9114 f1 0.9100 || Val 0.4496 acc 0.9105 f1 0.9091\n",
            "Epoch 82/100 | Train 0.5037 acc 0.9152 f1 0.9143 || Val 0.4622 acc 0.9060 f1 0.9005\n",
            "Epoch 83/100 | Train 0.5058 acc 0.9138 f1 0.9127 || Val 0.4500 acc 0.9038 f1 0.9004\n",
            "Epoch 84/100 | Train 0.5115 acc 0.9061 f1 0.9050 || Val 0.4501 acc 0.9128 f1 0.9108\n",
            "Epoch 85/100 | Train 0.5012 acc 0.9133 f1 0.9124 || Val 0.4896 acc 0.8837 f1 0.8788\n",
            "Epoch 86/100 | Train 0.5162 acc 0.8994 f1 0.8981 || Val 0.4037 acc 0.9463 f1 0.9449\n",
            "Epoch 87/100 | Train 0.4901 acc 0.9214 f1 0.9211 || Val 0.4455 acc 0.9239 f1 0.9218\n",
            "Epoch 88/100 | Train 0.5010 acc 0.9209 f1 0.9202 || Val 0.3921 acc 0.9508 f1 0.9500\n",
            "Epoch 89/100 | Train 0.4820 acc 0.9166 f1 0.9160 || Val 0.4238 acc 0.9284 f1 0.9275\n",
            "Epoch 90/100 | Train 0.4823 acc 0.9233 f1 0.9229 || Val 0.4268 acc 0.9284 f1 0.9268\n",
            "Epoch 91/100 | Train 0.4713 acc 0.9253 f1 0.9246 || Val 0.3940 acc 0.9418 f1 0.9400\n",
            "Epoch 92/100 | Train 0.5006 acc 0.9061 f1 0.9046 || Val 0.4172 acc 0.9374 f1 0.9353\n",
            "Epoch 93/100 | Train 0.4754 acc 0.9305 f1 0.9298 || Val 0.3877 acc 0.9441 f1 0.9416\n",
            "Epoch 94/100 | Train 0.4600 acc 0.9368 f1 0.9363 || Val 0.3999 acc 0.9351 f1 0.9330\n",
            "Epoch 95/100 | Train 0.4867 acc 0.9195 f1 0.9182 || Val 0.4117 acc 0.9374 f1 0.9367\n",
            "Epoch 96/100 | Train 0.4536 acc 0.9411 f1 0.9404 || Val 0.3982 acc 0.9396 f1 0.9383\n",
            "Epoch 97/100 | Train 0.4678 acc 0.9363 f1 0.9356 || Val 0.4349 acc 0.9239 f1 0.9255\n",
            "Epoch 98/100 | Train 0.4576 acc 0.9372 f1 0.9371 || Val 0.3713 acc 0.9530 f1 0.9512\n",
            "Epoch 99/100 | Train 0.4503 acc 0.9377 f1 0.9370 || Val 0.4264 acc 0.9284 f1 0.9247\n",
            "Epoch 100/100 | Train 0.4660 acc 0.9344 f1 0.9340 || Val 0.4145 acc 0.9351 f1 0.9317\n",
            "TEST → Loss 0.4165 | Acc 0.9420 | F1 0.9394\n"
          ]
        }
      ]
    }
  ]
}