{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7ByC5/ucJXME/31VZpPuJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae39900390754faba2965d62cce1daf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1c08ed88d364824aab8757c9338b55c",
              "IPY_MODEL_f578e5ebab6d4cd2aa2e93844fd8c90c",
              "IPY_MODEL_91df39d087fe453db61e876a863e5f1f"
            ],
            "layout": "IPY_MODEL_de6f84fe633d492a89bd2ccab566f7b8"
          }
        },
        "f1c08ed88d364824aab8757c9338b55c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec882351c8624f109ad5657bf433cc53",
            "placeholder": "​",
            "style": "IPY_MODEL_4ce6f50f56c64ecf8644dbbbaf526ebb",
            "value": "model.safetensors: 100%"
          }
        },
        "f578e5ebab6d4cd2aa2e93844fd8c90c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63875d252aed4d2e911730d49f9889b9",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a21e841ad78b41539b6199828957bb21",
            "value": 346284714
          }
        },
        "91df39d087fe453db61e876a863e5f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37eac0ce111a4978a6f7c7cdfdce805d",
            "placeholder": "​",
            "style": "IPY_MODEL_3e9b8e27dcd4426da6e13d5700f9da83",
            "value": " 346M/346M [00:03&lt;00:00, 68.9MB/s]"
          }
        },
        "de6f84fe633d492a89bd2ccab566f7b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec882351c8624f109ad5657bf433cc53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ce6f50f56c64ecf8644dbbbaf526ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63875d252aed4d2e911730d49f9889b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a21e841ad78b41539b6199828957bb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37eac0ce111a4978a6f7c7cdfdce805d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e9b8e27dcd4426da6e13d5700f9da83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimrashaheen001/Programming_for_AI/blob/main/Thesis_Implementation00005.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhLicRwbsNm_",
        "outputId": "3ca1fcaa-ecd1-4073-f2bb-2c75e070b840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.0.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-tabnet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 pytorch-tabnet-4.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install pytorch-tabnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rFj5_rSgt_fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# TabNet Notebook for Alzheimer's Clinical Data (Multiclass AUC + Bug Fixes)\n",
        "# =========================================================\n",
        "\n",
        "# 0) Install and imports\n",
        "try:\n",
        "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "except Exception:\n",
        "    !pip install pytorch-tabnet -q\n",
        "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from pytorch_tabnet.metrics import Metric\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# 1) Config\n",
        "CSV_PATH = \"/content/tabular.csv\"\n",
        "TARGET_COL = \"class\"\n",
        "ID_COL = \"ImageID\"\n",
        "TEST_SIZE = 0.15\n",
        "VALID_SIZE = 0.10\n",
        "RANDOM_STATE = 42\n",
        "TOP_K_FEATURES = 10\n",
        "\n",
        "# 2) Load data\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"CSV loaded:\", CSV_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "if TARGET_COL not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET_COL}' not found.\")\n",
        "\n",
        "# 3) Preprocess\n",
        "nunique = df.nunique(dropna=False)\n",
        "types = df.dtypes\n",
        "exclude_cols = [ID_COL, TARGET_COL] if ID_COL in df.columns else [TARGET_COL]\n",
        "features = [c for c in df.columns if c not in exclude_cols]\n",
        "\n",
        "categorical_columns = []\n",
        "categorical_dims = {}\n",
        "for col in features:\n",
        "    if types[col] == 'object' or nunique[col] < 200:\n",
        "        categorical_columns.append(col)\n",
        "        df[col] = df[col].fillna(\"NA_VALUE\")\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "        categorical_dims[col] = len(le.classes_)\n",
        "    else:\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "print(\"Categorical columns:\", categorical_columns)\n",
        "\n",
        "# 4) Prepare arrays\n",
        "X = df[features].values\n",
        "y_raw = df[TARGET_COL].values\n",
        "if y_raw.dtype == object or y_raw.dtype == 'O' or not np.issubdtype(y_raw.dtype, np.number):\n",
        "    le_target = LabelEncoder()\n",
        "    y = le_target.fit_transform(y_raw.astype(str))\n",
        "    print(\"Target classes:\", list(le_target.classes_))\n",
        "else:\n",
        "    y = y_raw.astype(int)\n",
        "\n",
        "# 5) Split data\n",
        "X_tmp, X_test, y_tmp, y_test, idx_tmp, idx_test = train_test_split(\n",
        "    X, y, df.index.values, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "valid_relative = VALID_SIZE / (1 - TEST_SIZE)\n",
        "X_train, X_valid, y_train, y_valid, idx_train, idx_valid = train_test_split(\n",
        "    X_tmp, y_tmp, idx_tmp, test_size=valid_relative, random_state=RANDOM_STATE, stratify=y_tmp\n",
        ")\n",
        "\n",
        "print(\"Train/Valid/Test sizes:\", X_train.shape[0], X_valid.shape[0], X_test.shape[0])\n",
        "\n",
        "# 6) TabNet cat_idxs/dims\n",
        "ordered_features = features\n",
        "cat_idxs = [i for i, f in enumerate(ordered_features) if f in categorical_columns]\n",
        "cat_dims = [categorical_dims[f] for f in ordered_features if f in categorical_columns]\n",
        "print(\"cat_idxs:\", cat_idxs)\n",
        "print(\"cat_dims:\", cat_dims)\n",
        "\n",
        "# 7) MacroAUC metric class\n",
        "class MacroAUC(Metric):\n",
        "    def __init__(self):\n",
        "        self._name = \"macro_auc\"\n",
        "        self._maximize = True\n",
        "    def __call__(self, y_true, y_score):\n",
        "        from sklearn.preprocessing import label_binarize\n",
        "        y_true = y_true.astype(int)\n",
        "        y_true_bin = label_binarize(y_true, classes=np.arange(y_score.shape[1]))\n",
        "        return roc_auc_score(y_true_bin, y_score, average=\"macro\", multi_class=\"ovr\")\n",
        "\n",
        "# 8) TabNet model\n",
        "max_epochs = 200\n",
        "batch_size = 256\n",
        "clf = TabNetClassifier(\n",
        "    cat_idxs=cat_idxs,\n",
        "    cat_dims=cat_dims,\n",
        "    cat_emb_dim=1,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    scheduler_params={\n",
        "        \"is_batch_level\": True,\n",
        "        \"max_lr\": 5e-2,\n",
        "        \"steps_per_epoch\": int(X_train.shape[0] / batch_size) + 1,\n",
        "        \"epochs\": max_epochs\n",
        "    },\n",
        "    scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n",
        "    mask_type='entmax',\n",
        ")\n",
        "\n",
        "# 9) Train\n",
        "clf.fit(\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "    eval_name=['train', 'valid'],\n",
        "    eval_metric=[MacroAUC, 'accuracy'],\n",
        "    max_epochs=max_epochs,\n",
        "    patience=20,\n",
        "    batch_size=batch_size,\n",
        "    virtual_batch_size=64,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# 10) History handling\n",
        "hist_dict = clf.history\n",
        "if hasattr(hist_dict, \"history\"):\n",
        "    hist_dict = hist_dict.history\n",
        "if not isinstance(hist_dict, dict):\n",
        "    hist_dict = dict(hist_dict)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "if 'loss' in hist_dict:\n",
        "    plt.plot(hist_dict['loss'])\n",
        "plt.title('Training loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "train_key = 'train_macro_auc'\n",
        "valid_key = 'valid_macro_auc'\n",
        "if train_key in hist_dict and valid_key in hist_dict:\n",
        "    plt.plot(hist_dict[train_key], label=train_key)\n",
        "    plt.plot(hist_dict[valid_key], label=valid_key)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 11) Evaluate\n",
        "preds_proba = clf.predict_proba(X_test)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "y_test_bin = label_binarize(y_test, classes=np.arange(preds_proba.shape[1]))\n",
        "test_auc = roc_auc_score(y_test_bin, preds_proba, average='macro', multi_class='ovr')\n",
        "test_preds = clf.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "print(\"Test AUC:\", test_auc, \"Test Acc:\", test_acc)\n",
        "\n",
        "# 12) Feature importances\n",
        "feat_importances = clf.feature_importances_\n",
        "feat_df = pd.DataFrame({\"feature\": ordered_features, \"importance\": feat_importances}).sort_values(\"importance\", ascending=False)\n",
        "feat_df.to_csv(\"tabnet_feature_importances.csv\", index=False)\n",
        "print(feat_df.head(20))\n",
        "\n",
        "# Top-k reduced CSV\n",
        "topk = min(TOP_K_FEATURES, len(ordered_features))\n",
        "top_features = feat_df.head(topk)[\"feature\"].tolist()\n",
        "if ID_COL in df.columns:\n",
        "    export_df = df[[ID_COL, TARGET_COL] + top_features]\n",
        "else:\n",
        "    export_df = df[[TARGET_COL] + top_features]\n",
        "export_df.to_csv(f\"tabular_top{topk}_features.csv\", index=False)\n",
        "print(f\"Saved reduced feature CSV with top {topk} features.\")\n",
        "\n",
        "# 13) Local explainability\n",
        "explained = clf.explain(X_test[:50])\n",
        "\n",
        "if isinstance(explained, dict):\n",
        "    masks_dict = explained['masks']\n",
        "    explain_matrix = explained.get('explain_matrix', None)\n",
        "    masks_array = np.stack(list(masks_dict.values()), axis=0)\n",
        "else:\n",
        "    explain_matrix, masks_array = explained\n",
        "    if isinstance(masks_array, dict):\n",
        "        masks_array = np.stack(list(masks_array.values()), axis=0)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
        "for i in range(min(3, masks_array.shape[0])):\n",
        "    axs[i].imshow(masks_array[i][:, :50], aspect='auto')\n",
        "    axs[i].set_title(f\"Sample {i} mask (first 50 features)\")\n",
        "plt.show()\n",
        "\n",
        "# 14) Save model and masks\n",
        "clf.save_model(\"tabnet_adni_model.zip\")\n",
        "masks_avg = masks_array.mean(axis=1)\n",
        "masks_df = pd.DataFrame(masks_avg, columns=ordered_features)\n",
        "masks_df[\"index\"] = idx_test[:masks_df.shape[0]]\n",
        "masks_df.to_csv(\"tabnet_local_masks_test_subset.csv\", index=False)\n",
        "print(\"Saved TabNet model and masks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vLVEaKfG1dDL",
        "outputId": "7d118953-8edd-4c91-ac15-0dcfdba8aa4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV loaded: /content/tabular.csv\n",
            "Shape: (84, 9)\n",
            "Columns: ['image_path', 'class', 'age', 'sex', 'years_in_education', 'APoe4', 'Abeta', 'P-tau181', 'T-tau']\n",
            "Categorical columns: ['image_path', 'age', 'sex', 'years_in_education', 'APoe4', 'Abeta', 'P-tau181', 'T-tau']\n",
            "Target classes: [np.str_('AD'), np.str_('CN'), np.str_('MCI')]\n",
            "Train/Valid/Test sizes: 62 9 13\n",
            "cat_idxs: [0, 1, 2, 3, 4, 5, 6, 7]\n",
            "cat_dims: [84, 33, 2, 13, 2, 84, 84, 84]\n",
            "epoch 0  | loss: 1.46819 | train_macro_auc: 0.59571 | train_accuracy: 0.35484 | valid_macro_auc: 0.48148 | valid_accuracy: 0.22222 |  0:00:00s\n",
            "epoch 1  | loss: 1.34181 | train_macro_auc: 0.59563 | train_accuracy: 0.37097 | valid_macro_auc: 0.42593 | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 2  | loss: 1.27008 | train_macro_auc: 0.59328 | train_accuracy: 0.41935 | valid_macro_auc: 0.42593 | valid_accuracy: 0.22222 |  0:00:00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3  | loss: 1.2058  | train_macro_auc: 0.58788 | train_accuracy: 0.37097 | valid_macro_auc: 0.40741 | valid_accuracy: 0.22222 |  0:00:00s\n",
            "epoch 4  | loss: 1.15024 | train_macro_auc: 0.56792 | train_accuracy: 0.37097 | valid_macro_auc: 0.42593 | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 5  | loss: 1.10093 | train_macro_auc: 0.55383 | train_accuracy: 0.37097 | valid_macro_auc: 0.42593 | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 6  | loss: 1.0543  | train_macro_auc: 0.5386  | train_accuracy: 0.3871  | valid_macro_auc: 0.48148 | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 7  | loss: 1.00682 | train_macro_auc: 0.52839 | train_accuracy: 0.37097 | valid_macro_auc: 0.5     | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 8  | loss: 0.95992 | train_macro_auc: 0.51747 | train_accuracy: 0.37097 | valid_macro_auc: 0.5     | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 9  | loss: 0.92452 | train_macro_auc: 0.50964 | train_accuracy: 0.35484 | valid_macro_auc: 0.5     | valid_accuracy: 0.22222 |  0:00:00s\n",
            "epoch 10 | loss: 0.88615 | train_macro_auc: 0.5038  | train_accuracy: 0.37097 | valid_macro_auc: 0.46296 | valid_accuracy: 0.22222 |  0:00:00s\n",
            "epoch 11 | loss: 0.83702 | train_macro_auc: 0.49926 | train_accuracy: 0.33871 | valid_macro_auc: 0.42593 | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 12 | loss: 0.78834 | train_macro_auc: 0.50913 | train_accuracy: 0.33871 | valid_macro_auc: 0.40741 | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 13 | loss: 0.76211 | train_macro_auc: 0.51945 | train_accuracy: 0.35484 | valid_macro_auc: 0.37037 | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 14 | loss: 0.73247 | train_macro_auc: 0.53358 | train_accuracy: 0.37097 | valid_macro_auc: 0.38889 | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 15 | loss: 0.69733 | train_macro_auc: 0.55669 | train_accuracy: 0.35484 | valid_macro_auc: 0.37037 | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 16 | loss: 0.66841 | train_macro_auc: 0.57483 | train_accuracy: 0.33871 | valid_macro_auc: 0.35185 | valid_accuracy: 0.11111 |  0:00:00s\n",
            "epoch 17 | loss: 0.64865 | train_macro_auc: 0.59407 | train_accuracy: 0.35484 | valid_macro_auc: 0.33333 | valid_accuracy: 0.11111 |  0:00:01s\n",
            "epoch 18 | loss: 0.63427 | train_macro_auc: 0.60469 | train_accuracy: 0.35484 | valid_macro_auc: 0.35185 | valid_accuracy: 0.11111 |  0:00:01s\n",
            "epoch 19 | loss: 0.60903 | train_macro_auc: 0.62779 | train_accuracy: 0.3871  | valid_macro_auc: 0.33333 | valid_accuracy: 0.22222 |  0:00:01s\n",
            "epoch 20 | loss: 0.56993 | train_macro_auc: 0.63829 | train_accuracy: 0.40323 | valid_macro_auc: 0.2963  | valid_accuracy: 0.22222 |  0:00:01s\n",
            "\n",
            "Early stopping occurred at epoch 20 with best_epoch = 0 and best_valid_accuracy = 0.22222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AUC: 0.41944444444444445 Test Acc: 0.38461538461538464\n",
            "              feature  importance\n",
            "4               APoe4    0.357366\n",
            "6            P-tau181    0.176716\n",
            "5               Abeta    0.117170\n",
            "3  years_in_education    0.116156\n",
            "7               T-tau    0.102527\n",
            "2                 sex    0.061096\n",
            "1                 age    0.037800\n",
            "0          image_path    0.031170\n",
            "Saved reduced feature CSV with top 8 features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved model at tabnet_adni_model.zip.zip\n",
            "Saved TabNet model and masks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/train.zip'\n",
        "extract_path = '/content/train'\n",
        "\n",
        "if zipfile.is_zipfile(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(\"❌ The file is not a valid zip file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEIFB4J2rmoA",
        "outputId": "0596d609-ef51-46cc-9d34-48d2d69a422e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/val.zip'\n",
        "extract_path = '/content/val'\n",
        "\n",
        "if zipfile.is_zipfile(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(\"❌ The file is not a valid zip file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrLhC2mcsqvS",
        "outputId": "d5a40cfe-b9ba-4727-f7cc-7af2fab4d199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/Data-1.zip'\n",
        "extract_path = '/content/dataset'\n",
        "\n",
        "if zipfile.is_zipfile(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(\"❌ The file is not a valid zip file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B52PGnr4VBC6",
        "outputId": "bb1dcff4-9a42-47fa-8548-4f44d6a0c543"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MgSQziz2s2tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Modified MRI ViT training script for direct dataset usage\n",
        "\n",
        "- Automatically creates train/test splits from your dataset structure\n",
        "- No text files needed - works directly with image folders\n",
        "- Configured for your AD/CN/MCI-1 classification task\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# ------------------------- utils -------------------------\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def explore_dataset_structure(data_root: Path):\n",
        "    \"\"\"\n",
        "    Debug function to explore the actual dataset structure\n",
        "    \"\"\"\n",
        "    print(f\"Exploring dataset structure at: {data_root}\")\n",
        "    print(f\"Dataset root exists: {data_root.exists()}\")\n",
        "\n",
        "    if data_root.exists():\n",
        "        print(\"\\nContents of dataset root:\")\n",
        "        for item in data_root.iterdir():\n",
        "            print(f\"  {item.name} ({'directory' if item.is_dir() else 'file'})\")\n",
        "\n",
        "        # Check if there's a Data folder\n",
        "        data_folder = data_root / \"Data\"\n",
        "        print(f\"\\nData folder exists: {data_folder.exists()}\")\n",
        "\n",
        "        if data_folder.exists():\n",
        "            print(\"\\nContents of Data folder:\")\n",
        "            for item in data_folder.iterdir():\n",
        "                print(f\"  {item.name} ({'directory' if item.is_dir() else 'file'})\")\n",
        "\n",
        "                # If it's a directory, check its contents\n",
        "                if item.is_dir():\n",
        "                    print(f\"    Contents of {item.name}:\")\n",
        "                    try:\n",
        "                        files = list(item.glob(\"*\"))\n",
        "                        print(f\"      Total files: {len(files)}\")\n",
        "                        image_files = [f for f in files if f.suffix.lower() in ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']]\n",
        "                        print(f\"      Image files: {len(image_files)}\")\n",
        "                        if image_files:\n",
        "                            print(f\"      First few images: {[f.name for f in image_files[:3]]}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"      Error reading folder: {e}\")\n",
        "\n",
        "def create_dataset_splits(data_root: Path, test_size: float = 0.2, random_state: int = 42):\n",
        "    \"\"\"\n",
        "    Create train/test splits from your dataset structure\n",
        "    \"\"\"\n",
        "\n",
        "    # First, explore the structure\n",
        "    explore_dataset_structure(data_root)\n",
        "\n",
        "    # Define class mapping\n",
        "    class_mapping = {'AD': 0, 'CN': 1, 'MCI-1': 2}\n",
        "\n",
        "    all_samples = []\n",
        "\n",
        "    # Try different possible structures\n",
        "    possible_structures = [\n",
        "        data_root / \"Data\" / \"Data\",  # Your actual structure\n",
        "        data_root / \"Data\",  # Original assumption\n",
        "        data_root,           # Direct in root\n",
        "        data_root / \"dataset\" / \"Data\",  # Nested dataset\n",
        "    ]\n",
        "\n",
        "    data_folder = None\n",
        "    for structure in possible_structures:\n",
        "        print(f\"\\nTrying structure: {structure}\")\n",
        "        if structure.exists():\n",
        "            # Check if it has the class folders\n",
        "            class_folders_exist = all((structure / class_name).exists() for class_name in ['AD', 'CN', 'MCI-1'])\n",
        "            if class_folders_exist:\n",
        "                data_folder = structure\n",
        "                print(f\"Found valid structure at: {data_folder}\")\n",
        "                break\n",
        "\n",
        "    if data_folder is None:\n",
        "        print(\"Could not find a valid dataset structure with AD, CN, MCI-1 folders\")\n",
        "\n",
        "        # Let's try to find any folders with images\n",
        "        print(\"\\nSearching for any folders with images...\")\n",
        "        for root in [data_root, data_root / \"Data\", data_root / \"dataset\"]:\n",
        "            if root.exists():\n",
        "                for item in root.rglob(\"*\"):\n",
        "                    if item.is_dir():\n",
        "                        image_files = list(item.glob(\"*.png\")) + list(item.glob(\"*.jpg\")) + list(item.glob(\"*.jpeg\"))\n",
        "                        if len(image_files) > 0:\n",
        "                            print(f\"Found {len(image_files)} images in: {item}\")\n",
        "\n",
        "        raise ValueError(\"No valid dataset structure found! Please check your dataset path and folder names.\")\n",
        "\n",
        "    # Collect all images from each class folder\n",
        "    for class_name in ['AD', 'CN', 'MCI-1']:\n",
        "        class_folder = data_folder / class_name\n",
        "        if class_folder.exists():\n",
        "            image_files = []\n",
        "            for ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']:\n",
        "                image_files.extend(class_folder.glob(f\"*{ext}\"))\n",
        "                image_files.extend(class_folder.glob(f\"*{ext.upper()}\"))\n",
        "\n",
        "            for img_file in image_files:\n",
        "                all_samples.append((str(img_file), class_mapping[class_name]))\n",
        "\n",
        "            print(f\"Found {len(image_files)} images in {class_name}\")\n",
        "        else:\n",
        "            print(f\"Warning: {class_name} folder not found at {class_folder}\")\n",
        "\n",
        "    print(f\"Total samples found: {len(all_samples)}\")\n",
        "\n",
        "    # Create train/test split\n",
        "    if len(all_samples) == 0:\n",
        "        raise ValueError(\"No images found! Please check your dataset path and structure.\")\n",
        "\n",
        "    train_samples, test_samples = train_test_split(\n",
        "        all_samples,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=[sample[1] for sample in all_samples]  # Stratify by class\n",
        "    )\n",
        "\n",
        "    print(f\"Train samples: {len(train_samples)}\")\n",
        "    print(f\"Test samples: {len(test_samples)}\")\n",
        "\n",
        "    return train_samples, test_samples\n",
        "\n",
        "# ------------------------- dataset -------------------------\n",
        "class MRIImageDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[str, int]], rgb: bool = True, image_size: int = 224, is_training: bool = True):\n",
        "        self.samples = samples\n",
        "        self.rgb = rgb\n",
        "        self.image_size = image_size\n",
        "        self.is_training = is_training\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            img = img.convert(\"RGB\") if self.rgb else img.convert(\"L\")\n",
        "            img = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
        "\n",
        "            # Data augmentation for training\n",
        "            if self.is_training:\n",
        "                if random.random() < 0.5:\n",
        "                    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                if random.random() < 0.3:\n",
        "                    img = img.rotate(random.uniform(-10, 10), resample=Image.BILINEAR)\n",
        "\n",
        "            # Convert to tensor\n",
        "            t = torch.from_numpy(np.array(img))\n",
        "            if t.ndim == 2:\n",
        "                t = t.unsqueeze(-1)\n",
        "            t = t.permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "            # Normalize\n",
        "            if self.rgb:\n",
        "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "            else:\n",
        "                mean = torch.tensor([0.5]).view(1, 1, 1)\n",
        "                std = torch.tensor([0.5]).view(1, 1, 1)\n",
        "            t = (t - mean) / std\n",
        "\n",
        "            return t, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a dummy tensor if image loading fails\n",
        "            if self.rgb:\n",
        "                dummy_tensor = torch.zeros((3, self.image_size, self.image_size))\n",
        "            else:\n",
        "                dummy_tensor = torch.zeros((1, self.image_size, self.image_size))\n",
        "            return dummy_tensor, label\n",
        "\n",
        "# ------------------------- model -------------------------\n",
        "class ViTMRI(nn.Module):\n",
        "    def __init__(self, num_classes: int, in_chans: int = 3, model_name: str = \"vit_base_patch16_224\"):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=True,\n",
        "            in_chans=in_chans,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "# ------------------------- metrics -------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), torch.tensor(y, device=device, dtype=torch.long)\n",
        "        logits = model(x)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "        all_preds.append(pred.cpu())\n",
        "        all_targets.append(y.cpu())\n",
        "\n",
        "    preds = torch.cat(all_preds)\n",
        "    targs = torch.cat(all_targets)\n",
        "    acc = correct / max(total, 1)\n",
        "    f1 = macro_f1(preds, targs)\n",
        "    return acc, f1\n",
        "\n",
        "def macro_f1(preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "    num_classes = int(max(preds.max().item(), targets.max().item())) + 1\n",
        "    f1s = []\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        tp = ((preds == c) & (targets == c)).sum().item()\n",
        "        fp = ((preds == c) & (targets != c)).sum().item()\n",
        "        fn = ((preds != c) & (targets == c)).sum().item()\n",
        "\n",
        "        precision = tp / (tp + fp + 1e-8)\n",
        "        recall = tp / (tp + fn + 1e-8)\n",
        "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "        f1s.append(f1)\n",
        "\n",
        "    return float(sum(f1s) / len(f1s))\n",
        "\n",
        "# ------------------------- training -------------------------\n",
        "def train(args):\n",
        "    set_seed(args.seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create train/test splits from your dataset\n",
        "    train_samples, test_samples = create_dataset_splits(\n",
        "        Path(args.data_root),\n",
        "        test_size=args.test_size,\n",
        "        random_state=args.seed\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_ds = MRIImageDataset(train_samples, args.rgb, args.image_size, is_training=True)\n",
        "    test_ds = MRIImageDataset(test_samples, args.rgb, args.image_size, is_training=False)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = ViTMRI(\n",
        "        num_classes=args.num_classes,\n",
        "        in_chans=(3 if args.rgb else 1),\n",
        "        model_name=args.model_name\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"Model created: {args.model_name}\")\n",
        "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=args.lr,\n",
        "        weight_decay=args.weight_decay\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n",
        "\n",
        "    # Training loop\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # Create training log\n",
        "    with open(\"training_log.csv\", \"w\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow([\"epoch\", \"train_loss\", \"test_acc\", \"test_f1\", \"lr\"])\n",
        "\n",
        "    print(f\"\\nStarting training for {args.epochs} epochs...\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x = x.to(device)\n",
        "            y = torch.tensor(y, device=device, dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=args.amp):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch}/{args.epochs} - Batch {batch_idx + 1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate average training loss\n",
        "        train_loss = running_loss / max(num_batches, 1)\n",
        "\n",
        "        # Evaluation phase\n",
        "        test_acc, test_f1 = evaluate(model, test_loader, device)\n",
        "\n",
        "        # Log results\n",
        "        with open(\"training_log.csv\", \"a\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow([\n",
        "                epoch,\n",
        "                f\"{train_loss:.6f}\",\n",
        "                f\"{test_acc:.4f}\",\n",
        "                f\"{test_f1:.4f}\",\n",
        "                f\"{scheduler.get_last_lr()[0]:.6f}\"\n",
        "            ])\n",
        "\n",
        "        print(f\"Epoch {epoch}/{args.epochs} - Train Loss: {train_loss:.4f} - Test Acc: {test_acc:.4f} - Test F1: {test_f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), \"best_mri_vit.pth\")\n",
        "            print(f\"[+] Saved best model with accuracy={best_acc:.4f}\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    print(f\"\\nTraining completed! Best test accuracy: {best_acc:.4f}\")\n",
        "\n",
        "# ------------------------- configuration -------------------------\n",
        "class Args:\n",
        "    # Dataset paths\n",
        "    data_root = \"/content/dataset\"  # Your dataset root path (correct)\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size = 16  # Increased for better GPU utilization\n",
        "    workers = 2\n",
        "    lr = 5e-5  # Slightly lower learning rate for better stability\n",
        "    weight_decay = 1e-4\n",
        "    num_classes = 3  # AD, CN, MCI-1\n",
        "    epochs = 25  # Increased epochs for better training\n",
        "    seed = 42\n",
        "\n",
        "    # Model settings\n",
        "    model_name = \"vit_base_patch16_224\"  # Can also try \"vit_small_patch16_224\" or \"vit_large_patch16_224\"\n",
        "\n",
        "    # Image settings\n",
        "    rgb = True  # Set to False for grayscale\n",
        "    image_size = 224\n",
        "\n",
        "    # Data split\n",
        "    test_size = 0.2  # 20% for testing, 80% for training\n",
        "\n",
        "    # Training settings\n",
        "    amp = True  # Automatic Mixed Precision for faster training\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Make sure to install required packages:\n",
        "    # pip install timm torch torchvision scikit-learn pillow numpy\n",
        "\n",
        "    train(Args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ae39900390754faba2965d62cce1daf6",
            "f1c08ed88d364824aab8757c9338b55c",
            "f578e5ebab6d4cd2aa2e93844fd8c90c",
            "91df39d087fe453db61e876a863e5f1f",
            "de6f84fe633d492a89bd2ccab566f7b8",
            "ec882351c8624f109ad5657bf433cc53",
            "4ce6f50f56c64ecf8644dbbbaf526ebb",
            "63875d252aed4d2e911730d49f9889b9",
            "a21e841ad78b41539b6199828957bb21",
            "37eac0ce111a4978a6f7c7cdfdce805d",
            "3e9b8e27dcd4426da6e13d5700f9da83"
          ]
        },
        "id": "RhTPGBIJXDC0",
        "outputId": "e9de886b-5a35-4057-c38c-f93621ed7366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Exploring dataset structure at: /content/dataset\n",
            "Dataset root exists: True\n",
            "\n",
            "Contents of dataset root:\n",
            "  Data (directory)\n",
            "\n",
            "Data folder exists: True\n",
            "\n",
            "Contents of Data folder:\n",
            "  Data (directory)\n",
            "    Contents of Data:\n",
            "      Total files: 3\n",
            "      Image files: 0\n",
            "\n",
            "Trying structure: /content/dataset/Data/Data\n",
            "Found valid structure at: /content/dataset/Data/Data\n",
            "Found 917 images in AD\n",
            "Found 931 images in CN\n",
            "Found 1134 images in MCI-1\n",
            "Total samples found: 2982\n",
            "Train samples: 2385\n",
            "Test samples: 597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae39900390754faba2965d62cce1daf6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created: vit_base_patch16_224\n",
            "Number of parameters: 85,800,963\n",
            "\n",
            "Starting training for 25 epochs...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2832239612.py:303: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n",
            "/tmp/ipython-input-2832239612.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y, device=device, dtype=torch.long)\n",
            "/tmp/ipython-input-2832239612.py:327: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=args.amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25 - Batch 10/150 - Loss: 1.2837\n",
            "Epoch 1/25 - Batch 20/150 - Loss: 0.9867\n",
            "Epoch 1/25 - Batch 30/150 - Loss: 0.9679\n",
            "Epoch 1/25 - Batch 40/150 - Loss: 0.6677\n",
            "Epoch 1/25 - Batch 50/150 - Loss: 0.8526\n",
            "Epoch 1/25 - Batch 60/150 - Loss: 0.3790\n",
            "Epoch 1/25 - Batch 70/150 - Loss: 0.5432\n",
            "Epoch 1/25 - Batch 80/150 - Loss: 0.5289\n",
            "Epoch 1/25 - Batch 90/150 - Loss: 0.1203\n",
            "Epoch 1/25 - Batch 100/150 - Loss: 0.0214\n",
            "Epoch 1/25 - Batch 110/150 - Loss: 0.0057\n",
            "Epoch 1/25 - Batch 120/150 - Loss: 0.0200\n",
            "Epoch 1/25 - Batch 130/150 - Loss: 0.0951\n",
            "Epoch 1/25 - Batch 140/150 - Loss: 0.1197\n",
            "Epoch 1/25 - Batch 150/150 - Loss: 3.1484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2832239612.py:222: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x, y = x.to(device), torch.tensor(y, device=device, dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25 - Train Loss: 0.4998 - Test Acc: 0.9363 - Test F1: 0.9381\n",
            "[+] Saved best model with accuracy=0.9363\n",
            "------------------------------------------------------------\n",
            "Epoch 2/25 - Batch 10/150 - Loss: 0.4172\n",
            "Epoch 2/25 - Batch 20/150 - Loss: 0.3120\n",
            "Epoch 2/25 - Batch 30/150 - Loss: 0.0802\n",
            "Epoch 2/25 - Batch 40/150 - Loss: 0.0454\n",
            "Epoch 2/25 - Batch 50/150 - Loss: 0.2281\n",
            "Epoch 2/25 - Batch 60/150 - Loss: 0.0013\n",
            "Epoch 2/25 - Batch 70/150 - Loss: 0.0014\n",
            "Epoch 2/25 - Batch 80/150 - Loss: 0.0808\n",
            "Epoch 2/25 - Batch 90/150 - Loss: 0.0208\n",
            "Epoch 2/25 - Batch 100/150 - Loss: 0.1286\n",
            "Epoch 2/25 - Batch 110/150 - Loss: 0.0626\n",
            "Epoch 2/25 - Batch 120/150 - Loss: 0.0047\n",
            "Epoch 2/25 - Batch 130/150 - Loss: 0.0007\n",
            "Epoch 2/25 - Batch 140/150 - Loss: 0.0020\n",
            "Epoch 2/25 - Batch 150/150 - Loss: 0.0001\n",
            "Epoch 2/25 - Train Loss: 0.0878 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "[+] Saved best model with accuracy=1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 3/25 - Batch 10/150 - Loss: 0.0011\n",
            "Epoch 3/25 - Batch 20/150 - Loss: 0.2762\n",
            "Epoch 3/25 - Batch 30/150 - Loss: 0.0077\n",
            "Epoch 3/25 - Batch 40/150 - Loss: 0.0054\n",
            "Epoch 3/25 - Batch 50/150 - Loss: 0.0023\n",
            "Epoch 3/25 - Batch 60/150 - Loss: 0.0038\n",
            "Epoch 3/25 - Batch 70/150 - Loss: 0.0225\n",
            "Epoch 3/25 - Batch 80/150 - Loss: 0.0011\n",
            "Epoch 3/25 - Batch 90/150 - Loss: 0.0048\n",
            "Epoch 3/25 - Batch 100/150 - Loss: 0.0005\n",
            "Epoch 3/25 - Batch 110/150 - Loss: 0.0002\n",
            "Epoch 3/25 - Batch 120/150 - Loss: 0.0152\n",
            "Epoch 3/25 - Batch 130/150 - Loss: 0.0002\n",
            "Epoch 3/25 - Batch 140/150 - Loss: 0.0031\n",
            "Epoch 3/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 3/25 - Train Loss: 0.0277 - Test Acc: 0.9899 - Test F1: 0.9901\n",
            "------------------------------------------------------------\n",
            "Epoch 4/25 - Batch 10/150 - Loss: 0.0575\n",
            "Epoch 4/25 - Batch 20/150 - Loss: 0.0001\n",
            "Epoch 4/25 - Batch 30/150 - Loss: 0.0233\n",
            "Epoch 4/25 - Batch 40/150 - Loss: 0.0004\n",
            "Epoch 4/25 - Batch 50/150 - Loss: 0.0001\n",
            "Epoch 4/25 - Batch 60/150 - Loss: 0.0179\n",
            "Epoch 4/25 - Batch 70/150 - Loss: 0.1152\n",
            "Epoch 4/25 - Batch 80/150 - Loss: 0.1454\n",
            "Epoch 4/25 - Batch 90/150 - Loss: 0.0925\n",
            "Epoch 4/25 - Batch 100/150 - Loss: 0.0015\n",
            "Epoch 4/25 - Batch 110/150 - Loss: 0.0003\n",
            "Epoch 4/25 - Batch 120/150 - Loss: 0.0006\n",
            "Epoch 4/25 - Batch 130/150 - Loss: 0.0006\n",
            "Epoch 4/25 - Batch 140/150 - Loss: 0.0009\n",
            "Epoch 4/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 4/25 - Train Loss: 0.0255 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 5/25 - Batch 10/150 - Loss: 0.0010\n",
            "Epoch 5/25 - Batch 20/150 - Loss: 0.0002\n",
            "Epoch 5/25 - Batch 30/150 - Loss: 0.0003\n",
            "Epoch 5/25 - Batch 40/150 - Loss: 0.0453\n",
            "Epoch 5/25 - Batch 50/150 - Loss: 0.0233\n",
            "Epoch 5/25 - Batch 60/150 - Loss: 0.0024\n",
            "Epoch 5/25 - Batch 70/150 - Loss: 0.0049\n",
            "Epoch 5/25 - Batch 80/150 - Loss: 0.0003\n",
            "Epoch 5/25 - Batch 90/150 - Loss: 0.0003\n",
            "Epoch 5/25 - Batch 100/150 - Loss: 0.0002\n",
            "Epoch 5/25 - Batch 110/150 - Loss: 0.0004\n",
            "Epoch 5/25 - Batch 120/150 - Loss: 0.0080\n",
            "Epoch 5/25 - Batch 130/150 - Loss: 0.3709\n",
            "Epoch 5/25 - Batch 140/150 - Loss: 0.0031\n",
            "Epoch 5/25 - Batch 150/150 - Loss: 0.0006\n",
            "Epoch 5/25 - Train Loss: 0.0185 - Test Acc: 0.9966 - Test F1: 0.9964\n",
            "------------------------------------------------------------\n",
            "Epoch 6/25 - Batch 10/150 - Loss: 0.1873\n",
            "Epoch 6/25 - Batch 20/150 - Loss: 0.1811\n",
            "Epoch 6/25 - Batch 30/150 - Loss: 0.0911\n",
            "Epoch 6/25 - Batch 40/150 - Loss: 0.0736\n",
            "Epoch 6/25 - Batch 50/150 - Loss: 0.0031\n",
            "Epoch 6/25 - Batch 60/150 - Loss: 0.0075\n",
            "Epoch 6/25 - Batch 70/150 - Loss: 0.0019\n",
            "Epoch 6/25 - Batch 80/150 - Loss: 0.0032\n",
            "Epoch 6/25 - Batch 90/150 - Loss: 0.0017\n",
            "Epoch 6/25 - Batch 100/150 - Loss: 0.0553\n",
            "Epoch 6/25 - Batch 110/150 - Loss: 0.0002\n",
            "Epoch 6/25 - Batch 120/150 - Loss: 0.0004\n",
            "Epoch 6/25 - Batch 130/150 - Loss: 0.0001\n",
            "Epoch 6/25 - Batch 140/150 - Loss: 0.0189\n",
            "Epoch 6/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 6/25 - Train Loss: 0.0472 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 7/25 - Batch 10/150 - Loss: 0.0010\n",
            "Epoch 7/25 - Batch 20/150 - Loss: 0.0001\n",
            "Epoch 7/25 - Batch 30/150 - Loss: 0.0001\n",
            "Epoch 7/25 - Batch 40/150 - Loss: 0.0001\n",
            "Epoch 7/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 7/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 7/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 7/25 - Batch 80/150 - Loss: 0.0001\n",
            "Epoch 7/25 - Batch 90/150 - Loss: 0.3463\n",
            "Epoch 7/25 - Batch 100/150 - Loss: 0.4282\n",
            "Epoch 7/25 - Batch 110/150 - Loss: 0.0076\n",
            "Epoch 7/25 - Batch 120/150 - Loss: 0.0014\n",
            "Epoch 7/25 - Batch 130/150 - Loss: 0.0048\n",
            "Epoch 7/25 - Batch 140/150 - Loss: 0.0023\n",
            "Epoch 7/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 7/25 - Train Loss: 0.0240 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 8/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 8/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 8/25 - Batch 30/150 - Loss: 0.0000\n",
            "Epoch 8/25 - Batch 40/150 - Loss: 0.0001\n",
            "Epoch 8/25 - Batch 50/150 - Loss: 0.0086\n",
            "Epoch 8/25 - Batch 60/150 - Loss: 0.0002\n",
            "Epoch 8/25 - Batch 70/150 - Loss: 0.2102\n",
            "Epoch 8/25 - Batch 80/150 - Loss: 0.0408\n",
            "Epoch 8/25 - Batch 90/150 - Loss: 0.0024\n",
            "Epoch 8/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 8/25 - Batch 110/150 - Loss: 0.0002\n",
            "Epoch 8/25 - Batch 120/150 - Loss: 0.0002\n",
            "Epoch 8/25 - Batch 130/150 - Loss: 0.0002\n",
            "Epoch 8/25 - Batch 140/150 - Loss: 0.0021\n",
            "Epoch 8/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 8/25 - Train Loss: 0.0085 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 9/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 9/25 - Batch 20/150 - Loss: 0.0005\n",
            "Epoch 9/25 - Batch 30/150 - Loss: 0.0000\n",
            "Epoch 9/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 9/25 - Batch 50/150 - Loss: 0.0665\n",
            "Epoch 9/25 - Batch 60/150 - Loss: 0.0001\n",
            "Epoch 9/25 - Batch 70/150 - Loss: 0.0014\n",
            "Epoch 9/25 - Batch 80/150 - Loss: 0.3025\n",
            "Epoch 9/25 - Batch 90/150 - Loss: 0.0100\n",
            "Epoch 9/25 - Batch 100/150 - Loss: 0.2550\n",
            "Epoch 9/25 - Batch 110/150 - Loss: 0.0037\n",
            "Epoch 9/25 - Batch 120/150 - Loss: 0.4380\n",
            "Epoch 9/25 - Batch 130/150 - Loss: 0.0925\n",
            "Epoch 9/25 - Batch 140/150 - Loss: 0.0017\n",
            "Epoch 9/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 9/25 - Train Loss: 0.0594 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 10/25 - Batch 10/150 - Loss: 0.0002\n",
            "Epoch 10/25 - Batch 20/150 - Loss: 0.0011\n",
            "Epoch 10/25 - Batch 30/150 - Loss: 0.0009\n",
            "Epoch 10/25 - Batch 40/150 - Loss: 0.0026\n",
            "Epoch 10/25 - Batch 50/150 - Loss: 0.0001\n",
            "Epoch 10/25 - Batch 60/150 - Loss: 0.0133\n",
            "Epoch 10/25 - Batch 70/150 - Loss: 0.0083\n",
            "Epoch 10/25 - Batch 80/150 - Loss: 0.0001\n",
            "Epoch 10/25 - Batch 90/150 - Loss: 0.0011\n",
            "Epoch 10/25 - Batch 100/150 - Loss: 0.0001\n",
            "Epoch 10/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 10/25 - Batch 120/150 - Loss: 0.0005\n",
            "Epoch 10/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 10/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 10/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 10/25 - Train Loss: 0.0020 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 11/25 - Batch 10/150 - Loss: 0.0001\n",
            "Epoch 11/25 - Batch 20/150 - Loss: 0.0001\n",
            "Epoch 11/25 - Batch 30/150 - Loss: 0.0001\n",
            "Epoch 11/25 - Batch 40/150 - Loss: 0.0006\n",
            "Epoch 11/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 11/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 11/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 11/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 11/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 11/25 - Batch 100/150 - Loss: 0.0001\n",
            "Epoch 11/25 - Batch 110/150 - Loss: 0.0001\n",
            "Epoch 11/25 - Batch 120/150 - Loss: 0.0005\n",
            "Epoch 11/25 - Batch 130/150 - Loss: 0.0001\n",
            "Epoch 11/25 - Batch 140/150 - Loss: 0.0001\n",
            "Epoch 11/25 - Batch 150/150 - Loss: 0.0004\n",
            "Epoch 11/25 - Train Loss: 0.0039 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 12/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 12/25 - Batch 20/150 - Loss: 0.0001\n",
            "Epoch 12/25 - Batch 30/150 - Loss: 0.0024\n",
            "Epoch 12/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 12/25 - Batch 50/150 - Loss: 0.0002\n",
            "Epoch 12/25 - Batch 60/150 - Loss: 0.0002\n",
            "Epoch 12/25 - Batch 70/150 - Loss: 0.0001\n",
            "Epoch 12/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 12/25 - Batch 90/150 - Loss: 0.0001\n",
            "Epoch 12/25 - Batch 100/150 - Loss: 0.0001\n",
            "Epoch 12/25 - Batch 110/150 - Loss: 0.0001\n",
            "Epoch 12/25 - Batch 120/150 - Loss: 0.0001\n",
            "Epoch 12/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 12/25 - Batch 140/150 - Loss: 0.0003\n",
            "Epoch 12/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 12/25 - Train Loss: 0.0010 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 13/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 30/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 80/150 - Loss: 0.0001\n",
            "Epoch 13/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 120/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 13/25 - Train Loss: 0.0001 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 14/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 30/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 120/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 14/25 - Train Loss: 0.0001 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 15/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 30/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 110/150 - Loss: 0.0002\n",
            "Epoch 15/25 - Batch 120/150 - Loss: 0.0004\n",
            "Epoch 15/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Batch 140/150 - Loss: 0.0017\n",
            "Epoch 15/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 15/25 - Train Loss: 0.0033 - Test Acc: 0.9966 - Test F1: 0.9967\n",
            "------------------------------------------------------------\n",
            "Epoch 16/25 - Batch 10/150 - Loss: 0.0411\n",
            "Epoch 16/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 16/25 - Batch 30/150 - Loss: 0.0012\n",
            "Epoch 16/25 - Batch 40/150 - Loss: 0.0129\n",
            "Epoch 16/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 16/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 16/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 16/25 - Batch 80/150 - Loss: 0.0001\n",
            "Epoch 16/25 - Batch 90/150 - Loss: 0.0002\n",
            "Epoch 16/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 16/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 16/25 - Batch 120/150 - Loss: 0.0001\n",
            "Epoch 16/25 - Batch 130/150 - Loss: 0.0002\n",
            "Epoch 16/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 16/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 16/25 - Train Loss: 0.0047 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 17/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 30/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 110/150 - Loss: 0.0001\n",
            "Epoch 17/25 - Batch 120/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 17/25 - Train Loss: 0.0001 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 18/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 30/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 120/150 - Loss: 0.0001\n",
            "Epoch 18/25 - Batch 130/150 - Loss: 0.0002\n",
            "Epoch 18/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 18/25 - Batch 150/150 - Loss: 0.0001\n",
            "Epoch 18/25 - Train Loss: 0.0013 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 19/25 - Batch 10/150 - Loss: 0.0003\n",
            "Epoch 19/25 - Batch 20/150 - Loss: 0.0004\n",
            "Epoch 19/25 - Batch 30/150 - Loss: 0.0001\n",
            "Epoch 19/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 19/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 19/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 19/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 19/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 19/25 - Batch 90/150 - Loss: 0.0001\n",
            "Epoch 19/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 19/25 - Batch 110/150 - Loss: 0.0022\n",
            "Epoch 19/25 - Batch 120/150 - Loss: 0.0000\n",
            "Epoch 19/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 19/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 19/25 - Batch 150/150 - Loss: 0.0001\n",
            "Epoch 19/25 - Train Loss: 0.0001 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 20/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 20/150 - Loss: 0.0001\n",
            "Epoch 20/25 - Batch 30/150 - Loss: 0.0001\n",
            "Epoch 20/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 70/150 - Loss: 0.0001\n",
            "Epoch 20/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 120/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 20/25 - Train Loss: 0.0000 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 21/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 30/150 - Loss: 0.0002\n",
            "Epoch 21/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 80/150 - Loss: 0.0001\n",
            "Epoch 21/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 120/150 - Loss: 0.0001\n",
            "Epoch 21/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 21/25 - Train Loss: 0.0000 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 22/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 30/150 - Loss: 0.0001\n",
            "Epoch 22/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 120/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 22/25 - Train Loss: 0.0000 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 23/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 30/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 80/150 - Loss: 0.0001\n",
            "Epoch 23/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 120/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 23/25 - Train Loss: 0.0000 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 24/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 30/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 60/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 120/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 24/25 - Train Loss: 0.0000 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 25/25 - Batch 10/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 20/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 30/150 - Loss: 0.0002\n",
            "Epoch 25/25 - Batch 40/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 50/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 60/150 - Loss: 0.0001\n",
            "Epoch 25/25 - Batch 70/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 80/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 90/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 100/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 110/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 120/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 130/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 140/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Batch 150/150 - Loss: 0.0000\n",
            "Epoch 25/25 - Train Loss: 0.0000 - Test Acc: 1.0000 - Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Training completed! Best test accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Modified MRI ViT training script for direct dataset usage\n",
        "\n",
        "- Automatically creates train/test splits from your dataset structure\n",
        "- No text files needed - works directly with image folders\n",
        "- Configured for your AD/CN/MCI-1 classification task\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# ------------------------- utils -------------------------\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def explore_dataset_structure(data_root: Path):\n",
        "    \"\"\"\n",
        "    Debug function to explore the actual dataset structure\n",
        "    \"\"\"\n",
        "    print(f\"Exploring dataset structure at: {data_root}\")\n",
        "    print(f\"Dataset root exists: {data_root.exists()}\")\n",
        "\n",
        "    if data_root.exists():\n",
        "        print(\"\\nContents of dataset root:\")\n",
        "        for item in data_root.iterdir():\n",
        "            print(f\"  {item.name} ({'directory' if item.is_dir() else 'file'})\")\n",
        "\n",
        "        # Check if there's a Data folder\n",
        "        data_folder = data_root / \"Data\"\n",
        "        print(f\"\\nData folder exists: {data_folder.exists()}\")\n",
        "\n",
        "        if data_folder.exists():\n",
        "            print(\"\\nContents of Data folder:\")\n",
        "            for item in data_folder.iterdir():\n",
        "                print(f\"  {item.name} ({'directory' if item.is_dir() else 'file'})\")\n",
        "\n",
        "                # If it's a directory, check its contents\n",
        "                if item.is_dir():\n",
        "                    print(f\"    Contents of {item.name}:\")\n",
        "                    try:\n",
        "                        files = list(item.glob(\"*\"))\n",
        "                        print(f\"      Total files: {len(files)}\")\n",
        "                        image_files = [f for f in files if f.suffix.lower() in ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']]\n",
        "                        print(f\"      Image files: {len(image_files)}\")\n",
        "                        if image_files:\n",
        "                            print(f\"      First few images: {[f.name for f in image_files[:3]]}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"      Error reading folder: {e}\")\n",
        "\n",
        "def create_dataset_splits(data_root: Path, test_size: float = 0.2, random_state: int = 42):\n",
        "    \"\"\"\n",
        "    Create train/test splits from your dataset structure\n",
        "    \"\"\"\n",
        "\n",
        "    # First, explore the structure\n",
        "    explore_dataset_structure(data_root)\n",
        "\n",
        "    # Define class mapping\n",
        "    class_mapping = {'AD': 0, 'CN': 1, 'MCI-1': 2}\n",
        "\n",
        "    all_samples = []\n",
        "\n",
        "    # Try different possible structures\n",
        "    possible_structures = [\n",
        "        data_root / \"Data\" / \"Data\",  # Your actual structure\n",
        "        data_root / \"Data\",  # Original assumption\n",
        "        data_root,           # Direct in root\n",
        "        data_root / \"dataset\" / \"Data\",  # Nested dataset\n",
        "    ]\n",
        "\n",
        "    data_folder = None\n",
        "    for structure in possible_structures:\n",
        "        print(f\"\\nTrying structure: {structure}\")\n",
        "        if structure.exists():\n",
        "            # Check if it has the class folders\n",
        "            class_folders_exist = all((structure / class_name).exists() for class_name in ['AD', 'CN', 'MCI-1'])\n",
        "            if class_folders_exist:\n",
        "                data_folder = structure\n",
        "                print(f\"Found valid structure at: {data_folder}\")\n",
        "                break\n",
        "\n",
        "    if data_folder is None:\n",
        "        print(\"Could not find a valid dataset structure with AD, CN, MCI-1 folders\")\n",
        "\n",
        "        # Let's try to find any folders with images\n",
        "        print(\"\\nSearching for any folders with images...\")\n",
        "        for root in [data_root, data_root / \"Data\", data_root / \"dataset\"]:\n",
        "            if root.exists():\n",
        "                for item in root.rglob(\"*\"):\n",
        "                    if item.is_dir():\n",
        "                        image_files = list(item.glob(\"*.png\")) + list(item.glob(\"*.jpg\")) + list(item.glob(\"*.jpeg\"))\n",
        "                        if len(image_files) > 0:\n",
        "                            print(f\"Found {len(image_files)} images in: {item}\")\n",
        "\n",
        "        raise ValueError(\"No valid dataset structure found! Please check your dataset path and folder names.\")\n",
        "\n",
        "    # Collect all images from each class folder\n",
        "    for class_name in ['AD', 'CN', 'MCI-1']:\n",
        "        class_folder = data_folder / class_name\n",
        "        if class_folder.exists():\n",
        "            image_files = []\n",
        "            for ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']:\n",
        "                image_files.extend(class_folder.glob(f\"*{ext}\"))\n",
        "                image_files.extend(class_folder.glob(f\"*{ext.upper()}\"))\n",
        "\n",
        "            for img_file in image_files:\n",
        "                all_samples.append((str(img_file), class_mapping[class_name]))\n",
        "\n",
        "            print(f\"Found {len(image_files)} images in {class_name}\")\n",
        "        else:\n",
        "            print(f\"Warning: {class_name} folder not found at {class_folder}\")\n",
        "\n",
        "    print(f\"Total samples found: {len(all_samples)}\")\n",
        "\n",
        "    # Create train/test split\n",
        "    if len(all_samples) == 0:\n",
        "        raise ValueError(\"No images found! Please check your dataset path and structure.\")\n",
        "\n",
        "    train_samples, test_samples = train_test_split(\n",
        "        all_samples,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=[sample[1] for sample in all_samples]  # Stratify by class\n",
        "    )\n",
        "\n",
        "    print(f\"Train samples: {len(train_samples)}\")\n",
        "    print(f\"Test samples: {len(test_samples)}\")\n",
        "\n",
        "    return train_samples, test_samples\n",
        "\n",
        "# ------------------------- dataset -------------------------\n",
        "class MRIImageDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[str, int]], rgb: bool = True, image_size: int = 224, is_training: bool = True):\n",
        "        self.samples = samples\n",
        "        self.rgb = rgb\n",
        "        self.image_size = image_size\n",
        "        self.is_training = is_training\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            img = img.convert(\"RGB\") if self.rgb else img.convert(\"L\")\n",
        "            img = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
        "\n",
        "            # Data augmentation for training\n",
        "            if self.is_training:\n",
        "                if random.random() < 0.5:\n",
        "                    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                if random.random() < 0.3:\n",
        "                    img = img.rotate(random.uniform(-10, 10), resample=Image.BILINEAR)\n",
        "\n",
        "            # Convert to tensor\n",
        "            t = torch.from_numpy(np.array(img))\n",
        "            if t.ndim == 2:\n",
        "                t = t.unsqueeze(-1)\n",
        "            t = t.permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "            # Normalize\n",
        "            if self.rgb:\n",
        "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "            else:\n",
        "                mean = torch.tensor([0.5]).view(1, 1, 1)\n",
        "                std = torch.tensor([0.5]).view(1, 1, 1)\n",
        "            t = (t - mean) / std\n",
        "\n",
        "            return t, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a dummy tensor if image loading fails\n",
        "            if self.rgb:\n",
        "                dummy_tensor = torch.zeros((3, self.image_size, self.image_size))\n",
        "            else:\n",
        "                dummy_tensor = torch.zeros((1, self.image_size, self.image_size))\n",
        "            return dummy_tensor, label\n",
        "\n",
        "## ------------------------- model -------------------------\n",
        "class ViTMRI(nn.Module):\n",
        "    def __init__(self, num_classes: int, in_chans: int = 3, model_name: str = \"vit_base_patch16_224\"):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=True,\n",
        "            in_chans=in_chans,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "        # Add dropout to help generalization\n",
        "        if hasattr(self.backbone, 'head'):\n",
        "            in_features = self.backbone.head.in_features\n",
        "            self.backbone.head = nn.Sequential(\n",
        "                nn.Dropout(0.4),  # Dropout before final classification\n",
        "                nn.Linear(in_features, num_classes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "\n",
        "# ------------------------- dataset -------------------------\n",
        "class MRIImageDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[str, int]], rgb: bool = True, image_size: int = 224, is_training: bool = True):\n",
        "        self.samples = samples\n",
        "        self.rgb = rgb\n",
        "        self.image_size = image_size\n",
        "        self.is_training = is_training\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            img = img.convert(\"RGB\") if self.rgb else img.convert(\"L\")\n",
        "\n",
        "            if self.is_training:\n",
        "                # Stronger augmentation\n",
        "                if random.random() < 0.5:\n",
        "                    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                if random.random() < 0.3:\n",
        "                    img = img.rotate(random.uniform(-15, 15), resample=Image.BILINEAR)\n",
        "                if random.random() < 0.3:\n",
        "                    img = img.transform(\n",
        "                        img.size, Image.AFFINE,\n",
        "                        (1, random.uniform(-0.1, 0.1), 0, random.uniform(-0.1, 0.1), 1, 0),\n",
        "                        resample=Image.BILINEAR\n",
        "                    )\n",
        "                if random.random() < 0.4:\n",
        "                    factor = random.uniform(0.8, 1.2)\n",
        "                    img = Image.fromarray(np.clip(np.array(img) * factor, 0, 255).astype(np.uint8))\n",
        "\n",
        "            img = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
        "\n",
        "            # Convert to tensor\n",
        "            t = torch.from_numpy(np.array(img))\n",
        "            if t.ndim == 2:\n",
        "                t = t.unsqueeze(-1)\n",
        "            t = t.permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "            # Normalize\n",
        "            if self.rgb:\n",
        "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "            else:\n",
        "                mean = torch.tensor([0.5]).view(1, 1, 1)\n",
        "                std = torch.tensor([0.5]).view(1, 1, 1)\n",
        "            t = (t - mean) / std\n",
        "\n",
        "            return t, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            dummy_tensor = torch.zeros((3 if self.rgb else 1, self.image_size, self.image_size))\n",
        "            return dummy_tensor, label\n",
        "\n",
        "\n",
        "# ------------------------- training -------------------------\n",
        "def train(args):\n",
        "    set_seed(args.seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    train_samples, test_samples = create_dataset_splits(\n",
        "        Path(args.data_root),\n",
        "        test_size=args.test_size,\n",
        "        random_state=args.seed\n",
        "    )\n",
        "\n",
        "    train_ds = MRIImageDataset(train_samples, args.rgb, args.image_size, is_training=True)\n",
        "    test_ds = MRIImageDataset(test_samples, args.rgb, args.image_size, is_training=False)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "    model = ViTMRI(num_classes=args.num_classes, in_chans=(3 if args.rgb else 1), model_name=args.model_name).to(device)\n",
        "    print(f\"Model created: {args.model_name}\")\n",
        "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    patience = 5  # Early stopping patience\n",
        "    patience_counter = 0\n",
        "\n",
        "    with open(\"training_log.csv\", \"w\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow([\"epoch\", \"train_loss\", \"test_acc\", \"test_f1\", \"lr\"])\n",
        "\n",
        "    print(f\"\\nStarting training for {args.epochs} epochs...\\n{'-' * 60}\")\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x = x.to(device)\n",
        "            y = torch.tensor(y, device=device, dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=args.amp):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = running_loss / max(num_batches, 1)\n",
        "        test_acc, test_f1 = evaluate(model, test_loader, device)\n",
        "\n",
        "        with open(\"training_log.csv\", \"a\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow([epoch, f\"{train_loss:.6f}\", f\"{test_acc:.4f}\", f\"{test_f1:.4f}\", f\"{scheduler.get_last_lr()[0]:.6f}\"])\n",
        "\n",
        "        print(f\"Epoch {epoch} - Train Loss: {train_loss:.4f} | Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f}\")\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_mri_vit.pth\")\n",
        "            print(f\"[+] Saved best model (acc={best_acc:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# ------------------------- configuration -------------------------\n",
        "class Args:\n",
        "    data_root = \"/content/dataset\"\n",
        "    batch_size = 16\n",
        "    workers = 2\n",
        "    lr = 3e-5  # Lower LR for better generalization\n",
        "    weight_decay = 5e-4  # Stronger regularization\n",
        "    num_classes = 3\n",
        "    epochs = 50\n",
        "    seed = 42\n",
        "    model_name = \"vit_base_patch16_224\"\n",
        "    rgb = True\n",
        "    image_size = 224\n",
        "    test_size = 0.2\n",
        "    amp = True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train(Args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNC_nXRB4c_U",
        "outputId": "b36f9b1d-412c-470f-d391-6764c3f3b806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Exploring dataset structure at: /content/dataset\n",
            "Dataset root exists: True\n",
            "\n",
            "Contents of dataset root:\n",
            "  Data (directory)\n",
            "\n",
            "Data folder exists: True\n",
            "\n",
            "Contents of Data folder:\n",
            "  Data (directory)\n",
            "    Contents of Data:\n",
            "      Total files: 3\n",
            "      Image files: 0\n",
            "\n",
            "Trying structure: /content/dataset/Data/Data\n",
            "Found valid structure at: /content/dataset/Data/Data\n",
            "Found 917 images in AD\n",
            "Found 931 images in CN\n",
            "Found 1134 images in MCI-1\n",
            "Total samples found: 2982\n",
            "Train samples: 2385\n",
            "Test samples: 597\n",
            "Model created: vit_base_patch16_224\n",
            "Number of parameters: 85,800,963\n",
            "\n",
            "Starting training for 50 epochs...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3745263517.py:306: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n",
            "/tmp/ipython-input-3745263517.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y, device=device, dtype=torch.long)\n",
            "/tmp/ipython-input-3745263517.py:326: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=args.amp):\n",
            "/tmp/ipython-input-2832239612.py:222: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x, y = x.to(device), torch.tensor(y, device=device, dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Train Loss: 0.6989 | Test Acc: 0.9983 | Test F1: 0.9982\n",
            "[+] Saved best model (acc=0.9983)\n",
            "------------------------------------------------------------\n",
            "Epoch 2 - Train Loss: 0.3262 | Test Acc: 0.9966 | Test F1: 0.9967\n",
            "------------------------------------------------------------\n",
            "Epoch 3 - Train Loss: 0.3338 | Test Acc: 0.9950 | Test F1: 0.9946\n",
            "------------------------------------------------------------\n",
            "Epoch 4 - Train Loss: 0.3439 | Test Acc: 1.0000 | Test F1: 1.0000\n",
            "[+] Saved best model (acc=1.0000)\n",
            "------------------------------------------------------------\n",
            "Epoch 5 - Train Loss: 0.3265 | Test Acc: 0.9966 | Test F1: 0.9964\n",
            "------------------------------------------------------------\n",
            "Epoch 6 - Train Loss: 0.3020 | Test Acc: 1.0000 | Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 7 - Train Loss: 0.2990 | Test Acc: 1.0000 | Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 8 - Train Loss: 0.2991 | Test Acc: 1.0000 | Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "Epoch 9 - Train Loss: 0.3058 | Test Acc: 0.9950 | Test F1: 0.9951\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "from transformers import SwinForImageClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "class Args:\n",
        "    data_root = \"/content/dataset/Data/Data\"   # root folder with subfolders per class\n",
        "    batch_size = 8\n",
        "    num_workers = 2\n",
        "    lr = 1e-4\n",
        "    num_epochs = 30\n",
        "    patience = 5     # Early stopping patience\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    amp = True\n",
        "\n",
        "# -----------------------------\n",
        "# Model: Swin-Tiny with Dropout\n",
        "# -----------------------------\n",
        "class SwinTinyModel(nn.Module):\n",
        "    def __init__(self, num_classes=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.swin = SwinForImageClassification.from_pretrained(\n",
        "            \"microsoft/swin-tiny-patch4-window7-224\",\n",
        "            num_labels=num_classes,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        # Replace classifier with Dropout + Linear\n",
        "        in_features = self.swin.classifier.in_features\n",
        "        self.swin.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.swin(img).logits\n",
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "def train_model():\n",
        "    # ✅ Strong augmentations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # small shifts\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = datasets.ImageFolder(root=Args.data_root, transform=transform)\n",
        "    num_classes = len(dataset.classes)\n",
        "    print(f\"Found {len(dataset)} images across {num_classes} classes: {dataset.classes}\")\n",
        "\n",
        "    # Train/Val/Test split\n",
        "    n_total = len(dataset)\n",
        "    n_train = int(0.7 * n_total)\n",
        "    n_val = int(0.15 * n_total)\n",
        "    n_test = n_total - n_train - n_val\n",
        "    train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=Args.batch_size, shuffle=True, num_workers=Args.num_workers)\n",
        "    val_loader = DataLoader(val_ds, batch_size=Args.batch_size, shuffle=False, num_workers=Args.num_workers)\n",
        "    test_loader = DataLoader(test_ds, batch_size=Args.batch_size, shuffle=False, num_workers=Args.num_workers)\n",
        "\n",
        "    # Model, loss, optimizer\n",
        "    model = SwinTinyModel(num_classes=num_classes, dropout=0.3).to(Args.device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=Args.lr, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=Args.amp)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Training loop with Early Stopping\n",
        "    # -----------------------------\n",
        "    best_acc = 0.0\n",
        "    best_loss = np.inf\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(Args.num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(Args.device), labels.to(Args.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.amp.autocast(\"cuda\", enabled=Args.amp):\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        y_true, y_pred = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(Args.device), labels.to(Args.device)\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "                preds = outputs.argmax(1)\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "                y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "        val_loss = np.mean(val_losses)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - \"\n",
        "              f\"Train Loss: {np.mean(train_losses):.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | \"\n",
        "              f\"Val Acc: {acc:.4f} | Val F1: {f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_swin_model.pth\")\n",
        "            print(f\"[+] Saved best model (acc={acc:.4f}, loss={val_loss:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= Args.patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "zucZCiFJ49Hi",
        "outputId": "06a7f43f-d9ea-4102-d473-b933468ceb33"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2982 images across 3 classes: ['AD', 'CN', 'MCI-1']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4283399569.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4283399569.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mArgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4283399569.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin/modeling_swin.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m         outputs = self.swin(\n\u001b[0m\u001b[1;32m   1141\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin/modeling_swin.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    944\u001b[0m         )\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    947\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0minput_dimensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin/modeling_swin.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, always_partition, return_dict)\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    808\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin/modeling_swin.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    738\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin/modeling_swin.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/swin/modeling_swin.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from transformers import SwinForImageClassification, AutoImageProcessor\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ======================================================\n",
        "# Config\n",
        "# ======================================================\n",
        "class Args:\n",
        "    data_root = \"/content/dataset/Data/Data\"   # <-- change path\n",
        "    img_size = 224\n",
        "    batch_size = 16\n",
        "    num_workers = 2\n",
        "    epochs = 20\n",
        "    patience = 5\n",
        "    lr_backbone = 1e-5\n",
        "    lr_head = 1e-4\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    unfreeze_epoch = 5   # unfreeze backbone after N epochs\n",
        "\n",
        "# ======================================================\n",
        "# Dataset & Augmentations\n",
        "# ======================================================\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((Args.img_size, Args.img_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomResizedCrop(Args.img_size, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((Args.img_size, Args.img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Use ImageFolder (subdirs = class names)\n",
        "dataset = ImageFolder(root=Args.data_root, transform=transform_train)\n",
        "num_classes = len(dataset.classes)\n",
        "print(f\"Found {len(dataset)} images across {num_classes} classes: {dataset.classes}\")\n",
        "\n",
        "# Split (70/15/15)\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Apply val/test transforms\n",
        "val_set.dataset.transform = transform_val\n",
        "test_set.dataset.transform = transform_val\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=Args.batch_size, shuffle=True, num_workers=Args.num_workers)\n",
        "val_loader = DataLoader(val_set, batch_size=Args.batch_size, shuffle=False, num_workers=Args.num_workers)\n",
        "test_loader = DataLoader(test_set, batch_size=Args.batch_size, shuffle=False, num_workers=Args.num_workers)\n",
        "\n",
        "# ======================================================\n",
        "# Model\n",
        "# ======================================================\n",
        "model = SwinForImageClassification.from_pretrained(\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\",\n",
        "    num_labels=num_classes,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Add dropout before classifier (to reduce overfitting)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(model.classifier.in_features, num_classes)\n",
        ")\n",
        "\n",
        "model.to(Args.device)\n",
        "\n",
        "# ======================================================\n",
        "# Optimizer with correct param separation\n",
        "# ======================================================\n",
        "backbone_params = []\n",
        "head_params = []\n",
        "for name, param in model.named_parameters():\n",
        "    if \"classifier\" in name:\n",
        "        head_params.append(param)\n",
        "    else:\n",
        "        backbone_params.append(param)\n",
        "\n",
        "optimizer = optim.AdamW([\n",
        "    {\"params\": head_params, \"lr\": Args.lr_head},\n",
        "    {\"params\": backbone_params, \"lr\": Args.lr_backbone}\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# ======================================================\n",
        "# Training utils\n",
        "# ======================================================\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    train_loss, preds, labels = 0, [], []\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Args.epochs} [Train]\")\n",
        "\n",
        "    for x, y in loop:\n",
        "        x, y = x.to(Args.device), y.to(Args.device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x).logits\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        preds.extend(out.argmax(1).cpu().numpy())\n",
        "        labels.extend(y.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    return train_loss / len(train_loader.dataset), acc, f1\n",
        "\n",
        "\n",
        "def evaluate(loader, epoch, split=\"Val\"):\n",
        "    model.eval()\n",
        "    val_loss, preds, labels = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(Args.device), y.to(Args.device)\n",
        "            out = model(x).logits\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "            val_loss += loss.item() * x.size(0)\n",
        "            preds.extend(out.argmax(1).cpu().numpy())\n",
        "            labels.extend(y.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    return val_loss / len(loader.dataset), acc, f1\n",
        "\n",
        "# ======================================================\n",
        "# Training Loop with Early Stopping + Unfreeze\n",
        "# ======================================================\n",
        "best_acc = 0\n",
        "patience_counter = 0\n",
        "backbone_frozen = True\n",
        "\n",
        "for epoch in range(Args.epochs):\n",
        "    # Unfreeze backbone after N epochs\n",
        "    if backbone_frozen and epoch >= Args.unfreeze_epoch:\n",
        "        print(\"🔓 Unfreezing backbone for fine-tuning...\")\n",
        "        for param in backbone_params:\n",
        "            param.requires_grad = True\n",
        "        backbone_frozen = False\n",
        "\n",
        "    train_loss, train_acc, train_f1 = train_one_epoch(epoch)\n",
        "    val_loss, val_acc, val_f1 = evaluate(val_loader, epoch, split=\"Val\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_swin_model.pth\")\n",
        "        print(f\"[+] Saved best model (acc={val_acc:.4f}, loss={val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= Args.patience:\n",
        "        print(\"⏹️ Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# ======================================================\n",
        "# Final Test\n",
        "# ======================================================\n",
        "model.load_state_dict(torch.load(\"best_swin_model.pth\"))\n",
        "test_loss, test_acc, test_f1 = evaluate(test_loader, epoch, split=\"Test\")\n",
        "print(f\"TEST → Loss: {test_loss:.4f} | Acc: {test_acc:.4f} | F1: {test_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkwe3akKmc1f",
        "outputId": "fb36424e-98a0-4685-df7f-79d65fa139c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2982 images across 3 classes: ['AD', 'CN', 'MCI-1']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/20 [Train]: 100%|██████████| 131/131 [00:32<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 0.6566, Acc: 0.7906, F1: 0.7839 | Val Loss: 0.3662, Acc: 0.9776, F1: 0.9776\n",
            "[+] Saved best model (acc=0.9776, loss=0.3662)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 131/131 [00:28<00:00,  4.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Train Loss: 0.3538, Acc: 0.9856, F1: 0.9853 | Val Loss: 0.3133, Acc: 0.9978, F1: 0.9978\n",
            "[+] Saved best model (acc=0.9978, loss=0.3133)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 131/131 [00:27<00:00,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Train Loss: 0.3164, Acc: 0.9986, F1: 0.9986 | Val Loss: 0.3034, Acc: 1.0000, F1: 1.0000\n",
            "[+] Saved best model (acc=1.0000, loss=0.3034)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 131/131 [00:28<00:00,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Train Loss: 0.3066, Acc: 1.0000, F1: 1.0000 | Val Loss: 0.2972, Acc: 1.0000, F1: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 131/131 [00:27<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Train Loss: 0.3052, Acc: 0.9990, F1: 0.9990 | Val Loss: 0.2981, Acc: 1.0000, F1: 1.0000\n",
            "🔓 Unfreezing backbone for fine-tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 131/131 [00:27<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Train Loss: 0.3015, Acc: 1.0000, F1: 1.0000 | Val Loss: 0.2942, Acc: 1.0000, F1: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 131/131 [00:28<00:00,  4.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Train Loss: 0.2988, Acc: 1.0000, F1: 1.0000 | Val Loss: 0.2940, Acc: 1.0000, F1: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 131/131 [00:27<00:00,  4.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Train Loss: 0.2985, Acc: 1.0000, F1: 1.0000 | Val Loss: 0.2935, Acc: 1.0000, F1: 1.0000\n",
            "⏹️ Early stopping triggered.\n",
            "TEST → Loss: 0.3018 | Acc: 1.0000 | F1: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ======================================================\n",
        "# Config\n",
        "# ======================================================\n",
        "class Args:\n",
        "    data_root = \"/content/dataset/Data/Data\"   # <-- change path\n",
        "    img_size = 224\n",
        "    batch_size = 16\n",
        "    num_workers = 2\n",
        "    epochs = 20\n",
        "    patience = 5\n",
        "    lr_backbone = 1e-5\n",
        "    lr_head = 1e-4\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    unfreeze_epoch = 5   # unfreeze backbone after N epochs\n",
        "\n",
        "# ======================================================\n",
        "# Dataset & Augmentations\n",
        "# ======================================================\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((Args.img_size, Args.img_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomResizedCrop(Args.img_size, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((Args.img_size, Args.img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "dataset = ImageFolder(root=Args.data_root, transform=transform_train)\n",
        "num_classes = len(dataset.classes)\n",
        "print(f\"Found {len(dataset)} images across {num_classes} classes: {dataset.classes}\")\n",
        "\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "val_set.dataset.transform = transform_val\n",
        "test_set.dataset.transform = transform_val\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=Args.batch_size, shuffle=True, num_workers=Args.num_workers)\n",
        "val_loader = DataLoader(val_set, batch_size=Args.batch_size, shuffle=False, num_workers=Args.num_workers)\n",
        "test_loader = DataLoader(test_set, batch_size=Args.batch_size, shuffle=False, num_workers=Args.num_workers)\n",
        "\n",
        "# ======================================================\n",
        "# TransUNet Backbone (classification adaptation)\n",
        "# ======================================================\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, embed_dim, H/patch, W/patch)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
        "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # add class token\n",
        "        x = x + self.pos_embed\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransUNetClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, img_size=224, patch_size=16, embed_dim=768, num_heads=8, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4, dropout=0.1)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)       # (B, num_patches+1, embed_dim)\n",
        "        x = x.permute(1, 0, 2)        # (S, B, E)\n",
        "        x = self.transformer(x)       # (S, B, E)\n",
        "        x = x[0]                      # take [CLS] token\n",
        "        x = self.norm(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "model = TransUNetClassifier(num_classes=num_classes).to(Args.device)\n",
        "\n",
        "# ======================================================\n",
        "# Optimizer\n",
        "# ======================================================\n",
        "backbone_params, head_params = [], []\n",
        "for name, param in model.named_parameters():\n",
        "    if \"fc\" in name:\n",
        "        head_params.append(param)\n",
        "    else:\n",
        "        backbone_params.append(param)\n",
        "\n",
        "optimizer = optim.AdamW([\n",
        "    {\"params\": head_params, \"lr\": Args.lr_head},\n",
        "    {\"params\": backbone_params, \"lr\": Args.lr_backbone}\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# ======================================================\n",
        "# Training utils\n",
        "# ======================================================\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    train_loss, preds, labels = 0, [], []\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Args.epochs} [Train]\")\n",
        "\n",
        "    for x, y in loop:\n",
        "        x, y = x.to(Args.device), y.to(Args.device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        preds.extend(out.argmax(1).cpu().numpy())\n",
        "        labels.extend(y.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    return train_loss / len(train_loader.dataset), acc, f1\n",
        "\n",
        "\n",
        "def evaluate(loader, epoch, split=\"Val\"):\n",
        "    model.eval()\n",
        "    val_loss, preds, labels = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(Args.device), y.to(Args.device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "            val_loss += loss.item() * x.size(0)\n",
        "            preds.extend(out.argmax(1).cpu().numpy())\n",
        "            labels.extend(y.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    return val_loss / len(loader.dataset), acc, f1\n",
        "\n",
        "# ======================================================\n",
        "# Training Loop with Early Stopping\n",
        "# ======================================================\n",
        "best_acc = 0\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(Args.epochs):\n",
        "    train_loss, train_acc, train_f1 = train_one_epoch(epoch)\n",
        "    val_loss, val_acc, val_f1 = evaluate(val_loader, epoch, split=\"Val\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_transunet_classifier.pth\")\n",
        "        print(f\"[+] Saved best model (acc={val_acc:.4f}, loss={val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= Args.patience:\n",
        "        print(\"⏹️ Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# ======================================================\n",
        "# Final Test\n",
        "# ======================================================\n",
        "model.load_state_dict(torch.load(\"best_transunet_classifier.pth\"))\n",
        "test_loss, test_acc, test_f1 = evaluate(test_loader, epoch, split=\"Test\")\n",
        "print(f\"TEST → Loss: {test_loss:.4f} | Acc: {test_acc:.4f} | F1: {test_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFIp7_o4oTzI",
        "outputId": "539f7c0d-d3f4-488d-87da-623568d9ec50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2982 images across 3 classes: ['AD', 'CN', 'MCI-1']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  4.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 1.1557, Acc: 0.3551, F1: 0.3469 | Val Loss: 1.0498, Acc: 0.4787, F1: 0.3467\n",
            "[+] Saved best model (acc=0.4787, loss=1.0498)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  4.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Train Loss: 1.0642, Acc: 0.4451, F1: 0.4354 | Val Loss: 0.9654, Acc: 0.5794, F1: 0.5712\n",
            "[+] Saved best model (acc=0.5794, loss=0.9654)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 131/131 [00:25<00:00,  5.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Train Loss: 0.9764, Acc: 0.5242, F1: 0.5204 | Val Loss: 1.1505, Acc: 0.3937, F1: 0.3107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 131/131 [00:25<00:00,  5.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Train Loss: 0.9195, Acc: 0.5683, F1: 0.5537 | Val Loss: 0.8520, Acc: 0.6577, F1: 0.6566\n",
            "[+] Saved best model (acc=0.6577, loss=0.8520)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  4.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Train Loss: 0.7899, Acc: 0.6895, F1: 0.6818 | Val Loss: 0.6895, Acc: 0.7763, F1: 0.7695\n",
            "[+] Saved best model (acc=0.7763, loss=0.6895)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Train Loss: 0.6968, Acc: 0.7571, F1: 0.7538 | Val Loss: 0.6038, Acc: 0.8479, F1: 0.8461\n",
            "[+] Saved best model (acc=0.8479, loss=0.6038)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Train Loss: 0.5809, Acc: 0.8448, F1: 0.8426 | Val Loss: 0.6757, Acc: 0.8054, F1: 0.7597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  4.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Train Loss: 0.5092, Acc: 0.8965, F1: 0.8945 | Val Loss: 0.4350, Acc: 0.9530, F1: 0.9521\n",
            "[+] Saved best model (acc=0.9530, loss=0.4350)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Train Loss: 0.4415, Acc: 0.9396, F1: 0.9385 | Val Loss: 0.4694, Acc: 0.9374, F1: 0.9362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Loss: 0.4044, Acc: 0.9645, F1: 0.9640 | Val Loss: 0.4140, Acc: 0.9575, F1: 0.9564\n",
            "[+] Saved best model (acc=0.9575, loss=0.4140)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Train Loss: 0.3851, Acc: 0.9689, F1: 0.9683 | Val Loss: 0.3877, Acc: 0.9709, F1: 0.9700\n",
            "[+] Saved best model (acc=0.9709, loss=0.3877)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Train Loss: 0.3659, Acc: 0.9784, F1: 0.9781 | Val Loss: 0.3641, Acc: 0.9821, F1: 0.9822\n",
            "[+] Saved best model (acc=0.9821, loss=0.3641)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Train Loss: 0.3468, Acc: 0.9880, F1: 0.9879 | Val Loss: 0.3753, Acc: 0.9821, F1: 0.9818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Train Loss: 0.3440, Acc: 0.9880, F1: 0.9878 | Val Loss: 0.3502, Acc: 0.9888, F1: 0.9891\n",
            "[+] Saved best model (acc=0.9888, loss=0.3502)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Train Loss: 0.3429, Acc: 0.9871, F1: 0.9867 | Val Loss: 0.3389, Acc: 0.9866, F1: 0.9866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  4.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Train Loss: 0.3292, Acc: 0.9928, F1: 0.9928 | Val Loss: 0.3265, Acc: 0.9911, F1: 0.9913\n",
            "[+] Saved best model (acc=0.9911, loss=0.3265)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Train Loss: 0.3244, Acc: 0.9947, F1: 0.9947 | Val Loss: 0.3379, Acc: 0.9866, F1: 0.9869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  4.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Train Loss: 0.3162, Acc: 0.9981, F1: 0.9980 | Val Loss: 0.3252, Acc: 0.9911, F1: 0.9913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 131/131 [00:25<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Train Loss: 0.3147, Acc: 0.9966, F1: 0.9965 | Val Loss: 0.3130, Acc: 0.9933, F1: 0.9931\n",
            "[+] Saved best model (acc=0.9933, loss=0.3130)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 131/131 [00:26<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Train Loss: 0.3119, Acc: 0.9976, F1: 0.9976 | Val Loss: 0.3124, Acc: 0.9955, F1: 0.9956\n",
            "[+] Saved best model (acc=0.9955, loss=0.3124)\n",
            "TEST → Loss: 0.3189 | Acc: 0.9911 | F1: 0.9911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ======================================================\n",
        "# Config\n",
        "# ======================================================\n",
        "class Args:\n",
        "    data_root = \"/content/dataset/Data/Data\"   # <-- change path\n",
        "    img_size = 224\n",
        "    batch_size = 16\n",
        "    num_workers = 2\n",
        "    epochs = 20\n",
        "    patience = 5\n",
        "    lr_backbone = 1e-5\n",
        "    lr_head = 1e-4\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    unfreeze_epoch = 5   # unfreeze backbone after N epochs\n",
        "    n_splits = 5         # KFold splits\n",
        "\n",
        "# ======================================================\n",
        "# Dataset & Augmentations\n",
        "# ======================================================\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((Args.img_size, Args.img_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomResizedCrop(Args.img_size, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((Args.img_size, Args.img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "dataset = ImageFolder(root=Args.data_root, transform=transform_train)\n",
        "num_classes = len(dataset.classes)\n",
        "print(f\"Found {len(dataset)} images across {num_classes} classes: {dataset.classes}\")\n",
        "\n",
        "# ======================================================\n",
        "# Model Definition\n",
        "# ======================================================\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransUNetClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, img_size=224, patch_size=16, embed_dim=768, num_heads=8, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
        "                                                   dim_feedforward=embed_dim*4, dropout=0.1)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x[0]\n",
        "        x = self.norm(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ======================================================\n",
        "# Training/Eval Functions\n",
        "# ======================================================\n",
        "def train_one_epoch(model, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    train_loss, preds, labels = 0, [], []\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Args.epochs} [Train]\")\n",
        "\n",
        "    for x, y in loop:\n",
        "        x, y = x.to(Args.device), y.to(Args.device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        preds.extend(out.argmax(1).cpu().numpy())\n",
        "        labels.extend(y.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    return train_loss / len(train_loader.dataset), acc, f1\n",
        "\n",
        "def evaluate(model, loader, criterion, epoch, split=\"Val\"):\n",
        "    model.eval()\n",
        "    val_loss, preds, labels = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(Args.device), y.to(Args.device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "            val_loss += loss.item() * x.size(0)\n",
        "            preds.extend(out.argmax(1).cpu().numpy())\n",
        "            labels.extend(y.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    return val_loss / len(loader.dataset), acc, f1\n",
        "\n",
        "# ======================================================\n",
        "# K-Fold Cross Validation\n",
        "# ======================================================\n",
        "kf = KFold(n_splits=Args.n_splits, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)))):\n",
        "    print(f\"\\n========== Fold {fold+1}/{Args.n_splits} ==========\")\n",
        "\n",
        "    # Subset for current fold\n",
        "    train_subset = Subset(dataset, train_idx)\n",
        "    val_subset = Subset(dataset, val_idx)\n",
        "\n",
        "    # Apply transforms separately\n",
        "    train_subset.dataset.transform = transform_train\n",
        "    val_subset.dataset.transform = transform_val\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=Args.batch_size, shuffle=True, num_workers=Args.num_workers)\n",
        "    val_loader = DataLoader(val_subset, batch_size=Args.batch_size, shuffle=False, num_workers=Args.num_workers)\n",
        "\n",
        "    # Initialize model + optimizer + loss fresh for each fold\n",
        "    model = TransUNetClassifier(num_classes=num_classes).to(Args.device)\n",
        "    backbone_params, head_params = [], []\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"fc\" in name:\n",
        "            head_params.append(param)\n",
        "        else:\n",
        "            backbone_params.append(param)\n",
        "\n",
        "    optimizer = optim.AdamW([\n",
        "        {\"params\": head_params, \"lr\": Args.lr_head},\n",
        "        {\"params\": backbone_params, \"lr\": Args.lr_backbone}\n",
        "    ], weight_decay=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    best_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(Args.epochs):\n",
        "        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, optimizer, criterion, epoch)\n",
        "        val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion, epoch, split=\"Val\")\n",
        "\n",
        "        print(f\"Fold {fold+1} | Epoch {epoch+1} | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f\"best_transunet_fold{fold+1}.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= Args.patience:\n",
        "            print(\"⏹️ Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    fold_results.append(best_acc)\n",
        "\n",
        "print(\"\\n========== Cross-validation Results ==========\")\n",
        "print(f\"Per-fold acc: {fold_results}\")\n",
        "print(f\"Mean acc: {np.mean(fold_results):.4f} | Std: {np.std(fold_results):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtwDOJtjrPrO",
        "outputId": "066beaf2-7b9f-474f-e581-06cef5a09927"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2982 images across 3 classes: ['AD', 'CN', 'MCI-1']\n",
            "\n",
            "========== Fold 1/5 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 1 | Train Loss: 1.1564, Acc: 0.3690, F1: 0.3575 | Val Loss: 1.0616, Acc: 0.4338, F1: 0.3385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 2 | Train Loss: 1.0590, Acc: 0.4415, F1: 0.4254 | Val Loss: 1.0376, Acc: 0.4807, F1: 0.3917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 3 | Train Loss: 0.9747, Acc: 0.5342, F1: 0.5226 | Val Loss: 0.9991, Acc: 0.5310, F1: 0.4322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 4 | Train Loss: 0.8486, Acc: 0.6423, F1: 0.6319 | Val Loss: 0.8044, Acc: 0.6633, F1: 0.6538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 5 | Train Loss: 0.7330, Acc: 0.7266, F1: 0.7189 | Val Loss: 0.6023, Acc: 0.8157, F1: 0.8207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 6 | Train Loss: 0.5800, Acc: 0.8465, F1: 0.8424 | Val Loss: 0.6959, Acc: 0.7672, F1: 0.7583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 7 | Train Loss: 0.5111, Acc: 0.8939, F1: 0.8917 | Val Loss: 0.4463, Acc: 0.9330, F1: 0.9319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 8 | Train Loss: 0.4536, Acc: 0.9266, F1: 0.9244 | Val Loss: 0.3871, Acc: 0.9732, F1: 0.9727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 9 | Train Loss: 0.4187, Acc: 0.9476, F1: 0.9466 | Val Loss: 0.4050, Acc: 0.9631, F1: 0.9623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 10 | Train Loss: 0.3804, Acc: 0.9753, F1: 0.9744 | Val Loss: 0.3576, Acc: 0.9849, F1: 0.9845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 11 | Train Loss: 0.3679, Acc: 0.9824, F1: 0.9816 | Val Loss: 0.3810, Acc: 0.9765, F1: 0.9760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 12 | Train Loss: 0.3598, Acc: 0.9841, F1: 0.9837 | Val Loss: 0.3487, Acc: 0.9832, F1: 0.9827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 13 | Train Loss: 0.3490, Acc: 0.9895, F1: 0.9892 | Val Loss: 0.3407, Acc: 0.9950, F1: 0.9948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 14 | Train Loss: 0.3389, Acc: 0.9950, F1: 0.9948 | Val Loss: 0.3364, Acc: 0.9933, F1: 0.9932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 15 | Train Loss: 0.3339, Acc: 0.9954, F1: 0.9952 | Val Loss: 0.3329, Acc: 0.9899, F1: 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 16 | Train Loss: 0.3297, Acc: 0.9954, F1: 0.9953 | Val Loss: 0.3403, Acc: 0.9916, F1: 0.9914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 17 | Train Loss: 0.3229, Acc: 0.9983, F1: 0.9983 | Val Loss: 0.3225, Acc: 0.9950, F1: 0.9948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 18 | Train Loss: 0.3184, Acc: 0.9975, F1: 0.9974 | Val Loss: 0.3121, Acc: 1.0000, F1: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 150/150 [00:30<00:00,  4.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 19 | Train Loss: 0.3147, Acc: 0.9987, F1: 0.9986 | Val Loss: 0.3128, Acc: 0.9983, F1: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | Epoch 20 | Train Loss: 0.3149, Acc: 0.9979, F1: 0.9977 | Val Loss: 0.3079, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "========== Fold 2/5 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 1 | Train Loss: 1.1554, Acc: 0.3547, F1: 0.3455 | Val Loss: 1.0827, Acc: 0.4188, F1: 0.2656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 2 | Train Loss: 1.0552, Acc: 0.4591, F1: 0.4530 | Val Loss: 0.9944, Acc: 0.5109, F1: 0.4832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 3 | Train Loss: 0.9676, Acc: 0.5275, F1: 0.5189 | Val Loss: 1.0399, Acc: 0.4623, F1: 0.3906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 4 | Train Loss: 0.8498, Acc: 0.6478, F1: 0.6424 | Val Loss: 0.6694, Acc: 0.7990, F1: 0.7984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 5 | Train Loss: 0.6706, Acc: 0.7836, F1: 0.7814 | Val Loss: 0.7153, Acc: 0.7487, F1: 0.7303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 6 | Train Loss: 0.5623, Acc: 0.8600, F1: 0.8580 | Val Loss: 0.6661, Acc: 0.8258, F1: 0.8060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 7 | Train Loss: 0.4736, Acc: 0.9178, F1: 0.9177 | Val Loss: 0.5190, Acc: 0.9079, F1: 0.9070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 8 | Train Loss: 0.4322, Acc: 0.9405, F1: 0.9403 | Val Loss: 0.4292, Acc: 0.9464, F1: 0.9442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 150/150 [00:30<00:00,  4.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 9 | Train Loss: 0.3977, Acc: 0.9639, F1: 0.9635 | Val Loss: 0.4828, Acc: 0.9162, F1: 0.9173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 10 | Train Loss: 0.4153, Acc: 0.9568, F1: 0.9568 | Val Loss: 0.3658, Acc: 0.9832, F1: 0.9830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 11 | Train Loss: 0.3640, Acc: 0.9820, F1: 0.9819 | Val Loss: 0.3570, Acc: 0.9866, F1: 0.9862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 12 | Train Loss: 0.3541, Acc: 0.9849, F1: 0.9847 | Val Loss: 0.3534, Acc: 0.9883, F1: 0.9880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 13 | Train Loss: 0.3450, Acc: 0.9912, F1: 0.9911 | Val Loss: 0.3583, Acc: 0.9832, F1: 0.9830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 14 | Train Loss: 0.3397, Acc: 0.9925, F1: 0.9925 | Val Loss: 0.3462, Acc: 0.9899, F1: 0.9896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 150/150 [00:30<00:00,  5.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 15 | Train Loss: 0.3292, Acc: 0.9958, F1: 0.9958 | Val Loss: 0.3304, Acc: 0.9933, F1: 0.9932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 16 | Train Loss: 0.3279, Acc: 0.9950, F1: 0.9950 | Val Loss: 0.3531, Acc: 0.9799, F1: 0.9789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 17 | Train Loss: 0.3191, Acc: 0.9983, F1: 0.9983 | Val Loss: 0.3224, Acc: 0.9950, F1: 0.9950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 18 | Train Loss: 0.3261, Acc: 0.9962, F1: 0.9962 | Val Loss: 0.3269, Acc: 0.9899, F1: 0.9896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 19 | Train Loss: 0.3158, Acc: 0.9987, F1: 0.9987 | Val Loss: 0.3184, Acc: 0.9966, F1: 0.9964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | Epoch 20 | Train Loss: 0.3115, Acc: 0.9987, F1: 0.9986 | Val Loss: 0.3165, Acc: 0.9966, F1: 0.9968\n",
            "\n",
            "========== Fold 3/5 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 1 | Train Loss: 1.1264, Acc: 0.3764, F1: 0.3709 | Val Loss: 1.0357, Acc: 0.5252, F1: 0.4065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 2 | Train Loss: 1.0403, Acc: 0.4661, F1: 0.4595 | Val Loss: 0.9276, Acc: 0.5621, F1: 0.4408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 3 | Train Loss: 0.9452, Acc: 0.5562, F1: 0.5510 | Val Loss: 0.8888, Acc: 0.6107, F1: 0.5977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 4 | Train Loss: 0.8294, Acc: 0.6672, F1: 0.6633 | Val Loss: 0.8785, Acc: 0.6124, F1: 0.5732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 5 | Train Loss: 0.7041, Acc: 0.7624, F1: 0.7592 | Val Loss: 0.5660, Acc: 0.8775, F1: 0.8729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 6 | Train Loss: 0.5874, Acc: 0.8491, F1: 0.8476 | Val Loss: 0.5292, Acc: 0.8826, F1: 0.8717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 7 | Train Loss: 0.5103, Acc: 0.9044, F1: 0.9032 | Val Loss: 0.6496, Acc: 0.7919, F1: 0.7906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 8 | Train Loss: 0.4557, Acc: 0.9308, F1: 0.9303 | Val Loss: 0.4194, Acc: 0.9446, F1: 0.9404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 9 | Train Loss: 0.4122, Acc: 0.9581, F1: 0.9572 | Val Loss: 0.3702, Acc: 0.9799, F1: 0.9785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 10 | Train Loss: 0.3884, Acc: 0.9673, F1: 0.9668 | Val Loss: 0.3594, Acc: 0.9849, F1: 0.9847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 11 | Train Loss: 0.3733, Acc: 0.9757, F1: 0.9752 | Val Loss: 0.3484, Acc: 0.9950, F1: 0.9946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 12 | Train Loss: 0.3556, Acc: 0.9883, F1: 0.9879 | Val Loss: 0.3458, Acc: 0.9933, F1: 0.9927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 13 | Train Loss: 0.3511, Acc: 0.9908, F1: 0.9905 | Val Loss: 0.3515, Acc: 0.9933, F1: 0.9930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 14 | Train Loss: 0.3446, Acc: 0.9908, F1: 0.9906 | Val Loss: 0.3569, Acc: 0.9899, F1: 0.9899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 15 | Train Loss: 0.3411, Acc: 0.9925, F1: 0.9923 | Val Loss: 0.3247, Acc: 0.9983, F1: 0.9981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 16 | Train Loss: 0.3337, Acc: 0.9958, F1: 0.9957 | Val Loss: 0.3280, Acc: 0.9983, F1: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 17 | Train Loss: 0.3237, Acc: 0.9987, F1: 0.9987 | Val Loss: 0.3328, Acc: 0.9933, F1: 0.9927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 18 | Train Loss: 0.3251, Acc: 0.9975, F1: 0.9974 | Val Loss: 0.3359, Acc: 0.9933, F1: 0.9927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 19 | Train Loss: 0.3224, Acc: 0.9983, F1: 0.9983 | Val Loss: 0.3175, Acc: 0.9983, F1: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | Epoch 20 | Train Loss: 0.3173, Acc: 0.9987, F1: 0.9987 | Val Loss: 0.3310, Acc: 0.9916, F1: 0.9908\n",
            "⏹️ Early stopping triggered.\n",
            "\n",
            "========== Fold 4/5 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 1 | Train Loss: 1.1555, Acc: 0.3562, F1: 0.3483 | Val Loss: 1.0503, Acc: 0.4648, F1: 0.3683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 2 | Train Loss: 1.0525, Acc: 0.4476, F1: 0.4378 | Val Loss: 1.1721, Acc: 0.3859, F1: 0.3003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 3 | Train Loss: 0.9437, Acc: 0.5650, F1: 0.5579 | Val Loss: 0.8611, Acc: 0.6191, F1: 0.6273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 4 | Train Loss: 0.7836, Acc: 0.6873, F1: 0.6822 | Val Loss: 0.6893, Acc: 0.7869, F1: 0.7800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 5 | Train Loss: 0.6420, Acc: 0.8068, F1: 0.8040 | Val Loss: 0.5897, Acc: 0.8255, F1: 0.8129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 6 | Train Loss: 0.5469, Acc: 0.8747, F1: 0.8728 | Val Loss: 0.4852, Acc: 0.9128, F1: 0.9129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 7 | Train Loss: 0.4800, Acc: 0.9124, F1: 0.9117 | Val Loss: 0.4365, Acc: 0.9396, F1: 0.9385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 8 | Train Loss: 0.4385, Acc: 0.9438, F1: 0.9431 | Val Loss: 0.4261, Acc: 0.9530, F1: 0.9508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 9 | Train Loss: 0.3970, Acc: 0.9656, F1: 0.9654 | Val Loss: 0.3998, Acc: 0.9614, F1: 0.9596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 10 | Train Loss: 0.3754, Acc: 0.9778, F1: 0.9776 | Val Loss: 0.3491, Acc: 0.9883, F1: 0.9876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 11 | Train Loss: 0.3634, Acc: 0.9849, F1: 0.9845 | Val Loss: 0.3379, Acc: 0.9966, F1: 0.9965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 12 | Train Loss: 0.3528, Acc: 0.9912, F1: 0.9911 | Val Loss: 0.3373, Acc: 0.9933, F1: 0.9929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 13 | Train Loss: 0.3429, Acc: 0.9958, F1: 0.9958 | Val Loss: 0.3297, Acc: 0.9983, F1: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 14 | Train Loss: 0.3342, Acc: 0.9979, F1: 0.9978 | Val Loss: 0.3435, Acc: 0.9950, F1: 0.9947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 15 | Train Loss: 0.3349, Acc: 0.9946, F1: 0.9945 | Val Loss: 0.3220, Acc: 0.9983, F1: 0.9982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 16 | Train Loss: 0.3231, Acc: 0.9987, F1: 0.9987 | Val Loss: 0.3211, Acc: 0.9983, F1: 0.9982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 17 | Train Loss: 0.3243, Acc: 0.9962, F1: 0.9961 | Val Loss: 0.3162, Acc: 1.0000, F1: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 18 | Train Loss: 0.3179, Acc: 0.9979, F1: 0.9979 | Val Loss: 0.3144, Acc: 1.0000, F1: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 19 | Train Loss: 0.3147, Acc: 0.9992, F1: 0.9991 | Val Loss: 0.3124, Acc: 0.9983, F1: 0.9982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 150/150 [00:30<00:00,  4.98it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | Epoch 20 | Train Loss: 0.3144, Acc: 0.9987, F1: 0.9987 | Val Loss: 0.3118, Acc: 0.9983, F1: 0.9982\n",
            "\n",
            "========== Fold 5/5 ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 1 | Train Loss: 1.1636, Acc: 0.3453, F1: 0.3370 | Val Loss: 1.0547, Acc: 0.4648, F1: 0.3397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 2 | Train Loss: 1.0678, Acc: 0.4514, F1: 0.4446 | Val Loss: 0.9568, Acc: 0.5201, F1: 0.4041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 3 | Train Loss: 0.9756, Acc: 0.4975, F1: 0.4954 | Val Loss: 0.8878, Acc: 0.5772, F1: 0.4786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 4 | Train Loss: 0.8297, Acc: 0.6526, F1: 0.6455 | Val Loss: 0.7033, Acc: 0.7500, F1: 0.7404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 5 | Train Loss: 0.6936, Acc: 0.7741, F1: 0.7710 | Val Loss: 0.6448, Acc: 0.7903, F1: 0.7684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 6 | Train Loss: 0.5695, Acc: 0.8521, F1: 0.8499 | Val Loss: 0.4747, Acc: 0.9144, F1: 0.9124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 7 | Train Loss: 0.4985, Acc: 0.9003, F1: 0.8983 | Val Loss: 0.4313, Acc: 0.9446, F1: 0.9425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 8 | Train Loss: 0.4431, Acc: 0.9371, F1: 0.9359 | Val Loss: 0.4371, Acc: 0.9463, F1: 0.9457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 9 | Train Loss: 0.4226, Acc: 0.9480, F1: 0.9469 | Val Loss: 0.3937, Acc: 0.9664, F1: 0.9654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 10 | Train Loss: 0.3840, Acc: 0.9728, F1: 0.9722 | Val Loss: 0.3605, Acc: 0.9832, F1: 0.9823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 11 | Train Loss: 0.3641, Acc: 0.9849, F1: 0.9843 | Val Loss: 0.3520, Acc: 0.9899, F1: 0.9893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 12 | Train Loss: 0.3506, Acc: 0.9899, F1: 0.9895 | Val Loss: 0.3384, Acc: 0.9899, F1: 0.9899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 13 | Train Loss: 0.3479, Acc: 0.9862, F1: 0.9859 | Val Loss: 0.3357, Acc: 0.9899, F1: 0.9900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 14 | Train Loss: 0.3307, Acc: 0.9958, F1: 0.9955 | Val Loss: 0.3365, Acc: 0.9916, F1: 0.9918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 150/150 [00:30<00:00,  5.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 15 | Train Loss: 0.3361, Acc: 0.9933, F1: 0.9933 | Val Loss: 0.3493, Acc: 0.9933, F1: 0.9929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 16 | Train Loss: 0.3295, Acc: 0.9954, F1: 0.9952 | Val Loss: 0.3217, Acc: 0.9950, F1: 0.9951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 17 | Train Loss: 0.3191, Acc: 0.9983, F1: 0.9983 | Val Loss: 0.3131, Acc: 0.9966, F1: 0.9967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 18 | Train Loss: 0.3196, Acc: 0.9975, F1: 0.9974 | Val Loss: 0.3203, Acc: 0.9950, F1: 0.9947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 19 | Train Loss: 0.3123, Acc: 0.9983, F1: 0.9983 | Val Loss: 0.3112, Acc: 0.9966, F1: 0.9967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 150/150 [00:29<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 | Epoch 20 | Train Loss: 0.3078, Acc: 0.9996, F1: 0.9995 | Val Loss: 0.3094, Acc: 0.9966, F1: 0.9967\n",
            "\n",
            "========== Cross-validation Results ==========\n",
            "Per-fold acc: [1.0, 0.9966499162479062, 0.9983221476510067, 1.0, 0.9966442953020134]\n",
            "Mean acc: 0.9983 | Std: 0.0015\n"
          ]
        }
      ]
    }
  ]
}